[["index.html", "Introduction Software Basics Author Team Acknowledgements", " OpenAirQ Toolkit Developed for the Partnership for Healthy Cities with support from Bloomberg Philathropies. Last Updated : 2020-12-15 on R version 4.0.3 (2020-10-10) Introduction This toolkit provides and introduction to spatial data processing and analysis in R for air quality analysis applications, allowing researchers, policymakers, analysts, and practicioners to gain data-driven air quality insights in their local community. After completing out tutorials, you will have the tools to replicate our analysis or perform similar analyses on a new geography of your choosing. This toolkit is divided into two sections. A first section detailing the air quality analyses done for the Chicagoland area, and a secondary appendix with tutorials on how to collect open-source air quality data from reputable government sources like the EPA and NASA. All data used in the tutorial is hosted on a GitHub repo named OpenAirQ-toolkit. Software Basics Tutorials assume that R and RStudio is already downloaded on your device. Luckily, this toolkit is compatible with Windows, macOS, and Linux systems. Basic familiarity in R is required for these toolkits. You should know how to change the working directory, install new packages, library packages, and comfortably navigate between folders on your computer. Additionally, an internet connection will be required for some tutorials. If you are new to R, we recommend the following intro-level tutorials provided through installation guides. You can also refer to this R for Social Scientists tutorial developed by Data Carpentry for a refresher. Before we begin, install the following packages for data wrangling and spatial data analysis. sf, for vector spatial data manipulation sp, for vector &amp; raster data manipulation raster, for raster data manipulation tmap, for cartography and mapmaking in R dplyr, for data frame manipulation Author Team This toolkit was developed for the Partnership for Healthy Cities by Marynia Kolak, Isaac Kamber, Lorenz Menendez, Haowen Shang, Yuming Liu, and Jizhou Wang at the Center for Spatial Data Science at the University of Chicago with support from Bloomberg Philantropies. Acknowledgements This research was supported by TBD, add any legal discliamers or other sponsor verbiage here. "],["02-vectormapping.html", "1 Vector Data Mapping 1.1 Introduction 1.2 Environment Setup 1.3 County Boundaries 1.4 State Boundaries 1.5 EPA Particulate Matter Stations 1.6 Weather Stations 1.7 Point Sources of Pollution Further Resources", " 1 Vector Data Mapping 1.1 Introduction This part will introduce you to the geographic boundaries of our air quality study area, as well as the locations of key pollution and meteorological sensors. We close with a presentation of point source pollution locations, such as factories and other manufacturing facilities. 1.2 Environment Setup The follow packages must be installed on your computer before proceeding. tmap, for flexible thematic mapping sf, for spatial vector data manipulation dplyr, for data.frame manipulation tmap will be set to interactive mode for this tutorial. Let’s library the above packages library(tmap) library(sf) ## Linking to GEOS 3.8.1, GDAL 3.1.4, PROJ 6.3.1 library(dplyr) ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union tmap_mode(&#39;view&#39;) ## tmap mode set to interactive viewing 1.3 County Boundaries The Chicago metropolitan area is the extent of the air quality study area. The study area is centered on Chicago, a major American city of 2.7 million people (2018) located along the Eastern shore of Lake Michigan and within Cook County. However, the study spans a total of 21 counties along Lake Michigan, covering Northeastern Illinois, Northwestern Indiana, and Southern Wisconsin. Included in our study area is the midwestern city of Milwaukee, approximately 100 miles due North of Chicago. The first order of business is to load spatial data. The 21 county boundaries are contained in an ESRI Shapefile ending in .shp. The code snippet below imports the shapefile as an sf object in R. counties = sf::st_read(&#39;./data/LargeAreaCounties/LargeAreaCounties.shp&#39;) Next, the spatial data is plotted using the tmap map making package. Navigate through this interactive map using the mouse and scroll wheel. Counties located in the same state are colored similarly. Hover over a county and a popup will appear displaying the two-letter state abbreviation. Notice how the study area extends to areas beyond just the city boundaries of Chicago. tm_shape(counties) + tm_borders() + tm_text(&quot;COUNTYNAME&quot;, size = 0.7, along.lines = TRUE) + tm_fill(col = &quot;STATE&quot;, alpha = 0.5) 1.4 State Boundaries To produce accurate analyses across the 21 county study area, it is necessary to get an understanding of the geographic context of the large geographic area that our counties are located in. This is due to region-wide weather and pollution patterns impacting local pollution levels observed in our study area. Since our counties are situated in the Midwestern region of the United States, data is actually collected across the four U.S. states of Illinois, Indiana, Michigan, and Wisconsin. As before, data on the geographic boundaires of these four states can be loaded into R as shown below. states = sf::st_read(&#39;./data/FourStates/FourStates.shp&#39;) The code snippet below produces an interactive mapping of the four states. For reference, the original 21 county study area is depicted in grey, at the center of the map. Each state is depicted in a different color for easy differentiation. This map provides an estimate of how much air qualtiy must be collected just to predict pollution levels in our relatively small 21 county study area. tm_shape(states) + tm_borders() + tm_text(&quot;NAME&quot;, size = 0.8, auto.placement = TRUE) + tm_fill(col = &quot;NAME&quot;, alpha = 0.5) + tm_shape(counties) + tm_fill(col = &quot;black&quot;, alpha = 0.25) + tm_borders(col = &quot;black&quot;, alpha = 0.25) 1.5 EPA Particulate Matter Stations Over 127 individual PM2.5 pollution monitoring stations are located across the four state data collection area. These sensors are maintained by state-level environmental protection agencies. Collected data is then reported to the federal Environmental Protection Agency (EPA). Some sensors are digital and provide pollution observations at least every day, and sometimes every hour. However, the vast majority are anolog and only report data every three to twelve days. Analog sensors work by sucking air through a paper wafer designed to trap particular matter. For each observation, a technician removes the wafer from the instrument and places a new one for the next observation. The used wafer is then sent to a lab where it is weighed, creating an estimate of average PM2.5 concentration during the monitoring period. Location information of sensors that were active between 2014 and 2018 are imported into R from a pre-compiled .geojson file. sensors = sf::st_read(&#39;./data/PM25_4States_2014.2018_POINTLOCS.geojson&#39;) %&gt;% dplyr::mutate(rec_duration = as.numeric(lastRec - firstRec)) The map below is identical to the map of the four state area from before, except it now includes the location of EPA sensors. The colored circles count the number of sensors in each local region. Zoom into the map using the scroll wheel to discover the exact location of EPA pollution sensors across the four state area, represented by the EPA logo. Notice how they are primarily concentrated around large urban areas. This is by design, as the EPA’s mission is to monitor pollution levels where people work and live, which also happens to be around cities. The area around Chicago contains approximately 17 sensors while the Milwaukee area has 6 sensors. Clicking on a given sensor reveals details about when the sensor was active. Note that not all sensors were active during the entire five year study period between 2014 and 2018. basemap + tm_shape(sensors) + tm_markers(shape = tmap_icons(&#39;https://github.com/GeoDaCenter/OpenAirQ-toolkit/blob/master/data/assets/EPA_logo.png?raw=true&#39;)) 1.6 Weather Stations Air quality is directly affected by regional and local weather patterns, so a dense network of weather stations is an important tool in our data toolbox. High temporal resolution weather data was sourced from a large network of ground-based weather stations typically located at airports. These sensors form the Automated Surface Observing System (or ASOS) and provide hourly data an many weather characteristics, such as temperature, pressure, wind velocity, and wind direction. ASOS sensors are maintained by a variety of agencies like the National Oceanic and Atmospheric Administration (NOAA), the Federal Aviation Administration (FAA), and the Department of Defense (DoD). Their primary purpose it to provide real-time, accurate weather information for pilots landing or departing at a given airport. Additionally, they serve as primary weather sensors for a variety of NOAA weather models. All sensors are digital, with data being continuously monitored in real-time for errors and inconsistencies. The location of all sensors in the study area were loaded as shown below. asos = sf::st_read(&#39;./data/4States_ASOS_2018_Locations.geojson&#39;) The map below describes the distribution of sensors in the four-state data collection area. There are a total of 249 sensors in the four state area. Similarly to the EPA sensor map, use the scroll wheel to zoom in and the mouse to explore the map. Individual sensors are represented with a blue airplane icon. Clicking on an airport will reveal the three-letter airport identifier and full name for that airport. As these weather sensors are used for aviation purposes, they report data continously throughout the day once installed. basemap + tm_shape(asos) + tm_markers(shape = tmap_icons(&#39;https://github.com/GeoDaCenter/OpenAirQ-toolkit/blob/master/data/assets/airport_icon.png?raw=true&#39;)) 1.7 Point Sources of Pollution The last vector dataset in our toolkit are point source emission locations, sourced from the EPA National Emissions Inventory of 2014. These locations represent known sources of pollution. Locations in this inventory represent various factories, processing plants, heavy industrial installations, powerplants, major HVAC systems, and more. Each location comes with information on the quantity of pollution per year released at the site. However, these number are self-reported by the polluting firm and are likely underestimates of true pollution occuring at that location. We load the location dataset as shown in the code snippet below. Each pollution sourced is represented as a point feature. points.pollution = sf::st_read(&#39;./data/Point_Source_Emissions_4States_2014.geojson&#39;) This map describes the spatial distribution of point source emissions across the four state area. There are over 33,000 individual locations in the inventory. Navigate through this interactive map by clicking on the colored circles to zoom into smaller and smaller areas. Each individual point source emissions is represented by a blue icon. Click on this icon to reveal more information about pollution at that location, including the firm creating the pollution and the amount of pollution generated per year at that site. basemap + tm_shape(points.pollution) + tm_markers() Further Resources You made it to the end of the first chapter, great job! If you would like to learn more about how to source these datsets, see appendix E. Additional resources describing the origin of these datasets is provided below. See the EPA’s air quality information page for more information on air quality measuring in the United States. Refer to Iowa State University for more information on the ASOS weather station network. See the EPA’s National Emissions Inventory homepage for more information on point source emissions. Mor einformation about mapping in R using tmap and other packages can be found in Chapter 8 Making maps with R from Geocomputation with R by Robin Lovelace, Jakub Nowosad, and Jannes Muenchow. "],["03-rastermapping.html", "2 Vector Data Mapping 2.1 Introduction 2.2 Environment Setup 2.3 &lt;Dataset 1&gt; Further Resources", " 2 Vector Data Mapping 2.1 Introduction This part will introduce you to 2.2 Environment Setup The follow packages must be installed on your computer before proceeding. tmap: flexible thematic mapping tmap will be set to interactive mode for this tutorial. Let’s library the above packages. We also set view mode in tmap to create interactive and clickable maps. library(tmap) tmap_mode(&#39;view&#39;) ## tmap mode set to interactive viewing 2.3 &lt;Dataset 1&gt; I am a description and interactive visualoization of the first raster dataset Further Resources Additional resources describing the origin of these datasets is provided below. I am an example resource for more information on a topic. "],["04-ToolkitPointsToSurfaces.html", "3 Point Sensors to Surfaces 3.1 Introduction 3.2 Environment Setup 3.3 Read and Examine the Data 3.4 Data Wrangling 3.5 Creating a Spatial Object using sp 3.6 Data Visualization Review 3.7 Constructing a Kernal Density Estimation Further Resources", " 3 Point Sensors to Surfaces 3.1 Introduction This chapter will introduce how to convert point sensors to surfaces. In it, we will work with the CSV file for the 2017 National Emissions Inventory, downloadable from the EPA’s website here. If you wish to follow along with this chapter, please download the dataset now. If you have a specific interest area and would like to skip the data wrangling in R, you can download state-specific and pollutant-specific summaries from the NEI website. We will begin with a brief review of the basics of data wrangling and filter the relatively large CSV file to the considerably smaller subset of the data with which we are concerned. Then, we will reinforce the data visualization skills covered in a previous chapter by mapping out the point locations of emissions sources in our study area. Finally, we will transition into the process of creating a continuous surface in the form of a Kernal Density Estimation (KDE) of PM2.5 point emission source density in Cook County, Illinois. By the end of this tutorial you will be able to: Understand and wrangle National Emissions Inventory data Use the sp package in R Generate a Kernal Density Estimation using real data 3.2 Environment Setup To process our data and create a Kernal Density Estimation, we will need the following packages: tidyverse, for data wrangling sp, for spatial data manipulation and analysis rgdal, for importing spatial data tmap, for cartography and mapmaking in R spatialEco, for Kernel Density Estimation If you do not already have any of these packages installed, you will want to install them using install.package(\"*PackageName*\"). Once they are installed, we are going to library the required packages: library(tidyverse) library(sp) library(rgdal) library(spatialEco) library(tmap) library(readr) 3.3 Read and Examine the Data Now that we have loaded our required packages, we will now read in our National Emissions Inventory CSV. After unzipping the zipped folder downloaded from the EPA, you will have two files: “process_12345.csv” and “process678910.csv”. For the purposes of this chapter, we will only need “process_12345.csv”. This file is quite large, so beware that it may take 30 seconds to load. nei.data &lt;- readr::read_csv(&quot;./data/process_12345.csv.zip&quot;) Having successfully read our data into the R environment, let’s take a second to examine it. nrow(nei.data) ## [1] 3967781 names(nei.data) ## [1] &quot;epa region code&quot; &quot;state&quot; &quot;fips state code&quot; &quot;tribal name&quot; ## [5] &quot;fips code&quot; &quot;county&quot; &quot;eis facility id&quot; &quot;program system code&quot; ## [9] &quot;agency facility id&quot; &quot;tri facility id&quot; &quot;company name&quot; &quot;site name&quot; ## [13] &quot;naics code&quot; &quot;naics description&quot; &quot;facility source type&quot; &quot;site latitude&quot; ## [17] &quot;site longitude&quot; &quot;address&quot; &quot;city&quot; &quot;zip code&quot; ## [21] &quot;postal abbreviation&quot; &quot;eis unit id&quot; &quot;agency unit id&quot; &quot;unit type&quot; ## [25] &quot;unit description&quot; &quot;design capacity&quot; &quot;design capacity uom&quot; &quot;eis process id&quot; ## [29] &quot;agency process id&quot; &quot;scc&quot; &quot;reg codes&quot; &quot;reg code description&quot; ## [33] &quot;process description&quot; &quot;reporting period&quot; &quot;emissions operating type&quot; &quot;calculation parameter value&quot; ## [37] &quot;calculation parameter uom&quot; &quot;calculation material&quot; &quot;calculation parameter type&quot; &quot;calc data source&quot; ## [41] &quot;calc data year&quot; &quot;pollutant code&quot; &quot;pollutant desc&quot; &quot;pollutant type(s)&quot; ## [45] &quot;total emissions&quot; &quot;emissions uom&quot; &quot;emission factor&quot; &quot;ef numerator uom&quot; ## [49] &quot;ef denominator uom&quot; &quot;ef text&quot; &quot;calc method code&quot; &quot;calculation method&quot; ## [53] &quot;emission comment&quot; &quot;source data set&quot; &quot;data tagged&quot; &quot;data set&quot; As we can see, the dataset is huge, with over 3 million observations and 53 attributes. None of the existing spatial data packages in R are well equipped to handle such a dataset of such size. Luckily, we are only interested in a small subset of the data – PM2.5 emissions sources in Illinois, Michigan, Wisconsin, and Indiana. 3.4 Data Wrangling As a reminder, this dataset contains data for many pollutants across the entire United States. Looking at the code snippet above, we can see that the tibble contains columns for state abbreviations and pollutant descriptions, two factors which we are interested in filtering. First, let’s filter our tibble to only those observations within our state, Illinois. We are going to be using the filter() function from the dplyr package (included in the tidyverse). state.abbr &lt;- c(&quot;IL&quot;) state.nei &lt;- nei.data %&gt;% filter(state %in% state.abbr) nrow(state.nei) ## [1] 511731 With that, we’re already down to just 386,338 emissions sources. While we still have a ways to go with our filtering, this is certainly progress. Let’s take a second to look back over what we just did. The second line of this code is using the pipe (%&gt;%) operator to pipe in the complete nei dataset into the filter function covered in an earlier chapter. %in% is an infix operator that matches the items from the first vector (the complete list of state abbreviations for all point emissions sources) with those of the second (the state abbreviations for state of interest). This code is written this way to allow this code to be used for larger, multistate study areas. If you are interested in examining points from multiple states, simply add their abbreviations to the state.abbr vector. If you are only using one state, feel free to simplify the code to your liking. We are next going to filter our data down further to include only those points within Cook County, IL. county.names &lt;- c(&quot;Cook&quot;) county.nei &lt;- state.nei %&gt;% filter(county %in% county.names) nrow(county.nei) ## [1] 107692 Let’s finish filtering our data by restricting our results to only those emissions sources emitting PM2.5. We will first examine the different labels for pollution descriptions using the unique() function. We will then filter our dataset for only those labels that seem related to PM2.5 using the same process as above. unique(county.nei$`pollutant desc`) ## [1] &quot;Elemental Carbon portion of PM2.5-PRI&quot; ## [2] &quot;Sulfate Portion of PM2.5-PRI&quot; ## [3] &quot;Remaining PMFINE portion of PM2.5-PRI&quot; ## [4] &quot;Organic Carbon portion of PM2.5-PRI&quot; ## [5] &quot;Nitrate portion of PM2.5-PRI&quot; ## [6] &quot;Sulfur Dioxide&quot; ## [7] &quot;PM Condensible&quot; ## [8] &quot;PM10 Filterable&quot; ## [9] &quot;PM10 Primary (Filt + Cond)&quot; ## [10] &quot;Carbon Monoxide&quot; ## [11] &quot;Nitrogen Oxides&quot; ## [12] &quot;PM2.5 Primary (Filt + Cond)&quot; ## [13] &quot;Nitrous Oxide&quot; ## [14] &quot;Volatile Organic Compounds&quot; ## [15] &quot;PM2.5 Filterable&quot; ## [16] &quot;Naphthalene&quot; ## [17] &quot;Ammonia&quot; ## [18] &quot;Acrolein&quot; ## [19] &quot;Toluene&quot; ## [20] &quot;Hexane&quot; ## [21] &quot;Anthracene&quot; ## [22] &quot;Pyrene&quot; ## [23] &quot;Fluorene&quot; ## [24] &quot;Phenanthrene&quot; ## [25] &quot;Acenaphthene&quot; ## [26] &quot;Cobalt&quot; ## [27] &quot;Acetaldehyde&quot; ## [28] &quot;Selenium&quot; ## [29] &quot;Benzo[g,h,i,]Perylene&quot; ## [30] &quot;Indeno[1,2,3-c,d]Pyrene&quot; ## [31] &quot;Benzo[b]Fluoranthene&quot; ## [32] &quot;Fluoranthene&quot; ## [33] &quot;Benzo[k]Fluoranthene&quot; ## [34] &quot;Acenaphthylene&quot; ## [35] &quot;Chrysene&quot; ## [36] &quot;Formaldehyde&quot; ## [37] &quot;Benzo[a]Pyrene&quot; ## [38] &quot;Dibenzo[a,h]Anthracene&quot; ## [39] &quot;Benz[a]Anthracene&quot; ## [40] &quot;Benzene&quot; ## [41] &quot;Manganese&quot; ## [42] &quot;Mercury&quot; ## [43] &quot;Nickel&quot; ## [44] &quot;Arsenic&quot; ## [45] &quot;Beryllium&quot; ## [46] &quot;Cadmium&quot; ## [47] &quot;Chromium (VI)&quot; ## [48] &quot;Chromium III&quot; ## [49] &quot;Glycol Ethers&quot; ## [50] &quot;Xylenes (Mixed Isomers)&quot; ## [51] &quot;Methyl Isobutyl Ketone&quot; ## [52] &quot;Ethyl Benzene&quot; ## [53] &quot;Phenol&quot; ## [54] &quot;2,2,4-Trimethylpentane&quot; ## [55] &quot;Cumene&quot; ## [56] &quot;Hydrochloric Acid&quot; ## [57] &quot;Lead&quot; ## [58] &quot;2-Methylnaphthalene&quot; ## [59] &quot;Methanol&quot; ## [60] &quot;Vinyl Acetate&quot; ## [61] &quot;Tetrachloroethylene&quot; ## [62] &quot;1,2-Epoxybutane&quot; ## [63] &quot;Phosphorus&quot; ## [64] &quot;Ethylene Glycol&quot; ## [65] &quot;Biphenyl&quot; ## [66] &quot;Ethyl Acrylate&quot; ## [67] &quot;Diethanolamine&quot; ## [68] &quot;Styrene&quot; ## [69] &quot;Methyl Methacrylate&quot; ## [70] &quot;Methylene Chloride&quot; ## [71] &quot;Bis(2-Ethylhexyl)Phthalate&quot; ## [72] &quot;Ethylene Oxide&quot; ## [73] &quot;Trichloroethylene&quot; ## [74] &quot;Methyl Tert-Butyl Ether&quot; ## [75] &quot;2-Nitropropane&quot; ## [76] &quot;Propylene Oxide&quot; ## [77] &quot;Hydrogen Sulfide&quot; ## [78] &quot;Hydrogen Cyanide&quot; ## [79] &quot;Dibutyl Phthalate&quot; ## [80] &quot;1,3-Butadiene&quot; ## [81] &quot;m-Xylene&quot; ## [82] &quot;Propionaldehyde&quot; ## [83] &quot;o-Xylene&quot; ## [84] &quot;Phthalic Anhydride&quot; ## [85] &quot;Cyanide&quot; ## [86] &quot;Methyl Chloroform&quot; ## [87] &quot;Antimony&quot; ## [88] &quot;Aniline&quot; ## [89] &quot;Chlorine&quot; ## [90] &quot;Maleic Anhydride&quot; ## [91] &quot;Acrylic Acid&quot; ## [92] &quot;Hydrogen Fluoride&quot; ## [93] &quot;Phosphine&quot; ## [94] &quot;Triethylamine&quot; ## [95] &quot;Propylene Dichloride&quot; ## [96] &quot;Vinylidene Chloride&quot; ## [97] &quot;p-Xylene&quot; ## [98] &quot;Ethylene Dibromide&quot; ## [99] &quot;Ethylene Dichloride&quot; ## [100] &quot;Carbon Tetrachloride&quot; ## [101] &quot;Chloroform&quot; ## [102] &quot;1,1,2,2-Tetrachloroethane&quot; ## [103] &quot;Methyl Bromide&quot; ## [104] &quot;Methyl Chloride&quot; ## [105] &quot;Bromoform&quot; ## [106] &quot;Ethyl Chloride&quot; ## [107] &quot;Carbon Disulfide&quot; ## [108] &quot;Tert-butyl Acetate&quot; ## [109] &quot;PM25-Primary from certain diesel engines&quot; ## [110] &quot;PM10-Primary from certain diesel engines&quot; ## [111] &quot;Carbon Dioxide&quot; ## [112] &quot;Methane&quot; ## [113] &quot;Sulfur Hexafluoride&quot; ## [114] &quot;Vinyl Chloride&quot; ## [115] &quot;2,4-Toluene Diisocyanate&quot; ## [116] &quot;Chlorobenzene&quot; ## [117] &quot;1-Bromopropane&quot; ## [118] &quot;Polycyclic aromatic compounds (includes 25 specific compounds)&quot; ## [119] &quot;Benzyl Chloride&quot; ## [120] &quot;2,4-Dinitrotoluene&quot; ## [121] &quot;Dimethyl Sulfate&quot; ## [122] &quot;Isophorone&quot; ## [123] &quot;2-Chloroacetophenone&quot; ## [124] &quot;Methylhydrazine&quot; ## [125] &quot;Acetophenone&quot; ## [126] &quot;5-Methylchrysene&quot; ## [127] &quot;Epichlorohydrin&quot; ## [128] &quot;4,4-Methylenediphenyl Diisocyanate&quot; ## [129] &quot;o-Toluidine&quot; ## [130] &quot;Acetonitrile&quot; ## [131] &quot;Trifluralin&quot; ## [132] &quot;2,4-Dichlorophenoxy Acetic Acid&quot; ## [133] &quot;1,4-Dichlorobenzene&quot; ## [134] &quot;p-Dioxane&quot; ## [135] &quot;Ethylidene Dichloride&quot; ## [136] &quot;Acrylonitrile&quot; ## [137] &quot;Carbonyl Sulfide&quot; ## [138] &quot;Cresol/Cresylic Acid (Mixed Isomers)&quot; ## [139] &quot;Dibenzofuran&quot; ## [140] &quot;Acrylamide&quot; ## [141] &quot;Hydroquinone&quot; ## [142] &quot;Quinoline&quot; ## [143] &quot;Quinone&quot; pm25.names &lt;- c(&quot;PM2.5 Filterable&quot;, &quot;PM2.5 Primary (Filt + Cond)&quot;) county.pm25 &lt;- county.nei %&gt;% filter(`pollutant desc` %in% pm25.names) nrow(county.pm25) ## [1] 8326 Now, with a manageable number of observations in our area of interest, we are going to start looking at our data spatially. 3.5 Creating a Spatial Object using sp We first want to use our filtered tibble to create an sp Spatial Points object. #Assign the proper coordinates coordinates(county.pm25) &lt;- county.pm25[,c(&quot;site longitude&quot;,&quot;site latitude&quot;)] #Assign the proper projection for this data source (EPSG 4326) proj4string(county.pm25) &lt;- CRS(&quot;+init=epsg:4326&quot;) #Check data with basic plot plot(county.pm25) With everything looking as it should, let’s look back on what we just did. We initialized the Spatial Points object using the coordinates() function, assigning the proper longitude and latitude from the dataset. We then used the proj4string() function to assign the correct Coordinate Reference System (CRS) to our data. Be careful not to use the wrong projection (check your data source). If you need to transform the projection of your dataset, use the spTransform() function. Let’s now briefly review data visualization with the tmap package using this point data. 3.6 Data Visualization Review Here, we will use the spatial data visualization skills learned in an earlier chapter to visualize the point locations of PM2.5 sources in Cook County. #Read in Cook County Shapefile using sp&#39;s readOGR function cook.county &lt;- readOGR(&quot;./data/CookCounty.geojson&quot;) #Check projection proj4string(cook.county) ## [1] &quot;+proj=tmerc +lat_0=36.6666666666667 +lon_0=-88.3333333333333 +k=0.999975 +x_0=300000 +y_0=0 +datum=NAD83 +units=us-ft +no_defs&quot; #Create tmap plot tm_shape(cook.county) + tm_borders() + tm_shape(county.pm25) + tm_dots() This is clearly a very basic plot of the data. We can get a basic idea of where the point density may be highest, but we cannot tell much else about the data. Let’s now create an interactive map with the dots colored and sized based off of the volume of emissions (self-reported) given off at each point location. #Set tmap mode to view tmap_mode(&quot;view&quot;) tm_shape(cook.county) + tm_borders() + tm_shape(county.pm25) + tm_bubbles(col = &quot;total emissions&quot;, alpha = 0.3, size = &quot;total emissions&quot;, style = &quot;fisher&quot;) Here, we used the tmap_mode() function to change the map style to interactive viewing and changed the arguments of the tm_bubbles() function to change the appearance of the point locations. Let’s now construct a continuous surface Kernal Density Estimation from our point data. 3.7 Constructing a Kernal Density Estimation A Kernal Density Estimation (KDE) map at its most basic level is, as the name suggests, a means of representing the density of features over a given area. The term heatmap is often used interchangeably with KDE. Constructing a KDE gives us a continuous surface from discrete point data. This is quite useful as both an end product or an input to a model that requires continuous surface inputs. Each cell of the constructed raster is assigned a value based on the estimated density of ponits in that part of the map. This value can either be entirely unweighted (based solely on the number of points in an area) or weighted on a given variable (points with higher values for that variable will make an area appear denser). There are countless online resources available for learning more about the mathematics/history of KDE. Let’s now create our first KDE from the point data we’ve been using. We are going to be using the sp.kde() function from the spatialEco package, however there are several other R packages that achieve a more or less identical outcome. #Construct KDE county.kde &lt;- sp.kde(county.pm25, nr=500, nc=500) plot(county.kde) We’ve now produced a continuous surface representing the density of PM2.5 emissions sources across Cook County. Let’s look over the sp.kde() function in a little more detail. In addition to inputting our sp object, we also input values of 500 for the nr and nc arguments. These abbreviations are short for “number of rows” and “number of columns” respectively. The sp.kde function creates a grid on which to map the results of the KDE, and these arguments tell the function what the dimensions of this grid should be. Let’s look at how changing these two arguments changes the appearance of our KDE map: #10x10 grid county.kde.10 &lt;- sp.kde(county.pm25, nr=10, nc=10) plot(county.kde.10) #100x100 grid county.kde.100 &lt;- sp.kde(county.pm25, nr=100, nc=100) plot(county.kde.100) #500x500 grid county.kde.500 &lt;- sp.kde(county.pm25, nr=500, nc=500) plot(county.kde.500) Note the changes in the plots as the resolution of the grid is increased. Let’s now look at the y argument used to add a weight to the KDE. Let’s say we wanted to weigh our KDE based on the amount of total emissions from individual sites. Here’s how you would do that: #Construct weighted KDE county.kde.weighted &lt;- sp.kde(county.pm25, y=county.pm25$`total emissions`, nr=500, nc=500) plot(county.kde.weighted) As you can see, weighing the KDE on the total emissions amount dramatically changes the map. These changes can be deemphasized/accentuated if you transform the variable weighing the data. If you are interested in reading more about the arguments of this function check out its R Documentation page. Further Resources You made it to the end of the first chapter, great job! If you would like to learn more about how to source these datsets, see appendix E. Additional resources describing the origin of these datasets is provided below. (EXAMPLE RESOURCE) See the EPA’s air quality information page for more information on air quality measuring in the United States. "],["05-interpmodels.html", "4 Interpolation Models 4.1 Introduction 4.2 Environment Setup 4.3 Importing Temperature Sensor Data 4.4 Interpolating Temperature 4.5 Method Selection Criteria Further Reading", " 4 Interpolation Models 4.1 Introduction Aerosol Optical Depth (AOD) data is remotely sensed using satellite technology. AOD values are estimated in a 1km x 1km grid pattern, where each grid cell is assigned an AOD value. AOD can estimate PM2.5 pollution concentrations at the surface after accounting for variables such as temperature, pressure, and others. Therefore, estimates of temperature, pressure, and others are also required for each 1km x 1km AOD grid cell. But, weather data is only measured at the location of a ground-based sensor. A spatial interpolation technique is required to infer the temperatures in grid cells without weather sensors. The map below shows the location of temperature sensors and average summer temperature at each sensor location. The grey areas of the map represent areas where temperature must be inferred. Figure 4.1: Temperature Sensor Locations (Lorenz Menendez) The following tutorial provides examples of common spatial interpolation methods used in the spatial data analysis community. While some methods are more complex and computationally expensive, all methods take into account the location and recorded values at various point sensor locations to generate an estimate at the unknown locations. This tutorial follows the steps provided in an R-Spatial tutorial on pollution variable interpolations. Interpolation models tested include voronoi polygons, nearest neighbor interpolation, inverse distance weighting (IDW), and finally spatial kriging. Don’t worry if you are unfamiliar with the methods, as they will be explained throughout the tutorial. 4.2 Environment Setup The following packages must be installed on your computer before proceeding readr, for ultra-fast CSV data reading dyplr, for data frame manipulation sf, for vector data manipulation sp, for vector &amp; raster data manipulation tmap, for cartography raster, for raster data manipulation dismo, for voronoi interpolation gstat, for spatio-statistical interpolation automap, for kriging interpolation Let’s library the above packages library(readr) library(dplyr) library(sf) library(sp) library(tmap) library(raster) library(dismo) library(gstat) library(automap) tmap_mode(&#39;view&#39;) 4.3 Importing Temperature Sensor Data Temperature data is the variable to be interpolated in this tutorial. In the code below, avergae monthly temperature for August 2018 at each sensor location is loaded into R. Other weather variables were also interpolated using similar methods, but these are omitted for brevity. For more information on compiling a weather dataset, refer to Appendix A. tmpf = readr::read_csv(&#39;./data/ASOS_tmpf_2014.2018.csv&#39;) %&gt;% filter(moyr == &quot;2018-08&quot;) tmpf ## # A tibble: 24 x 5 ## moyr site_num latitude longitude tmpf ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2018-08 06C 42.0 -88.1 72.0 ## 2 2018-08 57C 42.8 -88.4 71.2 ## 3 2018-08 ARR 41.8 -88.5 72.4 ## 4 2018-08 BUU 42.7 -88.3 70.1 ## 5 2018-08 C09 41.4 -88.4 73.1 ## 6 2018-08 DPA 41.9 -88.2 72.8 ## 7 2018-08 ENW 42.6 -87.9 70.9 ## 8 2018-08 IGQ 41.5 -87.5 73.5 ## 9 2018-08 IKK 41.1 -87.8 74.1 ## 10 2018-08 JOT 41.5 -88.2 75.4 ## # … with 14 more rows The three-letter airport identifier is in the site_num column, while spatial data information is housed as coordinates in the latitude and longitude columns. Monthly average temperature is in the tmpf column. Next, the imported data.frame is converted to a spatial sf object and plotted in an interactive map using tmap. Notice how temperatures in the northern counties tend to be significantly colder than temperaturs in the southern counties. counties = sf::st_read(&#39;./data/LargeAreaCounties/LargeAreaCounties.shp&#39;) sensors = sf::st_as_sf(tmpf, coords = c(&quot;longitude&quot;, &quot;latitude&quot;), crs = 4326) tm_shape(counties) + tm_borders() + tm_shape(sensors) + tm_dots(col = &quot;tmpf&quot;, palette = &quot;-RdBu&quot;, title = &quot;Average August 2018 Temperature (ºF)&quot;, popup.vars = c(&quot;Temp&quot; = &quot;tmpf&quot;, &quot;Airport Code&quot; = &quot;site_num&quot;)) 4.4 Interpolating Temperature In the following sections, a variety of interpolation models are applied to monthly temperature averages. The interpolation methods are ordered by their complexity and sophistication, with the simplest methods first. Voronoi Tesselation Voronoi tesselation, also known as Thiessen polygons, is an interpolation method whereby grid cells are assigned the temperature value of the nearest temperature sensor. First, the Thiessen polygons are created using the dismo package. The map below shows the area assigned to each sensor location, which are marked by black dots. library(dismo) v &lt;- voronoi(dsp) ## Loading required namespace: deldir tm_shape(v) + tm_borders() + tm_shape(sensors) + tm_dots(popup.vars = c(&quot;Temp&quot; = &quot;tmpf&quot;, &quot;Airport Code&quot; = &quot;site_num&quot;)) The Thiessen polygons are converted from vector polygons to raster data, and the corresponding temperature values are assigned. # Assigning values to polygons il &lt;- aggregate(IL) vil &lt;- raster::intersect(v, il) tm_shape(vil) + tm_fill(&#39;tmpf&#39;, alpha = 0.5, palette = &quot;-RdBu&quot;, title = &quot;Average August 2018 Temperature (ºF)&quot;, popup.vars = c(&quot;Temp&quot; = &quot;tmpf&quot;)) + tm_borders() r &lt;- blank.raster vr &lt;- rasterize(vil, r, &#39;tmpf&#39;) Nearest Neighbor Similar to the Voronoi, this method assigns temperature values by taking the average of a fixed number of nearby temperature sensors. Note that a nearest neighbor interpolation with the number of neighbors set to 1 is equivalent to a Voronoi tesselation. The method coded takes into account the five nearest temperature sensors. set.seed(5132015) gs &lt;- gstat(formula=tmpf~1, locations=dsp, nmax=5, set=list(idp = 0)) nn &lt;- interpolate(r, gs) ## [inverse distance weighted interpolation] nnmsk &lt;- mask(nn, vr) tm_shape(nnmsk) + tm_raster(n = 5, alpha = 0.5, palette = &quot;-RdBu&quot;, title = &quot;Average August 2018 Temperature (ºF)&quot;) Inverse Distance Weighted This model extends the nearest neighbor interpolation by not only taking into account multiple sensors, but also the relative distance between grid cell and sensors. Temperature values from sensors located closer to the grid cell are given more influence (higher weights) on the assigned tempertaure value than sensors located further away from the grid cell (lower weights). This model is a logical outcropping of Tobler’s first law of geography, “everything is related to everything else, but nearer things are more related than distant things.” set.seed(5132015) gs &lt;- gstat(formula=tmpf~1, locations=dsp) idw &lt;- interpolate(r, gs) idwr &lt;- mask(idw, vr) tm_shape(idwr) + tm_raster(n = 10, alpha = 0.5, palette = &quot;-RdBu&quot;, title = &quot;Average August 2018 Temperature (ºF)&quot;) The IDW model creates a smoother temperature surface compared to voronoi and nearest neighbor interpolations. Hard breaks between individual sensor regions is reduced to a minimum. However, IDW also introduced its own distortion, the ‘bullseye’. This effect occurs when a sensor value is significantly different than the rest, an artefact that is clearly visible around almost all snesor locations in our map. IDW Optimiziation IDW interpolations depend on two user-defined parameters, the number of sensors to consider, and a friction of distance coefficient describing how strong closer senors should be weighted relative to farther ones. Using an error metric such as Root-Mean Square Error (or RMSE), optimal parameters that minimize RMSE could be found. This optimization is performed below. f1 &lt;- function(x, test, train) { nmx &lt;- x[1] idp &lt;- x[2] if (nmx &lt; 1) return(Inf) if (idp &lt; .001) return(Inf) m &lt;- gstat(formula=tmpf~1, locations=train, nmax=nmx, set=list(idp=idp)) p &lt;- predict(m, newdata=test, debug.level=0)$var1.pred RMSE(test$tmpf, p) } i &lt;- sample(nrow(dsp), 0.2 * nrow(dsp)) tst &lt;- dsp[i,] trn &lt;- dsp[-i,] opt &lt;- optim(c(8, .5), f1, test=tst, train=trn) opt ## $par ## [1] 8.615118 2.028688 ## ## $value ## [1] 1.561507 ## ## $counts ## function gradient ## 39 NA ## ## $convergence ## [1] 0 ## ## $message ## NULL The optimal IDW interpolation can be gleaned from the opt$par variable. The number of sensors to consider should be ~4.90 while the decay parameter should be 8.44. Performing the IDW interpolation with these parameters yields the following results. m &lt;- gstat::gstat(formula=tmpf~1, locations=dsp, nmax=opt$par[1], set=list(idp=opt$par[2])) idw &lt;- interpolate(r, m) idw &lt;- mask(idw, il) tm_shape(idw) + tm_raster(n = 10, alpha = 0.5, palette = &quot;-RdBu&quot;, title = &quot;Average August 2018 Temperature (ºF)&quot;) The output from this method is super interesting. It looks similar to the voronoi polygons, but as if someone took a paintbrush to the edges of each polygon and mixed the colors together. In scientific terms, it’s as if someone took the voronoi polygons and added a thin gradient between each polygon. Ordinary Kriging Kriging is a complex interpolation method that seeks to find the best linear predictor of average temperature for unknown grid cells. The first step is to fit a variogram over the temperature data library(gstat) gs &lt;- gstat(formula=tmpf~1, locations=dsp) v &lt;- variogram(gs, width=20) head(v) ## np dist gamma dir.hor dir.ver id ## 1 7 14.82348 1.349970 0 0 var1 ## 2 31 31.27996 3.546520 0 0 var1 ## 3 35 49.86338 2.841576 0 0 var1 ## 4 31 70.06044 3.377242 0 0 var1 ## 5 18 88.05822 3.862724 0 0 var1 plot(v) We notice that there are only five points below the mean, which is very few points to properly fit a model to. But, let’s continue to see what happens. Next we fit the variogram. This time, we use the autofitVarogram function from the automap package. fve = automap:::autofitVariogram(formula = tmpf~1, input_data = dsp) fve ## $exp_var ## np dist gamma dir.hor dir.ver id ## 1 14 18.36815 2.110076 0 0 var1 ## 2 12 30.29441 4.746482 0 0 var1 ## 3 30 41.75700 3.110922 0 0 var1 ## 4 24 57.31836 2.323814 0 0 var1 ## 5 22 71.91136 3.795503 0 0 var1 ## 6 25 89.30900 3.669922 0 0 var1 ## ## $var_model ## model psill range ## 1 Nug 0.000000 0.00000 ## 2 Gau 3.531433 17.77443 ## ## $sserr ## [1] 0.04157277 ## ## attr(,&quot;class&quot;) ## [1] &quot;autofitVariogram&quot; &quot;list&quot; plot(fve) The autofitVariogram function fitted a Gaussian variogram to our small sample of datapoints. Executing an ordiary kriging model kp = krige(tmpf~1, dsp, as(blank.raster, &#39;SpatialGrid&#39;), model=fve$var_model) spplot(kp) Plotting this on the 21 counties ok &lt;- brick(kp) ok &lt;- mask(ok, il) names(ok) &lt;- c(&#39;prediction&#39;, &#39;variance&#39;) plot(ok) tm_shape(ok[[1]]) + tm_raster(n = 5, alpha = 0.5, palette = &quot;-RdBu&quot;, title = &quot;Average August 2018 Temperature (ºF)&quot;) 4.5 Method Selection Criteria After examining the reuslts of each method, a single one must be chosen. Root Mean Square Error (or RMSE) was chosen as a performance metric to compare each method. The method with the lowest RMSE was chosen for the final interpolation. RMSE is calculated using the following formula. \\(predicted\\) is the predicted temperature value by the interpolation method at a sensor location. \\(observed\\) is the true temperature value recorded at the sensor location. The difference between \\(predicted\\) and \\(observed\\) is then squared. The mean of the squared errors across multiple sensor locations is taken, then the square root. The formula is saved as an R function named RMSE, as shown in the code snippet below. \\[RMSE = \\sqrt{mean((predicted - observed)^2)}\\] RMSE &lt;- function(observed, predicted) { sqrt(mean((predicted - observed)^2, na.rm=TRUE)) } Before choosing any model, it must first be proved than the interpolation method effectively captures the variability in the dataset. To do this, we first calculate RMSE where the predicted temperature value is the mean of all temperature values in the dataset. Formally, we are calculating the RMSE of the null model, where there is no change in termperature over space. RMSE &lt;- function(observed, predicted) { sqrt(mean((predicted - observed)^2, na.rm=TRUE)) } null &lt;- RMSE(mean(tmpf$tmpf), tmpf$tmpf) null ## [1] 2.098343 The Root Mean Square Error (RMSE) for the null model is 2.09. When RMSE of an interpolation model is below 2.09, this could be an indication that the model is unable to effectively capture the variability in the original temperature dataset and does not provide any improvement in our understanding of temperature across the 21 counties. 4.5.1 Calculating RMSE This section demonstrates how to calculate RMSE for each method applied above. The first step is to randomly partition the sensors into give groups, numbered 1 through 5. Then, for each group, the interpolation method is applied using all sensors outside the group. Then, RMSE is calculated for sensors in the chosen group. For example, when calculating RMSE for group 1, the interpolation method is applied using data from sensosrs in groups 2,3,4,5. Then, RMSE is calculated using the predicted values at the locations of sensors in group 1. This process is repeated for all groups 1 through 5. This methodology is called cross-validation, and since five groups are used one can say, five-fold cross-validation. The number of groups is set at five due to the limited number of available sensors. The following code block is an implementation of five-fold cross-validation for the Voronoi Tesselation method. Note that the set.seed() function allows to keep the same random group selection each time you run the code block. The code outputs the resulting RMSE and a percent improvement over the RMSE of the null model. set.seed(5132015) # Randomly partition the Dataset into 5 groups (1 through 5) kf &lt;- kfold(nrow(dsp)) # Initialize a vector of length 5 vorrmse &lt;- rep(NA, 5) # Validate for (k in 1:5) { test &lt;- dsp[kf == k, ] # Learn on group k train &lt;- dsp[kf != k, ] # Train on groups != k v &lt;- voronoi(train) p1 &lt;- raster::extract(v, test)$tmpf vorrmse[k] &lt;- RMSE(test$tmpf, p1) # Save the RMSE } print(paste(&quot;Mean RMSE&quot;, mean(vorrmse), &quot;Improvement over NULL model&quot;, 1 - (mean(vorrmse) / null))) ## [1] &quot;Mean RMSE 1.33948660183267 Improvement over NULL model 0.361645442293137&quot; Code snippets of five-fold cross-validation for the other interpolations are below. # Nearest Neighbor nnrmse &lt;- rep(NA, 5) for (k in 1:5) { test &lt;- dsp[kf == k, ] train &lt;- dsp[kf != k, ] gs &lt;- gstat(formula=tmpf~1, locations=train) p &lt;- predict(gs, test) nnrmse[k] &lt;- RMSE(test$tmpf, p$var1.pred) } ## [inverse distance weighted interpolation] ## [inverse distance weighted interpolation] ## [inverse distance weighted interpolation] ## [inverse distance weighted interpolation] ## [inverse distance weighted interpolation] print(paste(&quot;Mean RMSE&quot;, mean(nnrmse), &quot;Improvement over NULL model&quot;, 1 - (mean(nnrmse) / null))) ## [1] &quot;Mean RMSE 1.52257896316905 Improvement over NULL model 0.274389740608266&quot; # Inverse Distance Weighted idwrmse &lt;- rep(NA, 5) for (k in 1:5) { test &lt;- dsp[kf == k, ] train &lt;- dsp[kf != k, ] m &lt;- gstat(formula=tmpf~1, locations=train, nmax=opt$par[1], set=list(idp=opt$par[2])) p4 &lt;- predict(m, test)$var1.pred idwrmse[k] &lt;- RMSE(test$tmpf, p4) } ## [inverse distance weighted interpolation] ## [inverse distance weighted interpolation] ## [inverse distance weighted interpolation] ## [inverse distance weighted interpolation] ## [inverse distance weighted interpolation] print(paste(&quot;Mean RMSE&quot;, mean(idwrmse), &quot;Improvement over NULL model&quot;, 1 - (mean(idwrmse) / null))) ## [1] &quot;Mean RMSE 1.5127365156469 Improvement over NULL model 0.279080322228238&quot; # Ordinary Kriging krigrmse = rep(NA, 5) for (i in 1:5) { test &lt;- dsp[kf == i,] train &lt;- dsp[kf != i, ] fve = automap:::autofitVariogram(formula = tmpf~1, input_data = train) kp = krige(tmpf~1, train, as(blank.raster, &#39;SpatialGrid&#39;), model=fve$var_model) p6 = raster::extract(as(kp, &#39;RasterLayer&#39;), test) krigrmse[i] &lt;- RMSE(test$tmpf, p6) } ## [using ordinary kriging] ## [using ordinary kriging] ## [using ordinary kriging] ## [using ordinary kriging] ## [using ordinary kriging] print(paste(&quot;Mean RMSE&quot;, mean(krigrmse), &quot;Improvement over NULL model&quot;, 1 - (mean(krigrmse) / null))) ## [1] &quot;Mean RMSE 1.80110959511671 Improvement over NULL model 0.141651348061827&quot; Below is a summary of RMSE and imporvement over the null for the four interpolation methods studied. Method RMSE Improvement over Null Voronoi Polygon 1.34 36.16% Nearest Neighbor 1.52 27.44% Inverse Distance Weighted 1.43 31.93% Ordinary Kriging 1.80 14.17% Further Reading See this R-Spatial tutorial for an application of this metholdogy on pollution sensor data in California. See this appendix from Intro to GIS and Spatial Analysis by Manual Gimond "],["09-sensor2raster.html", "5 Merging Satellite and Point Sensor Data 5.1 Generate Rasters 5.2 Export to CSV", " 5 Merging Satellite and Point Sensor Data There’s a fundamental challenge in our project to merge AOD data and predictor variables because the data capture techniques are very different. The satellite-based AOD data is measured continuously across the surface of the earth on a 1km by 1km grid system. Meanwhile, sensor data is only captured locally at the sensor location. Therefore, a method to interpolate local sensor data to generate a continuous surface of data is required. An ‘Optimized’ IDW interpolation was used to estimate sensor values across a 1km by 1km grid system. This method takes into account the sensor locations and value for each variable to estimate values in grid cells without a sensor based on a linear interpolation of nearby sensor values. The specific number of sensors to take into account and the distance decay power function were optimized by medinimizing the RMSE (Error). This method was adapted from an RSpatial Tutorial on IDW interpolation with pollution and weather data. To simplify implementation and replication, the entire workflow was coded in R and bundled into a packaged named sensor2raster. The next sections demonstrate how to apply this package to sensor data. # devtools::install_local(&#39;../data/sensor2raster_0.4.tar.gz&#39;) library(sensor2raster, quietly = TRUE) 5.1 Generate Rasters Creating raster surfaces is easy using the sensor2raster function. This function takes the raw output from the riem or aqsr packages and identifies the sensor locations and data values to interpolate. The underlying IDW interpolation is performed by the gstat package. The code chunk below demonstrates how to take ASOS weather data and convert it to a Raster format. The weather data variable is a data frame containing temperature data measured at airports in the Greater Chicago and Milwaukee Areas in 2018. We also pass the AOD.grid, a RasterLayer object representing the grid cells where we want to predict temperature. These grid cells correspond exactly to the pixels of satellite AOD data. temp.rasters = sensor2raster(sensor.data = weather.data, # Input the raw data.frame from riem data.type = &#39;ASOS&#39;, # Specify data type (&#39;ASOS&#39; or &#39;EPA&#39;) reference.grid = AOD.grid, # Grid to interpolate over subvariable = &#39;tmpf&#39;) # Column name of variable to interpolate 5.2 Export to CSV While Raster data is helpful for spatial data analysis and geovisualizations, it is sometimes helpful to store the interpolation in a non-spatial format. The grid2csv function allows you to convert Raster data to CSV either cell-by-cell, or by aggregating to a vector geometry. The exported data.frame is halfway between Long and Wide format due to the 3-dimensional nature of our data. The table below describes how these CSVs are structured in the cell-by-cell case. Var_Name Raster.Cell M1.2018 M2.2018 … M12.2018 Monthly_Temperature 1 23 25 … 20 Monthly_Temperature 2 23 25 … 20 … … … … … … Monthly_Temperature 100 10 15 … 11 The length of the table equals the number of cells in the RasterStack. Each cell is given a unique identifier stored in the Raster.Cell column. The Var_Name colums represent the variable of interest. When there are multiple variables, they are row binded together, giving a long table format. The rest of the columns represent the names given to the layers within each RasterStack. Im this case, each column represents a month and year combination. Additional time periods are appended column-wise to the table. The following table describes the outputted data frame from the monthly temperature Rasters generated earlier. temp.export = grid2csv(rasters = list(temp.rasters[[1]]), var.names = &#39;Monthly_Temperature&#39;) Var_Name Raster.Cell M1.2018 M2.2018 M3.2018 M4.2018 M5.2018 M6.2018 M7.2018 M8.2018 M9.2018 M10.2018 M11.2018 M12.2018 Monthly_Temperature 1 22.75343 24.89301 32.54753 36.13429 59.56493 64.5328 70.55568 70.15553 62.38351 48.01338 30.81887 29.27243 Monthly_Temperature 2 22.75343 24.89301 32.54753 36.13429 59.56493 64.5328 70.55568 70.15553 62.38351 48.01338 30.81887 29.27243 Monthly_Temperature 3 22.75343 24.89301 32.54753 36.13429 59.56493 64.5328 70.55568 70.15553 62.38351 48.01338 30.81887 29.27243 Monthly_Temperature 4 22.75343 24.89301 32.54753 36.13429 59.56493 64.5328 70.55568 70.15553 62.38351 48.01338 30.81887 29.27243 Monthly_Temperature 5 22.75343 24.89301 32.54753 36.13429 59.56493 64.5328 70.55568 70.15553 62.38351 48.01338 30.81887 29.27243 Monthly_Temperature 6 22.75343 24.89301 32.54753 36.13429 59.56493 64.5328 70.55568 70.15553 62.38351 48.01338 30.81887 29.27243 The format of the table changes slightly when subsetting using a vecor object. A new column sj_join appears in the table, representing a unique allowing the table to be joined back to the origin sf object if needed. When subsetting by point features, the Raster_Cell column describes the cell that overlapped with each point feature. When subsettting by line or polygon features, the Raster_Cell column describes the cell that overlapped with the centroid of each geometry. temp.export.sf = grid2csv(rasters = list(temp.rasters[[1]]), var.names = &#39;Monthly_Temperature&#39;, sf.obj = counties) Var_Name Raster_Cell sf_join M1.2018 M2.2018 21 Monthly_Temperature 4745 21 22.74704 24.89580 3 Monthly_Temperature 4970 3 23.00074 25.13390 8 Monthly_Temperature 11038 8 23.88734 26.08940 20 Monthly_Temperature 12957 20 23.22283 25.46473 19 Monthly_Temperature 15155 19 24.72884 26.94046 4 Monthly_Temperature 21000 4 24.81107 27.53006 The code chunk below demonstrates how to exploit the sf_join field to join the table data back to the spatial sf object. # counties.join = counties %&gt;% tibble::rowid_to_column() %&gt;% dplyr::rename(sf_join = rowid) # # counties.join = grid2csv(rasters = list(temp.rasters[[1]]), var.names = &#39;Monthly_Temperature&#39;, sf.obj = counties.join) %&gt;% # left_join(counties.join) %&gt;% # st_as_sf() "],["A-weatherdata.html", "Appendix A: Pollution &amp; Weather 5.3 EPA Pollution Data 5.4 FAA Weather Data", " Appendix A: Pollution &amp; Weather This project uses weather and pollution data from remotely sensed satellite imagery, but also ground based sensors maintained by the EPA and FAA to model air quality in the Midwest. Using the ground sensors, the team can attempt to predict pollutions levels based on satellite data. This chapter focuses on how weather and pollution data from ground sensors was downloaded and prepared for use in refining the prediction. 5.3 EPA Pollution Data Figure 5.1: EPA Pollution Monitoring Site (EPA.gov) EPA data was seamlessly imported into R using the aqsr package by Joshua P. Keller at Colorado State University. The package takes advanatge of the EPA AQS DataMart API to load data in R as data.frame objects with only a couple lines of code. It allows users to query for sensor data across multiple air quality variables, geographies, and timeframes. Let’s get started by downloading the package. # devtools::install_github(&quot;jpkeller/aqsr&quot;) library(aqsr) 5.3.1 Getting Started This section describes the process for querying EPA sensor data using the aqsr package. For more information on how each function works, please reference the package documentation. Obtaining an API Key For first time users of the AQS DataMart API, you must first register your email to recieve an API key. (Users who already have a DataMart API key, please skep to the next step). The API key is a required input for all querying functions in the aqsr package. Obtaining a key is made simple by calling the aqs_signup() function and inputting your own email address. aqs_signup(&#39;YourEmailHere@uchicago.edu&#39;) Save your API key from the email confirmation for future reference. In case you don’t recieve an email, verify that your email address was typed correctly, and check your spam folder. 5.3.1.1 Using your API Key in aqsr Setup your AQI key with the aqr package by using the create_user() function. This way, you won’t have to keep typing your email and API key each time you query for data. myuser = create_user(email = &#39;YourEmailHere@uchicago.edu&#39;, key = &#39;apikey123&#39;) 5.3.2 PM2.5 Data Query This section describes how to query for PM2.5 concetration data from EPA pollution sensors. We are looking for at PM2.5 data Wisconsin, Illinois, and Indiana between 2014 and 2018 for our project. First, let’s start small and query only for Illinois data for the first week of 2018. IL.data = aqs_dailyData_byState(aqs_user = myuser, # Previously defined user emailand API key param = 88101, # EPA AQS Parameter Code for PM2.5 bdate = &quot;20180101&quot;, # Starting Date (Jan 1st ,2018) edate = &quot;20180107&quot;, # Ending Date (Jan 7th, 2018) state = &quot;17&quot;) # State FIPS Code for Illinois state_code county_code site_number parameter_code poc latitude longitude datum parameter sample_duration 2 17 157 0001 88101 3 38.17628 -89.78846 WGS84 PM2.5 - Local Conditions 1 HOUR 2100 17 157 0001 88101 3 38.17628 -89.78846 WGS84 PM2.5 - Local Conditions 1 HOUR 3 17 157 0001 88101 3 38.17628 -89.78846 WGS84 PM2.5 - Local Conditions 1 HOUR 4 17 157 0001 88101 3 38.17628 -89.78846 WGS84 PM2.5 - Local Conditions 1 HOUR 5 17 157 0001 88101 3 38.17628 -89.78846 WGS84 PM2.5 - Local Conditions 1 HOUR ## state_code county_code site_number parameter_code poc latitude longitude datum parameter sample_duration ## 2 17 157 0001 88101 3 38.17628 -89.78846 WGS84 PM2.5 - Local Conditions 1 HOUR ## 2100 17 157 0001 88101 3 38.17628 -89.78846 WGS84 PM2.5 - Local Conditions 1 HOUR ## 3 17 157 0001 88101 3 38.17628 -89.78846 WGS84 PM2.5 - Local Conditions 1 HOUR ## 4 17 157 0001 88101 3 38.17628 -89.78846 WGS84 PM2.5 - Local Conditions 1 HOUR ## 5 17 157 0001 88101 3 38.17628 -89.78846 WGS84 PM2.5 - Local Conditions 1 HOUR ## 6 17 157 0001 88101 3 38.17628 -89.78846 WGS84 PM2.5 - Local Conditions 1 HOUR ## 7 17 157 0001 88101 3 38.17628 -89.78846 WGS84 PM2.5 - Local Conditions 1 HOUR ## 8 17 167 0012 88101 3 39.83192 -89.64416 WGS84 PM2.5 - Local Conditions 1 HOUR ## 9 17 167 0012 88101 3 39.83192 -89.64416 WGS84 PM2.5 - Local Conditions 1 HOUR ## 10 17 167 0012 88101 3 39.83192 -89.64416 WGS84 PM2.5 - Local Conditions 1 HOUR ## 11 17 167 0012 88101 3 39.83192 -89.64416 WGS84 PM2.5 - Local Conditions 1 HOUR ## 12 17 167 0012 88101 3 39.83192 -89.64416 WGS84 PM2.5 - Local Conditions 1 HOUR ## 13 17 167 0012 88101 3 39.83192 -89.64416 WGS84 PM2.5 - Local Conditions 1 HOUR ## 14 17 167 0012 88101 3 39.83192 -89.64416 WGS84 PM2.5 - Local Conditions 1 HOUR ## 15 17 065 0002 88101 3 38.08216 -88.62494 WGS84 PM2.5 - Local Conditions 1 HOUR ## 16 17 065 0002 88101 3 38.08216 -88.62494 WGS84 PM2.5 - Local Conditions 1 HOUR ## 17 17 065 0002 88101 3 38.08216 -88.62494 WGS84 PM2.5 - Local Conditions 1 HOUR ## 18 17 065 0002 88101 3 38.08216 -88.62494 WGS84 PM2.5 - Local Conditions 1 HOUR ## 19 17 065 0002 88101 3 38.08216 -88.62494 WGS84 PM2.5 - Local Conditions 1 HOUR ## 20 17 065 0002 88101 3 38.08216 -88.62494 WGS84 PM2.5 - Local Conditions 1 HOUR ## 21 17 065 0002 88101 3 38.08216 -88.62494 WGS84 PM2.5 - Local Conditions 1 HOUR ## 22 17 161 3002 88101 3 41.51473 -90.51735 WGS84 PM2.5 - Local Conditions 1 HOUR ## 23 17 161 3002 88101 3 41.51473 -90.51735 WGS84 PM2.5 - Local Conditions 1 HOUR ## 24 17 161 3002 88101 3 41.51473 -90.51735 WGS84 PM2.5 - Local Conditions 1 HOUR ## 25 17 161 3002 88101 3 41.51473 -90.51735 WGS84 PM2.5 - Local Conditions 1 HOUR ## 26 17 161 3002 88101 3 41.51473 -90.51735 WGS84 PM2.5 - Local Conditions 1 HOUR ## 27 17 161 3002 88101 3 41.51473 -90.51735 WGS84 PM2.5 - Local Conditions 1 HOUR ## 28 17 161 3002 88101 3 41.51473 -90.51735 WGS84 PM2.5 - Local Conditions 1 HOUR ## 29 17 019 1001 88101 3 40.05278 -88.37251 WGS84 PM2.5 - Local Conditions 1 HOUR ## 30 17 019 1001 88101 3 40.05278 -88.37251 WGS84 PM2.5 - Local Conditions 1 HOUR ## 31 17 019 1001 88101 3 40.05278 -88.37251 WGS84 PM2.5 - Local Conditions 1 HOUR ## 32 17 019 1001 88101 3 40.05278 -88.37251 WGS84 PM2.5 - Local Conditions 1 HOUR ## pollutant_standard date_local units_of_measure event_type observation_count observation_percent ## 2 &lt;NA&gt; 2018-01-07 Micrograms/cubic meter (LC) None 24 100 ## 2100 &lt;NA&gt; 2018-01-06 Micrograms/cubic meter (LC) None 24 100 ## 3 &lt;NA&gt; 2018-01-05 Micrograms/cubic meter (LC) None 24 100 ## 4 &lt;NA&gt; 2018-01-04 Micrograms/cubic meter (LC) None 24 100 ## 5 &lt;NA&gt; 2018-01-03 Micrograms/cubic meter (LC) None 24 100 ## 6 &lt;NA&gt; 2018-01-02 Micrograms/cubic meter (LC) None 24 100 ## 7 &lt;NA&gt; 2018-01-01 Micrograms/cubic meter (LC) None 24 100 ## 8 &lt;NA&gt; 2018-01-07 Micrograms/cubic meter (LC) None 24 100 ## 9 &lt;NA&gt; 2018-01-06 Micrograms/cubic meter (LC) None 24 100 ## 10 &lt;NA&gt; 2018-01-05 Micrograms/cubic meter (LC) None 24 100 ## 11 &lt;NA&gt; 2018-01-04 Micrograms/cubic meter (LC) None 24 100 ## 12 &lt;NA&gt; 2018-01-03 Micrograms/cubic meter (LC) None 24 100 ## 13 &lt;NA&gt; 2018-01-02 Micrograms/cubic meter (LC) None 24 100 ## 14 &lt;NA&gt; 2018-01-01 Micrograms/cubic meter (LC) None 24 100 ## 15 &lt;NA&gt; 2018-01-07 Micrograms/cubic meter (LC) None 24 100 ## 16 &lt;NA&gt; 2018-01-06 Micrograms/cubic meter (LC) None 24 100 ## 17 &lt;NA&gt; 2018-01-05 Micrograms/cubic meter (LC) None 24 100 ## 18 &lt;NA&gt; 2018-01-04 Micrograms/cubic meter (LC) None 24 100 ## 19 &lt;NA&gt; 2018-01-03 Micrograms/cubic meter (LC) None 24 100 ## 20 &lt;NA&gt; 2018-01-02 Micrograms/cubic meter (LC) None 24 100 ## 21 &lt;NA&gt; 2018-01-01 Micrograms/cubic meter (LC) None 24 100 ## 22 &lt;NA&gt; 2018-01-07 Micrograms/cubic meter (LC) None 24 100 ## 23 &lt;NA&gt; 2018-01-06 Micrograms/cubic meter (LC) None 24 100 ## 24 &lt;NA&gt; 2018-01-05 Micrograms/cubic meter (LC) None 24 100 ## 25 &lt;NA&gt; 2018-01-04 Micrograms/cubic meter (LC) None 24 100 ## 26 &lt;NA&gt; 2018-01-03 Micrograms/cubic meter (LC) None 24 100 ## 27 &lt;NA&gt; 2018-01-02 Micrograms/cubic meter (LC) None 24 100 ## 28 &lt;NA&gt; 2018-01-01 Micrograms/cubic meter (LC) None 24 100 ## 29 &lt;NA&gt; 2018-01-07 Micrograms/cubic meter (LC) None 24 100 ## 30 &lt;NA&gt; 2018-01-06 Micrograms/cubic meter (LC) None 24 100 ## 31 &lt;NA&gt; 2018-01-05 Micrograms/cubic meter (LC) None 24 100 ## 32 &lt;NA&gt; 2018-01-04 Micrograms/cubic meter (LC) None 24 100 ## validity_indicator arithmetic_mean first_max_value first_max_hour aqi method_code ## 2 Y 13.770833 23.0 20 NA 183 ## 2100 Y 9.220833 18.0 8 NA 183 ## 3 Y 12.029167 21.5 22 NA 183 ## 4 Y 13.587500 20.6 1 NA 183 ## 5 Y 10.791667 26.2 11 NA 183 ## 6 Y 10.695833 27.6 19 NA 183 ## 7 Y 8.070833 23.0 9 NA 183 ## 8 Y 12.966667 18.6 20 NA 183 ## 9 Y 10.562500 20.2 5 NA 183 ## 10 Y 8.620833 16.9 10 NA 183 ## 11 Y 9.616667 16.3 3 NA 183 ## 12 Y 9.945833 16.4 18 NA 183 ## 13 Y 9.920833 16.5 7 NA 183 ## 14 Y 6.320833 14.0 19 NA 183 ## 15 Y 9.687500 14.6 17 NA 183 ## 16 Y 9.950000 23.0 7 NA 183 ## 17 Y 8.579167 16.4 19 NA 183 ## 18 Y 11.000000 17.9 3 NA 183 ## 19 Y 8.370833 21.0 16 NA 183 ## 20 Y 5.395833 13.6 22 NA 183 ## 21 Y 6.770833 17.4 2 NA 183 ## 22 Y 14.033333 22.0 22 NA 183 ## 23 Y 11.837500 24.5 9 NA 183 ## 24 Y 9.866667 23.8 8 NA 183 ## 25 Y 12.300000 24.0 0 NA 183 ## 26 Y 10.262500 15.6 2 NA 183 ## 27 Y 13.637500 26.4 10 NA 183 ## 28 Y 9.841667 16.0 18 NA 183 ## 29 Y 11.366667 20.2 8 NA 183 ## 30 Y 11.708333 21.9 9 NA 183 ## 31 Y 7.820833 15.9 2 NA 183 ## 32 Y 8.587500 16.1 17 NA 183 ## method local_site_name site_address ## 2 Thermo Scientific 5014i or FH62C14-DHS w/VSCC - Beta Attenuation IEPA TRAILER HICKORY GROVE &amp; FALLVIEW ## 2100 Thermo Scientific 5014i or FH62C14-DHS w/VSCC - Beta Attenuation IEPA TRAILER HICKORY GROVE &amp; FALLVIEW ## 3 Thermo Scientific 5014i or FH62C14-DHS w/VSCC - Beta Attenuation IEPA TRAILER HICKORY GROVE &amp; FALLVIEW ## 4 Thermo Scientific 5014i or FH62C14-DHS w/VSCC - Beta Attenuation IEPA TRAILER HICKORY GROVE &amp; FALLVIEW ## 5 Thermo Scientific 5014i or FH62C14-DHS w/VSCC - Beta Attenuation IEPA TRAILER HICKORY GROVE &amp; FALLVIEW ## 6 Thermo Scientific 5014i or FH62C14-DHS w/VSCC - Beta Attenuation IEPA TRAILER HICKORY GROVE &amp; FALLVIEW ## 7 Thermo Scientific 5014i or FH62C14-DHS w/VSCC - Beta Attenuation IEPA TRAILER HICKORY GROVE &amp; FALLVIEW ## 8 Thermo Scientific 5014i or FH62C14-DHS w/VSCC - Beta Attenuation AGRICULTURE BUILDING STATE FAIR GROUNDS ## 9 Thermo Scientific 5014i or FH62C14-DHS w/VSCC - Beta Attenuation AGRICULTURE BUILDING STATE FAIR GROUNDS ## 10 Thermo Scientific 5014i or FH62C14-DHS w/VSCC - Beta Attenuation AGRICULTURE BUILDING STATE FAIR GROUNDS ## 11 Thermo Scientific 5014i or FH62C14-DHS w/VSCC - Beta Attenuation AGRICULTURE BUILDING STATE FAIR GROUNDS ## 12 Thermo Scientific 5014i or FH62C14-DHS w/VSCC - Beta Attenuation AGRICULTURE BUILDING STATE FAIR GROUNDS ## 13 Thermo Scientific 5014i or FH62C14-DHS w/VSCC - Beta Attenuation AGRICULTURE BUILDING STATE FAIR GROUNDS ## 14 Thermo Scientific 5014i or FH62C14-DHS w/VSCC - Beta Attenuation AGRICULTURE BUILDING STATE FAIR GROUNDS ## 15 Thermo Scientific 5014i or FH62C14-DHS w/VSCC - Beta Attenuation TEN MILE CREEK DNR OFFICE STATE ROUTE 14 ## 16 Thermo Scientific 5014i or FH62C14-DHS w/VSCC - Beta Attenuation TEN MILE CREEK DNR OFFICE STATE ROUTE 14 ## 17 Thermo Scientific 5014i or FH62C14-DHS w/VSCC - Beta Attenuation TEN MILE CREEK DNR OFFICE STATE ROUTE 14 ## 18 Thermo Scientific 5014i or FH62C14-DHS w/VSCC - Beta Attenuation TEN MILE CREEK DNR OFFICE STATE ROUTE 14 ## 19 Thermo Scientific 5014i or FH62C14-DHS w/VSCC - Beta Attenuation TEN MILE CREEK DNR OFFICE STATE ROUTE 14 ## 20 Thermo Scientific 5014i or FH62C14-DHS w/VSCC - Beta Attenuation TEN MILE CREEK DNR OFFICE STATE ROUTE 14 ## 21 Thermo Scientific 5014i or FH62C14-DHS w/VSCC - Beta Attenuation TEN MILE CREEK DNR OFFICE STATE ROUTE 14 ## 22 Thermo Scientific 5014i or FH62C14-DHS w/VSCC - Beta Attenuation ROCK ISLAND ARSENAL 32 RODMAN AVE. ## 23 Thermo Scientific 5014i or FH62C14-DHS w/VSCC - Beta Attenuation ROCK ISLAND ARSENAL 32 RODMAN AVE. ## 24 Thermo Scientific 5014i or FH62C14-DHS w/VSCC - Beta Attenuation ROCK ISLAND ARSENAL 32 RODMAN AVE. ## 25 Thermo Scientific 5014i or FH62C14-DHS w/VSCC - Beta Attenuation ROCK ISLAND ARSENAL 32 RODMAN AVE. ## 26 Thermo Scientific 5014i or FH62C14-DHS w/VSCC - Beta Attenuation ROCK ISLAND ARSENAL 32 RODMAN AVE. ## 27 Thermo Scientific 5014i or FH62C14-DHS w/VSCC - Beta Attenuation ROCK ISLAND ARSENAL 32 RODMAN AVE. ## 28 Thermo Scientific 5014i or FH62C14-DHS w/VSCC - Beta Attenuation ROCK ISLAND ARSENAL 32 RODMAN AVE. ## 29 Thermo Scientific 5014i or FH62C14-DHS w/VSCC - Beta Attenuation ISWS CLIMATE STATION TWP RD 500 E. ## 30 Thermo Scientific 5014i or FH62C14-DHS w/VSCC - Beta Attenuation ISWS CLIMATE STATION TWP RD 500 E. ## 31 Thermo Scientific 5014i or FH62C14-DHS w/VSCC - Beta Attenuation ISWS CLIMATE STATION TWP RD 500 E. ## 32 Thermo Scientific 5014i or FH62C14-DHS w/VSCC - Beta Attenuation ISWS CLIMATE STATION TWP RD 500 E. ## state county city cbsa_code cbsa date_of_last_change ## 2 Illinois Randolph Not in a city &lt;NA&gt; &lt;NA&gt; 2019-01-24 ## 2100 Illinois Randolph Not in a city &lt;NA&gt; &lt;NA&gt; 2019-01-24 ## 3 Illinois Randolph Not in a city &lt;NA&gt; &lt;NA&gt; 2019-01-24 ## 4 Illinois Randolph Not in a city &lt;NA&gt; &lt;NA&gt; 2019-01-24 ## 5 Illinois Randolph Not in a city &lt;NA&gt; &lt;NA&gt; 2019-01-24 ## 6 Illinois Randolph Not in a city &lt;NA&gt; &lt;NA&gt; 2019-01-24 ## 7 Illinois Randolph Not in a city &lt;NA&gt; &lt;NA&gt; 2019-01-24 ## 8 Illinois Sangamon Not in a city 44100 Springfield, IL 2019-01-24 ## 9 Illinois Sangamon Not in a city 44100 Springfield, IL 2019-01-24 ## 10 Illinois Sangamon Not in a city 44100 Springfield, IL 2019-01-24 ## 11 Illinois Sangamon Not in a city 44100 Springfield, IL 2019-01-24 ## 12 Illinois Sangamon Not in a city 44100 Springfield, IL 2019-01-24 ## 13 Illinois Sangamon Not in a city 44100 Springfield, IL 2019-01-24 ## 14 Illinois Sangamon Not in a city 44100 Springfield, IL 2019-01-24 ## 15 Illinois Hamilton Not in a city 34500 Mount Vernon, IL 2019-01-24 ## 16 Illinois Hamilton Not in a city 34500 Mount Vernon, IL 2019-01-24 ## 17 Illinois Hamilton Not in a city 34500 Mount Vernon, IL 2019-01-24 ## 18 Illinois Hamilton Not in a city 34500 Mount Vernon, IL 2019-01-24 ## 19 Illinois Hamilton Not in a city 34500 Mount Vernon, IL 2019-01-24 ## 20 Illinois Hamilton Not in a city 34500 Mount Vernon, IL 2019-01-24 ## 21 Illinois Hamilton Not in a city 34500 Mount Vernon, IL 2019-01-24 ## 22 Illinois Rock Island Rock Island Arsenal (U.S. Army) 19340 Davenport-Moline-Rock Island, IA-IL 2019-01-24 ## 23 Illinois Rock Island Rock Island Arsenal (U.S. Army) 19340 Davenport-Moline-Rock Island, IA-IL 2019-01-24 ## 24 Illinois Rock Island Rock Island Arsenal (U.S. Army) 19340 Davenport-Moline-Rock Island, IA-IL 2019-01-24 ## 25 Illinois Rock Island Rock Island Arsenal (U.S. Army) 19340 Davenport-Moline-Rock Island, IA-IL 2019-01-24 ## 26 Illinois Rock Island Rock Island Arsenal (U.S. Army) 19340 Davenport-Moline-Rock Island, IA-IL 2019-01-24 ## 27 Illinois Rock Island Rock Island Arsenal (U.S. Army) 19340 Davenport-Moline-Rock Island, IA-IL 2019-01-24 ## 28 Illinois Rock Island Rock Island Arsenal (U.S. Army) 19340 Davenport-Moline-Rock Island, IA-IL 2019-01-24 ## 29 Illinois Champaign Bondville 16580 Champaign-Urbana, IL 2019-01-24 ## 30 Illinois Champaign Bondville 16580 Champaign-Urbana, IL 2019-01-24 ## 31 Illinois Champaign Bondville 16580 Champaign-Urbana, IL 2019-01-24 ## 32 Illinois Champaign Bondville 16580 Champaign-Urbana, IL 2019-01-24 ## [ reached &#39;max&#39; / getOption(&quot;max.print&quot;) -- omitted 627 rows ] The outputted data frame includes many fields regarding the PM2.5 observation, including spatial data for the sensor’s location. We will focus on these details later on in our data wrangling process. The next code chunk describes how to query for PM2.5 data across our three states and four years. library(dplyr) # List of States to Iterate Through states = c(&quot;17&quot;, &quot;18&quot;, &quot;55&quot;) # Matrix of Start Dates and End Dates to Iterate Through dates = matrix(c(&quot;20140101&quot;, &quot;20141231&quot;, &quot;20150101&quot;, &quot;20151231&quot;, &quot;20160101&quot;, &quot;20161231&quot;, &quot;20170101&quot;, &quot;20171231&quot;, &quot;20180101&quot;, &quot;20181231&quot;), ncol = 2, byrow = TRUE) # Leveraging apply functions to iterate through both states and dates full.data = lapply(states, function(x){ mapply(aqs_dailyData_byState, bdate = dates[,1], edate = dates[,2], MoreArgs = list(aqs_user = myuser, param = 88101, state = x), SIMPLIFY = FALSE ) %&gt;% do.call(&quot;rbind&quot;, .) }) %&gt;% do.call(&quot;rbind&quot;, .) 5.4 FAA Weather Data Figure 5.2: An ASOS Observation Station in Elko, NV. (Wikimedia Commons) FAA weather data gathered from the Automated Surface Observing System (ASOS) can be imported using the riem package. This package, created by ROpenSci, queries weather data from the Iowa Environmental Mesonet, an online portal for international ASOS data maintained by Iowa State University. First, let’s load the package. # devtools::install_github(&#39;ropensci/riem&#39;) library(riem, quietly = TRUE) 5.4.1 Sample Query Below is an R code snippet that performs the simplest weather data query possible in the riem package. It specifies a particular weather station using an airport code and a date range to query for. The output is a tibble table of raw ASOS weather data. The code snippet below extracts sensor data at the San Francisco International Airport. SFO.weather = riem_measures(station = &#39;KSFO&#39;, date_start = &quot;2014-01-01&quot;, date_end = &#39;2014-01-02&#39;) station valid tmpf dwpf relh drct SFO 2014-01-01 00:56:00 53.96 42.98 66.28 290 SFO 2014-01-01 01:56:00 51.98 42.98 71.28 280 SFO 2014-01-01 02:56:00 51.08 44.06 76.80 290 SFO 2014-01-01 03:56:00 48.92 37.94 65.67 0 SFO 2014-01-01 04:56:00 50.00 39.92 68.15 0 station valid tmpf dwpf alti vsby SFO 2014-01-01 00:56:00 53.96 42.98 30.16 10 SFO 2014-01-01 01:56:00 51.98 42.98 30.17 10 SFO 2014-01-01 02:56:00 51.08 44.06 30.17 10 SFO 2014-01-01 03:56:00 48.92 37.94 30.18 10 SFO 2014-01-01 04:56:00 50.00 39.92 30.18 10 SFO 2014-01-01 05:56:00 48.92 37.04 30.19 10 The outputted table shows weather data for a 24-hour period on January 1st, 2014 at the San Francisco International Airport. The valid column species when each weather report was generated, typically at 1-hour intervals. The tmpf and dwpf columns give the ambient air temperature and dew point in Fahrenheit (ºF). Other important variables in our project include air pressure (alti), measured in inches of mercury (in.Hg), and visibility (vsby) in miles. For more information on all available varibles, see Iowa State’s Documentation. Next, we will apply this function at a large scare across multiple sensors and timescales. 5.4.2 Finding ASOS Sensors The FAA collects weather data at hourly intervals for each meteorological station, with some stations providing half-hour intervals. Even querying for short periods of time can yield large amounts of data. To optimise performance, we want to only query data from stations in our study area. Finding Sensors by State In our project, we focus on certain counties in Illinois, Indiana, and Wisconsin, so we are interested in finding the sensors within that study area. The first step is to query the locations of all weather stations in the three states using the riem package. In the example below, we query for sensors in the Illinois ASOS sensor network. IL.stations = riem_stations(network = &#39;IL_ASOS&#39;) id name lon lat ALN ALTON/ST LOUIS R -90.04599 38.88992 BMI BLOOMINGTON/NORM -88.91592 40.47711 CPS CAHOKIA/ST LOUIS -90.15622 38.57073 CIR Cairo -89.21960 37.06470 MDH CARBONDALE/MURPH -89.25000 37.78000 CUL Carmi -88.12306 38.08948 To query for data across multiple states, we are going the apply the riem_stations function to a list of weather station networks, as shown below. networks = list(&#39;IL_ASOS&#39;, &#39;IN_ASOS&#39;, &#39;WI_ASOS&#39;) library(dplyr, quietly = TRUE) station.locs = lapply(networks, riem::riem_stations) %&gt;% do.call(rbind, .) # Creates a single data table as output Note: You can find a list of state abbreviations by typing state.abb in your R console. Converting Latitude and Longitude Coordinates to Spatial Data The data tables returned by the riem package must be converted to spatial data to determine which sensors are located in the study area. Since the lon/lat coordinates are already provided, the data table is easily converted to a spatial sf object. station.locs.sf = sf::st_as_sf(station.locs, coords = c(&quot;lon&quot;, &quot;lat&quot;), crs = 4326) # Plot stations and study area boundaries to verify that the correct sensors were selected plot(station.locs.sf$geometry) plot(sf::st_read(&#39;https://uchicago.box.com/shared/static/uw0srt8nyyjfqo6l0dv07cyskwmv6r50.geojson&#39;, quiet = TRUE)$geometry, border = &#39;red&#39;, add = TRUE) We plot to results to verify that our query and data conversion process worked correctly. For reference, the boundaires of the study area is outlined in red. Selecting Sensors within our Study Area Next, we perform a spatial join to only keep the points located within the boundaries of our study area polygons. The spatial join is completed by the sf package, as shown below. For more information regarding spatial joins and spatial predicates, please see this helpful blog post by GISgeography.com. # Loading study area boundaries study.area = sf::st_read(&#39;https://uchicago.box.com/shared/static/uw0srt8nyyjfqo6l0dv07cyskwmv6r50.geojson&#39;, quiet = TRUE) study.sensors = sf::st_join(station.locs.sf, study.area, left = FALSE) # Verify Spatial Join by Plotting plot(study.area$geometry, border = &#39;red&#39;) plot(study.sensors$geometry, add = TRUE) title(&#39;Weather Stations Within the Study Area&#39;) Now that we have a dataset of which weather stations we are interested in, we can query for the weather data associated with each station. 5.4.3 Weather Data Query Again we use the lapply function in base R to execute the riem_measures function on a list of sensor IDs. This allows us to iteratively query for weather data from each individual sensor in a list. In the code snippet below, we take the study sensors obtained previously and query for a single day’s worth of weather data. library(dplyr, quietly = TRUE) weather.data = lapply(study.sensors$id, function(x){riem::riem_measures(x, date_start = &quot;2014-01-01&quot;, date_end = &quot;2014-01-02&quot;)}) %&gt;% do.call(rbind, .) # Creates a single data table as output station valid lon lat tmpf MDW 2014-01-01 00:49:00 -87.7524 41.786 12.20 MDW 2014-01-01 00:51:00 -87.7524 41.786 12.92 MDW 2014-01-01 01:51:00 -87.7524 41.786 12.92 MDW 2014-01-01 02:27:00 -87.7524 41.786 14.00 MDW 2014-01-01 02:51:00 -87.7524 41.786 14.00 Querying Full Weather Dataset Use caution when querying for a large amount of data. Data tables can easily become unwieldy after querying for a large number of weather stations across a wide time scale. The code snippet below downloads all ASOS weather data for sensors in our study area from January 1st 2014 to December 31st 2018, which is our study time period. It has approximately 4.8 Million records and takes 6-10 minutes to download. weather.data = lapply(study.sensors$id, function(x){riem::riem_measures(x, date_start = &quot;2014-01-01&quot;, date_end = &quot;2018-12-31&quot;)}) %&gt;% do.call(rbind, .) # Creates a single data table as output "],["A-NDVI.html", "Appendix A: NDVI 5.5 Overview 5.6 Environment Setup 5.7 Data manipulation and Plotting 5.8 More Plotting", " Appendix A: NDVI 5.5 Overview In this tutorial, we will learn to deal with raster data. For illustrative purposes, we will take a look at the NDVI data (Normalized Difference Vegetation Index) of 21 large counties in the Midwest. Our objectives are to: Visualize raster data using the “tmap” package Learn the basic techniques of cropping and masking Check and analyze summary statistics of raster data 5.6 Environment Setup Input/Output Our inputs include the shapefile of 21 large counties in the Midwest and the quarterly data of NDVI. The files are available for download on this project’s GitHub repo. Load Libraries We will use the following packages in this tutorial: raster: to manipulate and analyze raster data sf: to conduct basic spatial data manipulation tamp: to create spatial data visualization library(raster) library(sf) library(tmap) Since we are mainly analyzing raster data in this tutorial, we use the “raster” package heavily, which “implements basic and high-level functions for raster data and for vector data operations such as intersections.” The detailed documentation can be found here. Load Data The data we are analyzing is the quarterly average of the Normalized Difference Vegetation Index for 21 large counties in the Midwest, from 2014 to 2018. It is stacked chronologically. Note that the original dataset is quite large (over 50 gigabytes), so we processed the data in advance. We can load the pre-processed data simply by running the command below. ndvi.quarterly &lt;- stack(&quot;./data/NDVILargeAreaQuarterlyStack.tif&quot;) (Remark: You may encounter trouble loading the data if you do not have the “rgdal” package installed!) It is a good idea to take a glance at our dataset before we proceed any further. ndvi.quarterly ## class : RasterStack ## dimensions : 1194, 784, 936096, 20 (nrow, ncol, ncell, nlayers) ## resolution : 0.00295, 0.00208 (x, y) ## extent : -88.77872, -86.46592, 40.73568, 43.2192 (xmin, xmax, ymin, ymax) ## crs : +proj=longlat +datum=WGS84 +no_defs ## names : NDVILarge//rlyStack.1, NDVILarge//rlyStack.2, NDVILarge//rlyStack.3, NDVILarge//rlyStack.4, NDVILarge//rlyStack.5, NDVILarge//rlyStack.6, NDVILarge//rlyStack.7, NDVILarge//rlyStack.8, NDVILarge//rlyStack.9, NDVILarge//lyStack.10, NDVILarge//lyStack.11, NDVILarge//lyStack.12, NDVILarge//lyStack.13, NDVILarge//lyStack.14, NDVILarge//lyStack.15, ... ## min values : -0.1979000, -0.1400464, -0.1463546, -0.1714621, -0.1730933, -0.1068576, -0.1513947, -0.1989500, -0.1977000, -0.1508639, -0.1616515, -0.1834559, -0.1792000, -0.1500082, -0.1286048, ... ## max values : 0.4057376, 0.8217609, 0.9162192, 0.7483148, 0.5150092, 0.8381958, 0.9147448, 0.8577355, 0.7820903, 0.8141744, 0.9077643, 0.8238385, 0.8000358, 0.8359118, 0.8964590, ... Also remember to load the shapfile of the 21 large counties. We will use it shortly when we start plotting. counties &lt;- st_read(&quot;./data/LargeAreaCounties&quot;) ## Reading layer `LargeAreaCounties&#39; from data source `/Users/LorenzMenendez/Desktop/OpenAirQ-toolkit/data/LargeAreaCounties&#39; using driver `ESRI Shapefile&#39; ## Simple feature collection with 21 features and 8 fields ## geometry type: MULTIPOLYGON ## dimension: XY ## bbox: xmin: -88.77879 ymin: 40.73641 xmax: -86.46629 ymax: 43.21831 ## geographic CRS: WGS 84 For more detailed description of the data, please refer to the main chapters of this tutorial book. Insert Link! 5.7 Data manipulation and Plotting We start our analysis by looking at the data for one specific quarter - the 3rd quarter of 2018. Since this is the 19th layer of our dataset (recall that our dataset is stacked chronologically), we can easily extract the data using the line of code shown below. this.qtr &lt;- raster::subset(ndvi.quarterly, 19, drop = FALSE) Thanks to the data processing done beforehand, we don’t have much data manipulation to do. It is time for us to start making plots! We begin with the most basic raster map. tm_shape(this.qtr) + tm_raster() + tm_layout(legend.outside = TRUE) This plot gives an overview of the NDVI of the 21 large counties. Without the county borders explicitly drawn, we are unable to compare the NDVI across counties. Moreover, the plot is far from aesthetically pleasing. Hence we redraw the graph, this time with counties borders as well as an informative title for the plot. In addition, we modify the title of the legend so that it is more comprehensible to readers. tm_shape(this.qtr) + tm_raster(title = &quot;NDVI&quot;) + tm_shape(counties) + tm_borders() + tm_layout(legend.outside = TRUE, main.title = &quot;NDVI - The 3rd Quarter of 2018&quot;) With this nicer plot, it is possible to compare the NDVI of different counties. Those who are familiar with the geography of the Midwest would immediately recognize that Cook county seems to have a lower NDVI than other counties. It is not at all surprising since the City of Chicago is located in Cook County, and metropolitan areas tend to have less vegetation than rural areas. We can also make the plot interactive so that the readers can explore the plot more thoroughly. tmap_mode(&quot;view&quot;) tm_shape(this.qtr) + tm_raster(title = &quot;NDVI&quot;) + tm_shape(counties) + tm_borders() + tm_layout(legend.outside = TRUE, main.title = &quot;NDVI - the 3rd Quarter of 2018&quot;) We can turn off the interactive mode by running the following code. tmap_mode(&quot;plot&quot;) Before we jump to the next section, it may be helpful to quickly examine the summary statistics for the sub-dataset this.qtr. Notice that we use raster::summary here to specifiy that we want R to use the summary function from the “raster” package. raster::summary(this.qtr) ## NDVILargeAreaQuarterlyStack.19 ## Min. -9.315484e-02 ## 1st Qu. 6.263003e-01 ## Median 6.918114e-01 ## 3rd Qu. 7.413675e-01 ## Max. 9.006166e-01 ## NA&#39;s 4.571800e+05 5.8 More Plotting Now that we have learned how to make a raster map, let’s make more. In this section, we will place our focus on the NDVI of Cook County, IL - the home county of Chicago. First, we will extract the shape of Cook County. cook.county &lt;- counties[counties$COUNTYNAME == &quot;Cook&quot;, ] Before we do anything else, we can plot the shape of Cook County that we just extracted to make sure we subsetted the dataset correctly. plot(cook.county$geometry) This is a very crude plot since it only shows the border of Cook County. Rest assured that our final product is much more visually pleasing than this! The following two lines of code crop and mask raster data to Cook County. Cropping and then masking accelerates the actions for large raster data. It probably does not matter in our case, since we are only looking at Cook County, and the volume of our data is rather small. cook.ndvi &lt;- raster::crop(this.qtr, cook.county) cook.ndvi &lt;- raster::mask(cook.ndvi, cook.county) Now we can plot a raster map for Cook County. The commands we use here are very similar to the ones we used before. tm_shape(cook.ndvi) + tm_raster(title = &quot;NDVI&quot;, palette = &quot;Greens&quot;) + tm_shape(cook.county) + tm_borders() + tm_layout(legend.outside = TRUE, main.title = &quot;NDVI for Cook County:\\nThe 3rd Quarter of 2018&quot;) With this raster map, we are able to discern patterns of NDVI with Cook County. The middle-east part of Cook County, where the City of Chicago is located, is shown in a lighter green than other parts of the county, signaling a lower NDVI. The northern and southern parts of the county, which are mostly rural areas, are shown to have a higher NDVI, much as we would expect. Before we conclude this tutorial, let’s again take a look at the summary statistics. raster::summary(cook.ndvi) ## NDVILargeAreaQuarterlyStack.19 ## Min. 2.923896e-02 ## 1st Qu. 3.955676e-01 ## Median 5.302855e-01 ## 3rd Qu. 6.440714e-01 ## Max. 8.844491e-01 ## NA&#39;s 3.973500e+04 base::mean(na.omit(getValues(cook.ndvi))) ## [1] 0.5213509 base::mean(na.omit(getValues(this.qtr))) ## [1] 0.6662209 The mean NDVI for Cook County is 0.52, while the mean for all 21 counties is 0.67. This is reasonable because NDVI tends to be lower in large cities, and Cook County happens to be at the center of the Chicago metropolitan area. Moreover, this is consistent with what we observe from the plots - Cook County is shown in a lighter green than other counties, which we have pointed out earlier in the tutorial. This marks the end of our tutorial. Hopefully, now you feel comfortable dealing with raster data. Of course, there are many more techniques to be learned in order to conduct more sophisticated spatial analysis with raster data. There are abundant online resources that introduce more complicated tools to handle raster data, which you may want to explore on your own. "],["B-Point_Emission_Data.html", "Appendix B: Point Emission Data 5.9 Overview 5.10 Environment Setup 5.11 Data Manipulation 5.12 Making Point Data Maps", " Appendix B: Point Emission Data 5.9 Overview In this tutorial, we will demonstrate how to represent PM2.5 point emission data on maps. As an example, we will look at the PM2.5 data for four states - Illinois, Indiana, Wisconsin, and Michigan. The goal of this exercise is to create a point data map that provides a clear visualization of the point source of PM2.5 emission. To summarize, our objectives are to: Gain famililarity with the pollution data from 2014 National Emissions Inventory Perform simple data manipulation on the PM 2.5 data Visualize PM 2.5 pollution using the “tmap” package 5.10 Environment Setup Input/Output The files that will be used in this tutorial are the pollution data for Illinois, Indiana, Wisconsin, and Michigan as well as he shapefile of the four states. The files are available for download on this project’s GitHub repo. Load Libraries We start by loading the necessary packages - tidyverse, sf, and tmap: tidyverse: to conduct basic statistical analyses sf: to perform simple spatial data manipulation. tamp: to create spatial data visualization library(tidyverse) library(sf) library(tmap) Load Data Besides the packages, we also need to load our data, which can be done by running the commented-out code below. We call the data frame pe1. pe1 &lt;- read_csv(&quot;process_12345.csv&quot;) Note that this file above is unfortunately too large to be uploaded to Github, so we will instead use a pre-processed data set: We also load the shapefile for the four states: fourstates &lt;- st_read(&quot;./data/FourStates&quot;) ## Reading layer `FourStates&#39; from data source `/Users/LorenzMenendez/Desktop/OpenAirQ-toolkit/data/FourStates&#39; using driver `ESRI Shapefile&#39; ## Simple feature collection with 4 features and 14 fields ## geometry type: POLYGON ## dimension: XY ## bbox: xmin: -92.88943 ymin: 36.9703 xmax: -82.12297 ymax: 48.30606 ## geographic CRS: WGS 84 For more detailed description of the data, please refer to the main chapters of this tutorial book. Insert Link! 5.11 Data Manipulation Once we have our data and the packages ready, we will start the data manipulation process. Since we will only look at the data from Illinois, Indiana, Wisconsin, and Michigan, we can use the filter function to pick out only the four states that we are interested in, and we name the new data frame fourstate.pe1. This is accomplished by the commented-out code below. This dataset comes from 2014 National Emissions Inventory, which is “a comprehensive and detailed estimate of air emissions of criteria pollutants, criteria precursors, and hazardous air pollutants from air emissions sources.” You can read more about this dataset on this website. states.abbr &lt;- c(&quot;IL&quot;, &quot;IN&quot;, &quot;WI&quot;, &quot;MI&quot;) fourstate.pe &lt;- pe1 %&gt;% filter(state %in% states.abbr) (Remark: The lines of code above does not need to be run if the data file loaded is four_state.csv. They were written to clean the data from process_12345.csv.) Next, we turn our focus to the pollutant data. To familiarize ourselves with the variable pollutant desc, we use the unique function to examine all the unique values of this variable. #Find pollutant names for pm2.5 unique(fourstate.pe$`pollutant desc`) ## [1] &quot;Hexane&quot; ## [2] &quot;Toluene&quot; ## [3] &quot;Propionaldehyde&quot; ## [4] &quot;Xylenes (Mixed Isomers)&quot; ## [5] &quot;Benzo[g,h,i,]Perylene&quot; ## [6] &quot;Indeno[1,2,3-c,d]Pyrene&quot; ## [7] &quot;Benzo[b]Fluoranthene&quot; ## [8] &quot;Fluoranthene&quot; ## [9] &quot;Benzo[k]Fluoranthene&quot; ## [10] &quot;Acenaphthylene&quot; ## [11] &quot;Chrysene&quot; ## [12] &quot;Formaldehyde&quot; ## [13] &quot;Benzo[a]Pyrene&quot; ## [14] &quot;2,2,4-Trimethylpentane&quot; ## [15] &quot;Benz[a]Anthracene&quot; ## [16] &quot;Benzene&quot; ## [17] &quot;Lead&quot; ## [18] &quot;Acetaldehyde&quot; ## [19] &quot;Acenaphthene&quot; ## [20] &quot;Phenanthrene&quot; ## [21] &quot;Fluorene&quot; ## [22] &quot;Naphthalene&quot; ## [23] &quot;Carbon Monoxide&quot; ## [24] &quot;Nitrogen Oxides&quot; ## [25] &quot;PM10 Primary (Filt + Cond)&quot; ## [26] &quot;PM2.5 Primary (Filt + Cond)&quot; ## [27] &quot;Sulfur Dioxide&quot; ## [28] &quot;Volatile Organic Compounds&quot; ## [29] &quot;Ethyl Benzene&quot; ## [30] &quot;Styrene&quot; ## [31] &quot;1,3-Butadiene&quot; ## [32] &quot;Acrolein&quot; ## [33] &quot;Anthracene&quot; ## [34] &quot;m-Xylene&quot; ## [35] &quot;Phenol&quot; ## [36] &quot;Methanol&quot; ## [37] &quot;2-Methylnaphthalene&quot; ## [38] &quot;o-Xylene&quot; ## [39] &quot;Cumene&quot; ## [40] &quot;Dibenzo[a,h]Anthracene&quot; ## [41] &quot;PM2.5 Filterable&quot; ## [42] &quot;PM10 Filterable&quot; ## [43] &quot;PM Condensible&quot; ## [44] &quot;Carbon Dioxide&quot; ## [45] &quot;Methane&quot; ## [46] &quot;Nitrous Oxide&quot; ## [47] &quot;Cadmium&quot; ## [48] &quot;Hexamethylene Diisocyanate&quot; ## [49] &quot;Ammonia&quot; ## [50] &quot;Glycol Ethers&quot; ## [51] &quot;Manganese&quot; ## [52] &quot;Methylene Chloride&quot; ## [53] &quot;Chromium III&quot; ## [54] &quot;Chromium (VI)&quot; ## [55] &quot;Nickel&quot; ## [56] &quot;Arsenic&quot; ## [57] &quot;Cobalt&quot; ## [58] &quot;Mercury&quot; ## [59] &quot;Antimony&quot; ## [60] &quot;Selenium&quot; ## [61] &quot;Beryllium&quot; ## [62] &quot;Hydrochloric Acid&quot; ## [63] &quot;Hydrogen Fluoride&quot; ## [64] &quot;Pyrene&quot; ## [65] &quot;Isophorone&quot; ## [66] &quot;Vinyl Chloride&quot; ## [67] &quot;Biphenyl&quot; ## [68] &quot;Tetrachloroethylene&quot; ## [69] &quot;Methyl Chloroform&quot; ## [70] &quot;PAH/POM - Unspecified&quot; ## [71] &quot;2,4-Dinitrophenol&quot; ## [72] &quot;Chlorine&quot; ## [73] &quot;Pentachlorophenol&quot; ## [74] &quot;4-Nitrophenol&quot; ## [75] &quot;Ethyl Chloride&quot; ## [76] &quot;Carbon Disulfide&quot; ## [77] &quot;Ethylidene Dichloride&quot; ## [78] &quot;Propylene Dichloride&quot; ## [79] &quot;Trichloroethylene&quot; ## [80] &quot;1,1,2,2-Tetrachloroethane&quot; ## [81] &quot;Ethylene Dichloride&quot; ## [82] &quot;Acrylonitrile&quot; ## [83] &quot;Methyl Isobutyl Ketone&quot; ## [84] &quot;Chlorobenzene&quot; ## [85] &quot;Carbonyl Sulfide&quot; ## [86] &quot;Carbon Tetrachloride&quot; ## [87] &quot;Chloroform&quot; ## [88] &quot;Dimethyl Phthalate&quot; ## [89] &quot;PAH, total&quot; ## [90] &quot;Acrylic Acid&quot; ## [91] &quot;Polychlorinated Biphenyls&quot; ## [92] &quot;Ethylene Dibromide&quot; ## [93] &quot;1,3-Dichloropropene&quot; ## [94] &quot;Methyl Chloride&quot; ## [95] &quot;Phosphorus&quot; ## [96] &quot;Propylene Oxide&quot; ## [97] &quot;Vinyl Acetate&quot; ## [98] &quot;Methyl Tert-Butyl Ether&quot; ## [99] &quot;Acetophenone&quot; ## [100] &quot;Benzo[e]Pyrene&quot; ## [101] &quot;Perylene&quot; ## [102] &quot;3-Methylcholanthrene&quot; ## [103] &quot;Benzofluoranthenes&quot; ## [104] &quot;p-Xylene&quot; ## [105] &quot;Methyl Bromide&quot; ## [106] &quot;Quinoline&quot; ## [107] &quot;2,4-Dichlorophenoxy Acetic Acid&quot; ## [108] &quot;1-Bromopropane&quot; ## [109] &quot;Allyl Chloride&quot; ## [110] &quot;Ethylene Glycol&quot; ## [111] &quot;Chloromethyl Methyl Ether&quot; ## [112] &quot;Hexachlorobenzene&quot; ## [113] &quot;Triethylamine&quot; ## [114] &quot;2,4,6-Trichlorophenol&quot; ## [115] &quot;Hydrazine&quot; ## [116] &quot;N,N-Dimethylformamide&quot; ## [117] &quot;Acetonitrile&quot; ## [118] &quot;Vinylidene Chloride&quot; ## [119] &quot;Phosgene&quot; ## [120] &quot;Chloroacetic Acid&quot; ## [121] &quot;2,4-Toluene Diisocyanate&quot; ## [122] &quot;Diethanolamine&quot; ## [123] &quot;Methyl Methacrylate&quot; ## [124] &quot;Maleic Anhydride&quot; ## [125] &quot;Hexachloroethane&quot; ## [126] &quot;Cyanide&quot; ## [127] &quot;Phthalic Anhydride&quot; ## [128] &quot;Polycyclic aromatic compounds (includes 25 specific compounds)&quot; ## [129] &quot;Epichlorohydrin&quot; ## [130] &quot;Dimethyl Sulfate&quot; ## [131] &quot;Hydroquinone&quot; ## [132] &quot;Chloroprene&quot; ## [133] &quot;Hydrogen Sulfide&quot; ## [134] &quot;Acrylamide&quot; ## [135] &quot;Cresol/Cresylic Acid (Mixed Isomers)&quot; ## [136] &quot;4,4 -Methylenediphenyl Diisocyanate&quot; ## [137] &quot;Methyl Isocyanate&quot; ## [138] &quot;Tert-butyl Acetate&quot; ## [139] &quot;Bis(2-Ethylhexyl)Phthalate&quot; ## [140] &quot;Ethylene Oxide&quot; ## [141] &quot;1,1,2-Trichloroethane&quot; ## [142] &quot;o-Cresol&quot; ## [143] &quot;Catechol&quot; ## [144] &quot;Sulfur Hexafluoride&quot; ## [145] &quot;2,3,3 ,4,4 ,5/2,3,3 ,4,4 ,5-Hexachlorobiphenyl (PCBs156/157)&quot; ## [146] &quot;2,3,3 ,4,4 -Pentachlorobiphenyl (PCB-105)&quot; ## [147] &quot;2,3 ,4,4 ,5-Pentachlorobiphenyl (PCB118)&quot; ## [148] &quot;3,3 ,4,4 -Tetrachlorobiphenyl (PCB-77)&quot; ## [149] &quot;2,3 ,4,4 ,5,5 -Hexachlorobiphenyl (PCB-167)&quot; ## [150] &quot;2,3,4,4 ,5-Pentachlorobiphenyl (PCB-114)&quot; ## [151] &quot;Methylhydrazine&quot; ## [152] &quot;Benzyl Chloride&quot; ## [153] &quot;1,4-Dichlorobenzene&quot; ## [154] &quot;Bromoform&quot; ## [155] &quot;2-Nitropropane&quot; ## [156] &quot;2-Chloronaphthalene&quot; ## [157] &quot;Quinone&quot; ## [158] &quot;2-Chloroacetophenone&quot; ## [159] &quot;2,4-Dinitrotoluene&quot; ## [160] &quot;Carbazole&quot; ## [161] &quot;Aniline&quot; ## [162] &quot;p-Dioxane&quot; ## [163] &quot;4,6-Dinitro-o-Cresol&quot; ## [164] &quot;Dibutyl Phthalate&quot; ## [165] &quot;1,3-Propanesultone&quot; ## [166] &quot;Benzidine&quot; ## [167] &quot;Dibenzofuran&quot; ## [168] &quot;Ethyl Carbamate&quot; ## [169] &quot;1,2-Epoxybutane&quot; ## [170] &quot;Hydrogen Cyanide&quot; ## [171] &quot;Cellosolve Acetate&quot; ## [172] &quot;Ethyl Acrylate&quot; ## [173] &quot;o-Toluidine&quot; ## [174] &quot;7,12-Dimethylbenz[a]Anthracene&quot; ## [175] &quot;Captan&quot; ## [176] &quot;5-Methylchrysene&quot; ## [177] &quot;Methyl Iodide&quot; ## [178] &quot;Nitrobenzene&quot; ## [179] &quot;Hexachlorobutadiene&quot; ## [180] &quot;Hexachlorocyclopentadiene&quot; ## [181] &quot;1,2,4-Trichlorobenzene&quot; ## [182] &quot;Asbestos&quot; ## [183] &quot;Dichloroethyl Ether&quot; ## [184] &quot;Coke Oven Emissions&quot; ## [185] &quot;Trifluralin&quot; ## [186] &quot;Toxaphene&quot; ## [187] &quot;Carbaryl&quot; ## [188] &quot;Heptachlor&quot; ## [189] &quot;4,4 -Methylenebis(2-Chloraniline)&quot; ## [190] &quot;m-Cresol&quot; ## [191] &quot;Methoxychlor&quot; ## [192] &quot;Phosphine&quot; ## [193] &quot;Calcium Cyanamide&quot; ## [194] &quot;1,2-Propylenimine&quot; ## [195] &quot;Diethyl Sulfate&quot; ## [196] &quot;p-Cresol&quot; ## [197] &quot;Ethylene Thiourea&quot; ## [198] &quot;Chlordane&quot; ## [199] &quot;N,N-Dimethylaniline&quot; ## [200] &quot;p-Phenylenediamine&quot; ## [201] &quot;o-Anisidine&quot; ## [202] &quot;1,2-Diphenylhydrazine&quot; We see that there are quite a number of different pollutants, but we are primarily interested in the PM2.5 data. Therefore, we use the filter function again to subset the data, retaining only those observations with the pollutant being “PM2.5 Filterable” or “PM2.5 Primary (Filt + Cond)”. pm25 &lt;- c(&quot;PM2.5 Filterable&quot;, &quot;PM2.5 Primary (Filt + Cond)&quot;) #Filter for pm2.5 fourstate.pm &lt;- fourstate.pe %&gt;% filter(`pollutant desc` %in% pm25) Now we will take care of the duplicates and missing values, both of which should be removed from our data. The following line of code eliminates any duplicated values in eis facility id. #remove duplicates fourstate.pm.final &lt;- fourstate.pm[!duplicated(fourstate.pm$`eis facility id`),] The following line of code eliminates any missing values in site latitude. We call this cleaned data frame fourstate.pm.final. #remove na coords fourstate.pm.final &lt;- fourstate.pm.final[!is.na(fourstate.pm.final$`site latitude`),] The last step in the data manipulation process is to turn our data points into a spatial object, which we can accomplish by using the st_as_sf function. Notice that we use the coordinate reference system 4326, which is the geodetic coordinate system for world. More information about coordinate reference systems can be found on this website. #Turn into sf object fourstate.pm.spatial &lt;- st_as_sf(fourstate.pm.final, coords = c(&quot;site longitude&quot;, &quot;site latitude&quot;), crs = 4326) 5.12 Making Point Data Maps Finally, we are ready to make some maps! The command for generating a point data map is actually quite easy. We just need to specify the shapefile that stores the shape of the states and the point data which we have turned into a spatial object earlier. We adjust the size of the dots to 0.01 so that the pattern of those points is discernible. tm_shape(fourstates) + tm_borders() + tm_shape(fourstate.pm.spatial) + tm_dots(size = 0.01) The map above is neat, but it only conveys limited information. It is impossible, for example, to tell from the map how much PM2.5 emission each of these dots produces. To improve the simple map above, we use the tem_bubbles function instead of tm_dots. Within tm_bubbles, we can choose how to classify the PM2.5 data by specifying the style. Some common choices of style include “fisher”, “jenks”, “quantile”, etc. Customization of the color palette is also possible, and there are plenty of online tutorials on this topic. tm_shape(fourstates) + tm_borders() + tm_shape(fourstate.pm.spatial) + tm_bubbles(col = &quot;total emissions&quot;, size = 0.01, style = &quot;fisher&quot;, palette = &quot;Reds&quot;) As we can see, the map above is not only more aesthetically pleasing, but it also communicates more information regarding the quantity of PM2.5 that is produced by each site. Sites that generate more PM2.5 are shown in a darker red. With this map, we can identify those sites of heavy PM2.5 emission with great ease by zooming in. This concludes our tutorial. Following the simple steps above would allow you to create some simple point data maps, which are often a neat and easily interpretable visualization of the spatial data that we seek to analyze. "],["C-Elevation_Data.html", "Appendix C: Elevation Data 5.13 Overview 5.14 Environment Setup 5.15 Inspecting Data 5.16 Data Manipulation 5.17 Plotting Grid Map", " Appendix C: Elevation Data 5.13 Overview The goal of this tutorial is to introduce the packages and techniques that are commonly utilized to manipulate raster data and create grid cells. We will use the elevation data of several larges counties in the Midwest as an example, and special emphasis will be placed on introducing the functionalities of the “velox” package. In short, our objectives are to: Learn the features of the “velox” package Inspect the elevation data of several large counties in the Midwest Output a new shapefile that contains the elevation data for each grid 5.14 Environment Setup Input/Output Our inputs include a shapefile of several large counties in the Midwest, a grid data file, and a raster file that contains the elevation data which is the focus of this tutorial. The files can be found here. The files are available for download on this project’s GitHub repo. Our output is a new shapefile in which we write the the elevation data for each grid. Load Libraries We will use the following packages in this tutorial: sf: to conduct basic spatial data manipulation tidyverse: to perform simple statistical analyses raster: to manipulate and analyze raster data gstat: to conduct geostatistical modeling and simulation tamp: to create spatial data visualization velox: to manipulate raster data in time efficient manner The only package that is worth special mention the “velox,” which has excellent performance in fast raster data manipulation. Unfortunately, this package is no longer available. For the lack of substitutes, however, we will still utilize the velox package in this tutorial. To install velox from archive, we can run the following code. library(devtools) devtools::install_github(&quot;https://github.com/hunzikp/velox&quot;) (Note, you may have to install an installer, such as Xcode, in order to compile the package locally. Some helpful information can be found here.) library(sf) library(tidyverse) library(raster) library(gstat) library(tmap) library(velox) 5.14.1 Load Data Loading data is the next step. We load the “grid” data (in kilometers), which is named km.grid, the shapefile of large counties, which is named lac, and also the elevation data, which is named lac.elevation. km.grid &lt;- st_read(&quot;./data/Km_Grid&quot;) ## Reading layer `Km_Grid&#39; from data source `/Users/LorenzMenendez/Desktop/OpenAirQ-toolkit/data/Km_Grid&#39; using driver `ESRI Shapefile&#39; ## Simple feature collection with 29903 features and 1 field ## geometry type: POLYGON ## dimension: XY ## bbox: xmin: -88.78167 ymin: 40.73704 xmax: -86.46887 ymax: 43.19439 ## geographic CRS: WGS 84 lac &lt;- st_read(&quot;./data/LargeAreaCounties&quot;) ## Reading layer `LargeAreaCounties&#39; from data source `/Users/LorenzMenendez/Desktop/OpenAirQ-toolkit/data/LargeAreaCounties&#39; using driver `ESRI Shapefile&#39; ## Simple feature collection with 21 features and 8 fields ## geometry type: MULTIPOLYGON ## dimension: XY ## bbox: xmin: -88.77879 ymin: 40.73641 xmax: -86.46629 ymax: 43.21831 ## geographic CRS: WGS 84 lac.elevation &lt;- raster(&quot;./data/lac.elevation.grd&quot;) The elevation data comes from the 3D Elevation Program (3DEP) of United States Geological Survey. One nice thing about this program is that all of its data are available “free of charge and without use restrictions”. If you are interested in learning more about what data are available, feel free to explore this website. For more detailed description of the data, please refer to the main chapters of this tutorial book. 5.15 Inspecting Data Before we proceed any further, let’s check the resolution of our elevation data first. The resolution is shown in degrees because that is the unit of CRS. It is worth pointing out that the resolution of the original raster data file is finer than what is shown below. Unfortunately, because Github only supports data files that are less than 100 MB, we downsampled the raster data to decrease its size. In other words, we decreased the resolution of the raster data by applying the aggregate function in the “raster” package. The good news is that the current resolution (after the downsampling) is sufficient for our purposes. raster::res(lac.elevation) ## [1] 0.0005555556 0.0005555556 We can now plot a raster map, as shown below. tm_shape(lac.elevation) + tm_raster(alpha = .5) + tm_shape(lac) + tm_borders() + tm_layout(legend.outside = TRUE, main.title = &quot;Elevation of Large Counties in Midwest&quot;, main.title.size = 1.2) 5.16 Data Manipulation Raster data manipulation is where the velox packages really shines as it contains a large number of functions that you will find useful in handling raster data. For example, aggregate allows users to aggregate a VeloxRaster object to a lower resolution. Another function, crop, as its name suggests, lets you crop a VeloxRaster function. If you want to rasterize a set of polygons, then you can use rasterize. There are many more functions that are of interest in raster data manipulation, but it is impossible to list all of them here. Detailed documentation about this package can be found here, and you should feel free to explore it at your own pace. In our tutorial, we will mainly be using velox and extract. First, we call the velox function to convert lac.elevation into a VeloxRaster object. Then we extract the mean value of each grid (km \\(\\times\\) km), and we name it km.elecation.vx. Note that “velox” is indeed good at fast raster data manipulation - running the two lines of code takes very little time. elevation.vx &lt;- velox(lac.elevation) km.elevation.vx &lt;- elevation.vx$extract(km.grid, fun = mean) head(km.elevation.vx) ## [,1] ## 1 266.9327 ## 2 267.1034 ## 3 267.6558 ## 4 264.9626 ## 5 272.6876 ## 6 285.0676 Let’s pause for a second and examine the object km.elevation.vx. As we would expect, it is just a list of the averages of each grid cell. Next, we join this elevation data with km.grid, which we loaded earlier but haven’t touched at all. km.grid$Elevation &lt;- as.numeric(km.elevation.vx) 5.17 Plotting Grid Map With the data ready, we can now plot a graph. tm_shape(km.grid) + tm_fill(&quot;Elevation&quot;) + tm_layout(legend.outside = TRUE, main.title = &quot;Elevation of Large Counties in Midwest&quot;, main.title.size = 1.2) We now have a nice grid map - we have successfully accomplished the goal of this tutorial. To save the results of all this work, we can use st_write to output a new shapefile. st_write(km.grid, &quot;Elevation_Grid.shp&quot;) This is the end of the tutorial. The example of the elevation data is not extraordinarily exciting on its own. It is only used to demonstrate the techniques to create grid cells from raster data. The “velox” introduced in this tutorial is sadly no longer available. However, we still recommend trying out this package as its ability to perform fast raster processing is unmatched. "],["F-Roads_Data.html", "Appendix F: Roads Data 5.18 Overview 5.19 Environment Setup 5.20 Data Manipulation 5.21 Outputting Data", " Appendix F: Roads Data 5.18 Overview Transportation system is a significant source of air pollution, and roads are without doubt the most important component of transportation system. The ability to manipulate roads data is therefore of critical importance. In this tutorial, we will take a look at the major roads in several large Midwest counties, and we will introduce other techniques, such as how to handle data with physical units, along the way. Our objectives are to: perform spatial data manipulation using common functions such as st_intersection and st_join learn to work with data of type “units” summarize data by groups and output results 5.19 Environment Setup Input/Output Our input includes the shapefile of several large counties in the Midwest, a grid data file, and the shapefile of the major roads in Illinois, Indiana, Wisconsin, and Michigan. The files can be found here. Insert Link! Our output is a shapefile that records the lengths of primary raods, secondary roads, and motorways in each 1 km km grid. Load Libraries We start by loading the following packages: raster: to manipulate and analyze raster data tidyverse: to perform simple statistical analyses sf: to conduct basic spatial data manipulation velox: to manipulate raster data in time efficient manner units: to handle numeric values with physical measurement units It should be mentioned here that the “velox” package, which is great at fast raster data manipulation, is sadly no longer available. Detailed instructions for installing this package from R archive are included in other parts of this tutorial book and can also be found here. library(raster) library(tidyverse) library(sf) library(velox) library(units) Load Data With the packages ready, we now load the data needed for this tutorial - roads is the shapefile for the major roads in Illinois, Indiana, Wisconsin, and Michigan, km.grid is the grid data, and counties is the shapefile for 21 large counties in the Midwest. All of the data can be loaded using the st_read function. roads &lt;- st_read(&quot;./data/4StateMajorRoads&quot;) ## Reading layer `4StateMajorRoads&#39; from data source `/Users/LorenzMenendez/Desktop/OpenAirQ-toolkit/data/4StateMajorRoads&#39; using driver `ESRI Shapefile&#39; ## Simple feature collection with 261863 features and 11 fields ## geometry type: LINESTRING ## dimension: XY ## bbox: xmin: -92.87577 ymin: 36.97244 xmax: -82.41341 ymax: 47.47978 ## geographic CRS: WGS 84 km.grid &lt;- st_read(&quot;./data/Km_Grid&quot;) ## Reading layer `Km_Grid&#39; from data source `/Users/LorenzMenendez/Desktop/OpenAirQ-toolkit/data/Km_Grid&#39; using driver `ESRI Shapefile&#39; ## Simple feature collection with 29903 features and 1 field ## geometry type: POLYGON ## dimension: XY ## bbox: xmin: -88.78167 ymin: 40.73704 xmax: -86.46887 ymax: 43.19439 ## geographic CRS: WGS 84 counties &lt;- st_read(&quot;./data/LargeAreaCounties&quot;) ## Reading layer `LargeAreaCounties&#39; from data source `/Users/LorenzMenendez/Desktop/OpenAirQ-toolkit/data/LargeAreaCounties&#39; using driver `ESRI Shapefile&#39; ## Simple feature collection with 21 features and 8 fields ## geometry type: MULTIPOLYGON ## dimension: XY ## bbox: xmin: -88.77879 ymin: 40.73641 xmax: -86.46629 ymax: 43.21831 ## geographic CRS: WGS 84 5.20 Data Manipulation Prelimnary Work First, we create a new field in km.grid called DATA. This field is defined as a sequence of positive integers. Basically, we are assigning a unqie ID number of each features in km.grid. It might be puzzling at this time what function the DATA field serves, but it will make sense later when we perform spatial joins. km.grid$DATA &lt;- seq(1:length(km.grid$DATA)) Our next step is to take the spatial intersection of roads and km.grid. To accomplish this, we simply call the st_intersection function, which does exactly what its name suggests. roads.intersection &lt;- st_intersection(roads, km.grid) To get the length of the roads, we use the st_length function. The geometry of raods is of the type lingstring, whose length is easily computed by the st_lengthfunction. We then store the output (i.e. the lengths of roads) in the sf object roads.intersection by creating a new field called length. roads.intersection$length &lt;- st_length(roads.intersection) Now, let’s pause for a second and inspect the DATA field of roads.intersection. Recall that we created the DATA field as an identifier of the grid. Other than that the values of DATA is right-skewed, the summary statistics do not tell us much useful information. However, it is nevertheless a good habit to inspect your data from time to time so that you are aware of what data you are working with. summary(roads.intersection$DATA) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 3 5545 10895 10140 13279 29903 Primary Roads Next, we want to take a look at all roads that are in the class “primary” or “primary_link.” More technically, we would like to extract all features of roads.intersection that has value “primary” or “primary_link” in the field fclass. To achieve this, we take advantage of the which function, which returns the positions of all features that satisfy the criterion. Then we simply bond pri.1 and pri.2 into a new vector, and this new vector pri.id has the positions of all the features that we want. pri.1 &lt;- which(roads.intersection$fclass == &quot;primary&quot;) pri.2 &lt;- which(roads.intersection$fclass == &quot;primary_link&quot;) pri.id &lt;- c(pri.1, pri.2) To get the roads that are classified as either “primary” or primary_link\", we simply use the pri.id we defined above to subset roads.intersection. It should be noted that there are many ways to extract the features that we want from roads.intersection. The method we use here is one of the most straightforward ones, but there are many alternatives, some of which are potentially better than the one shown here. In most cases, deciding which method to use is simply a matter of personal preference. primary.int &lt;- roads.intersection[pri.id,] It is now a time to do a spatial join, which can be done using the st_join function. In plain English, we basically combine the sf objects km.grid and primary.int into a new sf object called primary.merged. primary.merged &lt;- st_join(km.grid, primary.int) Now, let’s take a look at primary. This is just an sf object with 14 fields and 68769 features. primary.merged ## Simple feature collection with 68769 features and 14 fields ## geometry type: POLYGON ## dimension: XY ## bbox: xmin: -88.78167 ymin: 40.73704 xmax: -86.46887 ymax: 43.19439 ## geographic CRS: WGS 84 ## First 10 features: ## DATA.x osm_id code fclass name ref oneway maxspeed layer bridge tunnel ## 1 1 &lt;NA&gt; NA &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; NA NA &lt;NA&gt; &lt;NA&gt; ## 2 2 &lt;NA&gt; NA &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; NA NA &lt;NA&gt; &lt;NA&gt; ## 3 3 21662145 5113 primary State Highway 67 WI 67 B 0 0 F F ## 3.1 3 21662145 5113 primary State Highway 67 WI 67 B 0 0 F F ## 4 4 &lt;NA&gt; NA &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; NA NA &lt;NA&gt; &lt;NA&gt; ## 5 5 &lt;NA&gt; NA &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; NA NA &lt;NA&gt; &lt;NA&gt; ## 6 6 &lt;NA&gt; NA &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; NA NA &lt;NA&gt; &lt;NA&gt; ## 7 7 &lt;NA&gt; NA &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; NA NA &lt;NA&gt; &lt;NA&gt; ## 8 8 &lt;NA&gt; NA &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; NA NA &lt;NA&gt; &lt;NA&gt; ## 9 9 &lt;NA&gt; NA &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; NA NA &lt;NA&gt; &lt;NA&gt; ## path DATA.y length ## 1 &lt;NA&gt; NA NA [m] ## 2 &lt;NA&gt; NA NA [m] ## 3 /Users/isaackamber/Downloads/wisconsin-latest-free.shp/gis_osm_roads_free_1.shp 3 930.0061 [m] ## 3.1 /Users/isaackamber/Downloads/wisconsin-latest-free.shp/gis_osm_roads_free_1.shp 57 756.1883 [m] ## 4 &lt;NA&gt; NA NA [m] ## 5 &lt;NA&gt; NA NA [m] ## 6 &lt;NA&gt; NA NA [m] ## 7 &lt;NA&gt; NA NA [m] ## 8 &lt;NA&gt; NA NA [m] ## 9 &lt;NA&gt; NA NA [m] ## geometry ## 1 POLYGON ((-88.53387 43.1943... ## 2 POLYGON ((-88.52207 43.1943... ## 3 POLYGON ((-88.51027 43.1943... ## 3.1 POLYGON ((-88.51027 43.1943... ## 4 POLYGON ((-88.49847 43.1943... ## 5 POLYGON ((-88.48667 43.1943... ## 6 POLYGON ((-88.47487 43.1943... ## 7 POLYGON ((-88.46307 43.1943... ## 8 POLYGON ((-88.45127 43.1943... ## 9 POLYGON ((-88.43947 43.1943... The next step is to aggregate the lengths of the roads. Recall that we created the DATA field in km.grid at the beginning of this tutorial as a unique identifier of the grid. Now, we group the data by DATA.x (notice the name of the field changed after the spatial join), and we summarize the data using the sum function. We store the result of the this summation into the field length. primary.roadlengths &lt;- primary.merged %&gt;% group_by(DATA.x) %&gt;% summarize(length = sum(length)) What we have obtained above is the total length of “primary” and “primary_link” roads in each grid. The new sf object, grouped by grids, is named primary.roadlengths. Let’s take a quick look at what information it contains. primary.roadlengths ## Simple feature collection with 29903 features and 2 fields ## geometry type: POLYGON ## dimension: XY ## bbox: xmin: -88.78167 ymin: 40.73704 xmax: -86.46887 ymax: 43.19439 ## geographic CRS: WGS 84 ## # A tibble: 29,903 x 3 ## DATA.x length geometry ## &lt;int&gt; [m] &lt;POLYGON [°]&gt; ## 1 1 NA ((-88.53387 43.19439, -88.52207 43.19439, -88.52207 43.18606, -88.53387 43.18606, -88.533... ## 2 2 NA ((-88.52207 43.19439, -88.51027 43.19439, -88.51027 43.18606, -88.52207 43.18606, -88.522... ## 3 3 1686.194 ((-88.51027 43.19439, -88.49847 43.19439, -88.49847 43.18606, -88.51027 43.18606, -88.510... ## 4 4 NA ((-88.49847 43.19439, -88.48667 43.19439, -88.48667 43.18606, -88.49847 43.18606, -88.498... ## 5 5 NA ((-88.48667 43.19439, -88.47487 43.19439, -88.47487 43.18606, -88.48667 43.18606, -88.486... ## 6 6 NA ((-88.47487 43.19439, -88.46307 43.19439, -88.46307 43.18606, -88.47487 43.18606, -88.474... ## 7 7 NA ((-88.46307 43.19439, -88.45127 43.19439, -88.45127 43.18606, -88.46307 43.18606, -88.463... ## 8 8 NA ((-88.45127 43.19439, -88.43947 43.19439, -88.43947 43.18606, -88.45127 43.18606, -88.451... ## 9 9 NA ((-88.43947 43.19439, -88.42767 43.19439, -88.42767 43.18606, -88.43947 43.18606, -88.439... ## 10 10 NA ((-88.42767 43.19439, -88.41587 43.19439, -88.41587 43.18606, -88.42767 43.18606, -88.427... ## # … with 29,893 more rows We see that only certain features has numeric values in the field length, which makes perfect sense as roads only cross a few grids. It would be quite strange if we see every feature as a large numeric value in the field length - that would suggest that the entire land is covered with roads, which is the case in the real world. The next thing we want to deal with is the unit. Note that the values in the field length is of the type “units” i.e. a numeric value with a physical measurement unit. The original unit of length is a little complicated. We will simplify things by just assigning it the unit “meter”. units(primary.roadlengths$length) &lt;- with(ud_units, m) What we want to do is to replace all missing values in length with “0 meter.” To achieve that, we first create a variable with value 0 and then assign it the unit “meter.” This newly created x0 is what will replace the missing values. x0 &lt;- 0 units(x0) &lt;- with(ud_units, m) We use the which function to return the positions of all missing values, and on those exact position, we replace the value with x0. Hence, all missing values now read “0 meter.” primary.roadlengths[which(is.na(primary.roadlengths$length)),2] &lt;- x0 As always, it is critical that we inspect our data from time to time. If we print out primary.roadlengths, we see that all missing values have indeed by replaced by “0 meter.” primary.roadlengths ## Simple feature collection with 29903 features and 2 fields ## geometry type: POLYGON ## dimension: XY ## bbox: xmin: -88.78167 ymin: 40.73704 xmax: -86.46887 ymax: 43.19439 ## geographic CRS: WGS 84 ## # A tibble: 29,903 x 3 ## DATA.x length geometry ## &lt;int&gt; [m] &lt;POLYGON [°]&gt; ## 1 1 0.000 ((-88.53387 43.19439, -88.52207 43.19439, -88.52207 43.18606, -88.53387 43.18606, -88.533... ## 2 2 0.000 ((-88.52207 43.19439, -88.51027 43.19439, -88.51027 43.18606, -88.52207 43.18606, -88.522... ## 3 3 1686.194 ((-88.51027 43.19439, -88.49847 43.19439, -88.49847 43.18606, -88.51027 43.18606, -88.510... ## 4 4 0.000 ((-88.49847 43.19439, -88.48667 43.19439, -88.48667 43.18606, -88.49847 43.18606, -88.498... ## 5 5 0.000 ((-88.48667 43.19439, -88.47487 43.19439, -88.47487 43.18606, -88.48667 43.18606, -88.486... ## 6 6 0.000 ((-88.47487 43.19439, -88.46307 43.19439, -88.46307 43.18606, -88.47487 43.18606, -88.474... ## 7 7 0.000 ((-88.46307 43.19439, -88.45127 43.19439, -88.45127 43.18606, -88.46307 43.18606, -88.463... ## 8 8 0.000 ((-88.45127 43.19439, -88.43947 43.19439, -88.43947 43.18606, -88.45127 43.18606, -88.451... ## 9 9 0.000 ((-88.43947 43.19439, -88.42767 43.19439, -88.42767 43.18606, -88.43947 43.18606, -88.439... ## 10 10 0.000 ((-88.42767 43.19439, -88.41587 43.19439, -88.41587 43.18606, -88.42767 43.18606, -88.427... ## # … with 29,893 more rows Secondary Roads From this point on, things start to get a little repetitive. We, in this section, repeat lots of code from the last section. The only difference is that we now want to extract the information - basically the lengths of the roads - of all roads that are classified as “secondary” or “secondary_link.” For illustrative purposes, we show all the code below, but we shall not comment much on the code as it is almost identical to the code in the last section. sec.1 &lt;- which(roads.intersection$fclass == &quot;secondary&quot;) sec.2 &lt;- which(roads.intersection$fclass == &quot;secondary_link&quot;) sec.id &lt;- c(sec.1, sec.2) secondary.int &lt;- roads.intersection[sec.id,] secondary.merged &lt;- st_join(km.grid, secondary.int) secondary.roadlengths = secondary.merged %&gt;% group_by(DATA.x) %&gt;% summarize(length = sum(length)) secondary.roadlengths[which(is.na(secondary.roadlengths$length)),2] &lt;- x0 We inspect secondary.roadlengths, and it looks fine in terms of both data format and data value. secondary.roadlengths ## Simple feature collection with 29903 features and 2 fields ## geometry type: POLYGON ## dimension: XY ## bbox: xmin: -88.78167 ymin: 40.73704 xmax: -86.46887 ymax: 43.19439 ## geographic CRS: WGS 84 ## # A tibble: 29,903 x 3 ## DATA.x length geometry ## &lt;int&gt; [m] &lt;POLYGON [°]&gt; ## 1 1 0.000 ((-88.53387 43.19439, -88.52207 43.19439, -88.52207 43.18606, -88.53387 43.18606, -88.533... ## 2 2 0.000 ((-88.52207 43.19439, -88.51027 43.19439, -88.51027 43.18606, -88.52207 43.18606, -88.522... ## 3 3 0.000 ((-88.51027 43.19439, -88.49847 43.19439, -88.49847 43.18606, -88.51027 43.18606, -88.510... ## 4 4 0.000 ((-88.49847 43.19439, -88.48667 43.19439, -88.48667 43.18606, -88.49847 43.18606, -88.498... ## 5 5 0.000 ((-88.48667 43.19439, -88.47487 43.19439, -88.47487 43.18606, -88.48667 43.18606, -88.486... ## 6 6 0.000 ((-88.47487 43.19439, -88.46307 43.19439, -88.46307 43.18606, -88.47487 43.18606, -88.474... ## 7 7 1853.511 ((-88.46307 43.19439, -88.45127 43.19439, -88.45127 43.18606, -88.46307 43.18606, -88.463... ## 8 8 0.000 ((-88.45127 43.19439, -88.43947 43.19439, -88.43947 43.18606, -88.45127 43.18606, -88.451... ## 9 9 0.000 ((-88.43947 43.19439, -88.42767 43.19439, -88.42767 43.18606, -88.43947 43.18606, -88.439... ## 10 10 0.000 ((-88.42767 43.19439, -88.41587 43.19439, -88.41587 43.18606, -88.42767 43.18606, -88.427... ## # … with 29,893 more rows Motorway The code in this section is exactly the same as in the last two section. We include the code here for the sake of completeness, but you should feel free to skip this section. mot.1 &lt;- which(roads.intersection$fclass == &quot;motorway&quot;) mot.2 &lt;- which(roads.intersection$fclass == &quot;motorway_link&quot;) mot.id &lt;- c(mot.1, mot.2) motorway.int &lt;- roads.intersection[mot.id,] motorway.merged &lt;- st_join(km.grid, motorway.int) motorway.roadlengths = motorway.merged %&gt;% group_by(DATA.x) %&gt;% summarize(length = sum(length)) motorway.roadlengths[which(is.na(motorway.roadlengths$length)),2] &lt;- x0 Before we proceed, we take a galnce at motorway.roadlengths. motorway.roadlengths ## Simple feature collection with 29903 features and 2 fields ## geometry type: POLYGON ## dimension: XY ## bbox: xmin: -88.78167 ymin: 40.73704 xmax: -86.46887 ymax: 43.19439 ## geographic CRS: WGS 84 ## # A tibble: 29,903 x 3 ## DATA.x length geometry ## &lt;int&gt; [m] &lt;POLYGON [°]&gt; ## 1 1 0 ((-88.53387 43.19439, -88.52207 43.19439, -88.52207 43.18606, -88.53387 43.18606, -88.533... ## 2 2 0 ((-88.52207 43.19439, -88.51027 43.19439, -88.51027 43.18606, -88.52207 43.18606, -88.522... ## 3 3 0 ((-88.51027 43.19439, -88.49847 43.19439, -88.49847 43.18606, -88.51027 43.18606, -88.510... ## 4 4 0 ((-88.49847 43.19439, -88.48667 43.19439, -88.48667 43.18606, -88.49847 43.18606, -88.498... ## 5 5 0 ((-88.48667 43.19439, -88.47487 43.19439, -88.47487 43.18606, -88.48667 43.18606, -88.486... ## 6 6 0 ((-88.47487 43.19439, -88.46307 43.19439, -88.46307 43.18606, -88.47487 43.18606, -88.474... ## 7 7 0 ((-88.46307 43.19439, -88.45127 43.19439, -88.45127 43.18606, -88.46307 43.18606, -88.463... ## 8 8 0 ((-88.45127 43.19439, -88.43947 43.19439, -88.43947 43.18606, -88.45127 43.18606, -88.451... ## 9 9 0 ((-88.43947 43.19439, -88.42767 43.19439, -88.42767 43.18606, -88.43947 43.18606, -88.439... ## 10 10 0 ((-88.42767 43.19439, -88.41587 43.19439, -88.41587 43.18606, -88.42767 43.18606, -88.427... ## # … with 29,893 more rows 5.21 Outputting Data We now enter the last section of this tutorial, where we would like to save all the work we have done so far. This is not difficult - we just create three fields in km.grid (primary, secondary, and motorway), where we store the lengths of the roads. km.grid$primary &lt;- primary.roadlengths$length km.grid$secondary &lt;- secondary.roadlengths$length km.grid$motorway &lt;- motorway.roadlengths$length Then, we output a new shapefile with the modified km.grid using the st_write function. st_write(km.grid, &quot;Road_Grid.shp&quot;) This concludes our tutorial. In this exercise, we showed how roads data can be handled. Of course, some of the techniques introduced here can be applied in many other instances. The brief introduction on dealing with data of the type “units” should be particularly helpful if you have not been previously exposed to this data type. You should feel free to explore more on your own and apply what you have learned to your own research. "]]
