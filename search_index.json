[
["index.html", "Point Sensors to Surfaces Preface Hello World!", " Point Sensors to Surfaces Authors Names Here 12/19/2019 Preface Hello World! This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com. When you click the Knit button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this: summary(cars) ## speed dist ## Min. : 4.0 Min. : 2.00 ## 1st Qu.:12.0 1st Qu.: 26.00 ## Median :15.0 Median : 36.00 ## Mean :15.4 Mean : 42.98 ## 3rd Qu.:19.0 3rd Qu.: 56.00 ## Max. :25.0 Max. :120.00 "],
["vector-data-mapping.html", "1 Vector Data Mapping 1.1 Required Packages 1.2 Political Boundaries 1.3 Ground-based Data Sensor Locations 1.4 Point Sources of Pollution", " 1 Vector Data Mapping 1.1 Required Packages tmap: Flexible thematic mapping sf: Spatial vector data manipulation dplyr: data.frame manipulation tmap will be set to interactive mode for this tutorial. tmap_mode(&#39;view&#39;) ## tmap mode set to interactive viewing 1.2 Political Boundaries Air quality modeling occured over the Chicago metropolitan area. This geography consist of 21 individual counties within the states of Illinois, Indiana, and Wisconsin. Read in County Boundaries counties = sf::st_read(&#39;../data/LargeAreaCounties/LargeAreaCounties.shp&#39;) ## Reading layer `LargeAreaCounties&#39; from data source `/Users/LorenzMenendez/Desktop/OpenAirQ-toolkit/data/LargeAreaCounties/LargeAreaCounties.shp&#39; using driver `ESRI Shapefile&#39; ## Simple feature collection with 21 features and 8 fields ## geometry type: MULTIPOLYGON ## dimension: XY ## bbox: xmin: -88.77879 ymin: 40.73641 xmax: -86.46629 ymax: 43.21831 ## CRS: 4326 Plot with tmap tm_shape(counties) + tm_borders() + tm_text(&quot;COUNTYNAME&quot;, size = 0.7, along.lines = TRUE) + tm_fill(col = &quot;STATE&quot;, alpha = 0.5) Air modeling data, however, is collected at a larger spatial scale to account for region-wide effects that could affect the 21 country study area. Four midwestern states (Illinois, Indiana, Michigan, and Wisconsin) were chosen as a data collection area. states = sf::st_read(&#39;../data/FourStates/FourStates.shp&#39;) ## Reading layer `FourStates&#39; from data source `/Users/LorenzMenendez/Desktop/OpenAirQ-toolkit/data/FourStates/FourStates.shp&#39; using driver `ESRI Shapefile&#39; ## Simple feature collection with 4 features and 14 fields ## geometry type: POLYGON ## dimension: XY ## bbox: xmin: -92.88943 ymin: 36.9703 xmax: -82.12297 ymax: 48.30606 ## CRS: 4326 tm_shape(states) + tm_borders() + tm_text(&quot;NAME&quot;, size = 0.8, auto.placement = TRUE) + tm_fill(col = &quot;NAME&quot;, alpha = 0.5) + tm_shape(counties) + tm_fill(col = &quot;black&quot;, alpha = 0.25) + tm_borders(col = &quot;black&quot;, alpha = 0.25) 1.3 Ground-based Data Sensor Locations 1.3.1 EPA Particulate Matter Sensors Over 127 PM2.5 pollution monitoring stations are located across the four state data collection area. The map below describes the distribution of these point locations. More information on the data collection methods and output data from each sensor will be discussed later on. sensors = sf::st_read(&#39;../data/PM25_4States_2014.2018_POINTLOCS.geojson&#39;) %&gt;% dplyr::mutate(rec_duration = as.numeric(lastRec - firstRec)) ## Reading layer `PM25_4States_2014.2018_POINTLOCS&#39; from data source `/Users/LorenzMenendez/Desktop/OpenAirQ-toolkit/data/PM25_4States_2014.2018_POINTLOCS.geojson&#39; using driver `GeoJSON&#39; ## Simple feature collection with 127 features and 3 fields ## geometry type: POINT ## dimension: XY ## bbox: xmin: -91.2269 ymin: 37.97444 xmax: -82.45623 ymax: 46.60225 ## CRS: 4326 basemap + tm_shape(sensors) + tm_markers(shape = tmap_icons(&#39;https://github.com/GeoDaCenter/OpenAirQ-toolkit/blob/master/data/assets/EPA_logo.png?raw=true&#39;)) 1.3.2 Weather Stations High temporal resolution weather data was sourced from a large network of ground-based weather stations co-located at many airports. They provide data at regular one hour intervals on variables such as temperature, pressure, wind velocity, and wind direction. The map below describes the distribution of sensors in the four-state data collection area. asos = sf::st_read(&#39;../data/4States_ASOS_2018_Locations.geojson&#39;) ## Reading layer `4States_ASOS_2018_Locations&#39; from data source `/Users/LorenzMenendez/Desktop/OpenAirQ-toolkit/data/4States_ASOS_2018_Locations.geojson&#39; using driver `GeoJSON&#39; ## Simple feature collection with 250 features and 2 fields ## geometry type: POINT ## dimension: XY ## bbox: xmin: -92.69 ymin: 37.0647 xmax: -82.52886 ymax: 47.46694 ## CRS: 4326 basemap + tm_shape(asos) + tm_markers(shape = tmap_icons(&#39;https://github.com/GeoDaCenter/OpenAirQ-toolkit/blob/master/data/assets/airport_icon.png?raw=true&#39;)) 1.4 Point Sources of Pollution The EPA National Emissions Inventory (NEI) dataset (2014) describes the location of large sources of pollution, such as powerplants, factories, and other industrial buildings. points.pollution = sf::st_read(&#39;../data/Point_Source_Emissions_4States_2014.geojson&#39;) ## Reading layer `Point_Source_Emissions_4States_2014&#39; from data source `/Users/LorenzMenendez/Desktop/OpenAirQ-toolkit/data/Point_Source_Emissions_4States_2014.geojson&#39; using driver `GeoJSON&#39; ## Simple feature collection with 33783 features and 10 fields ## geometry type: POINT ## dimension: XY ## bbox: xmin: -92.7558 ymin: 37.01605 xmax: -82.4233 ymax: 47.1859 ## CRS: 4326 basemap + tm_shape(points.pollution) + tm_markers() "],
["data-prep-management.html", "2 Data Prep &amp; Management 2.1 EPA Pollution Data 2.2 FAA Weather Data 2.3 Introduction 2.4 Loading the Required Packages 2.5 Read and Examine the Data 2.6 Data Wrangling 2.7 Creating a Spatial Object using sp 2.8 Data Visualization Review 2.9 Constructing a Kernal Density Estimation", " 2 Data Prep &amp; Management This project uses weather and pollution data from remotely sensed satellite imagery, but also ground based sensors maintained by the EPA and FAA to model air quality in the Midwest. Using the ground sensors, the team can attempt to predict pollutions levels based on satellite data. This chapter focuses on how weather and pollution data from ground sensors was downloaded and prepared for use in refining the prediction. 2.1 EPA Pollution Data Figure 2.1: EPA Pollution Monitoring Site (EPA.gov) EPA data was seamlessly imported into R using the aqsr package by Joshua P. Keller at Colorado State University. The package takes advanatge of the EPA AQS DataMart API to load data in R as data.frame objects with only a couple lines of code. It allows users to query for sensor data across multiple air quality variables, geographies, and timeframes. Let’s get started by downloading the package. # devtools::install_github(&quot;jpkeller/aqsr&quot;) library(aqsr) 2.1.1 Getting Started This section describes the process for querying EPA sensor data using the aqsr package. For more information on how each function works, please reference the package documentation. Obtaining an API Key For first time users of the AQS DataMart API, you must first register your email to recieve an API key. (Users who already have a DataMart API key, please skep to the next step). The API key is a required input for all querying functions in the aqsr package. Obtaining a key is made simple by calling the aqs_signup() function and inputting your own email address. aqs_signup(&#39;YourEmailHere@uchicago.edu&#39;) Save your API key from the email confirmation for future reference. In case you don’t recieve an email, verify that your email address was typed correctly, and check your spam folder. 2.1.1.1 Using your API Key in aqsr Setup your AQI key with the aqr package by using the create_user() function. This way, you won’t have to keep typing your email and API key each time you query for data. myuser = create_user(email = &#39;YourEmailHere@uchicago.edu&#39;, key = &#39;apikey123&#39;) 2.1.2 PM2.5 Data Query This section describes how to query for PM2.5 concetration data from EPA pollution sensors. We are looking for at PM2.5 data Wisconsin, Illinois, and Indiana between 2014 and 2018 for our project. First, let’s start small and query only for Illinois data for the first week of 2018. IL.data = aqs_dailyData_byState(aqs_user = myuser, # Previously defined user emailand API key param = 88101, # EPA AQS Parameter Code for PM2.5 bdate = &quot;20180101&quot;, # Starting Date (Jan 1st ,2018) edate = &quot;20180107&quot;, # Ending Date (Jan 7th, 2018) state = &quot;17&quot;) # State FIPS Code for Illinois state_code county_code site_number parameter_code poc latitude longitude datum parameter sample_duration 2 17 065 0002 88101 3 38.08216 -88.62494 WGS84 PM2.5 - Local Conditions 1 HOUR 2100 17 065 0002 88101 3 38.08216 -88.62494 WGS84 PM2.5 - Local Conditions 1 HOUR 3 17 065 0002 88101 3 38.08216 -88.62494 WGS84 PM2.5 - Local Conditions 1 HOUR 4 17 065 0002 88101 3 38.08216 -88.62494 WGS84 PM2.5 - Local Conditions 1 HOUR 5 17 065 0002 88101 3 38.08216 -88.62494 WGS84 PM2.5 - Local Conditions 1 HOUR The outputted data frame includes many fields regarding the PM2.5 observation, including spatial data for the sensor’s location. We will focus on these details later on in our data wrangling process. The next code chunk describes how to query for PM2.5 data across our three states and four years. library(dplyr) # List of States to Iterate Through states = c(&quot;17&quot;, &quot;18&quot;, &quot;55&quot;) # Matrix of Start Dates and End Dates to Iterate Through dates = matrix(c(&quot;20140101&quot;, &quot;20141231&quot;, &quot;20150101&quot;, &quot;20151231&quot;, &quot;20160101&quot;, &quot;20161231&quot;, &quot;20170101&quot;, &quot;20171231&quot;, &quot;20180101&quot;, &quot;20181231&quot;), ncol = 2, byrow = TRUE) # Leveraging apply functions to iterate through both states and dates full.data = lapply(states, function(x){ mapply(aqs_dailyData_byState, bdate = dates[,1], edate = dates[,2], MoreArgs = list(aqs_user = myuser, param = 88101, state = x), SIMPLIFY = FALSE ) %&gt;% do.call(&quot;rbind&quot;, .) }) %&gt;% do.call(&quot;rbind&quot;, .) 2.2 FAA Weather Data Figure 2.2: An ASOS Observation Station in Elko, NV. (Wikimedia Commons) FAA weather data gathered from the Automated Surface Observing System (ASOS) can be imported using the riem package. This package, created by ROpenSci, queries weather data from the Iowa Environmental Mesonet, an online portal for international ASOS data maintained by Iowa State University. First, let’s load the package. # devtools::install_github(&#39;ropensci/riem&#39;) library(riem, quietly = TRUE) 2.2.1 Sample Query Below is an R code snippet that performs the simplest weather data query possible in the riem package. It specifies a particular weather station using an airport code and a date range to query for. The output is a tibble table of raw ASOS weather data. The code snippet below extracts sensor data at the San Francisco International Airport. SFO.weather = riem_measures(station = &#39;KSFO&#39;, date_start = &quot;2014-01-01&quot;, date_end = &#39;2014-01-02&#39;) station valid tmpf dwpf relh drct SFO 2014-01-01 00:56:00 53.96 42.98 66.28 290 SFO 2014-01-01 01:56:00 51.98 42.98 71.28 280 SFO 2014-01-01 02:56:00 51.08 44.06 76.80 290 SFO 2014-01-01 03:56:00 48.92 37.94 65.67 0 SFO 2014-01-01 04:56:00 50.00 39.92 68.15 0 station valid tmpf dwpf alti vsby SFO 2014-01-01 00:56:00 53.96 42.98 30.16 10 SFO 2014-01-01 01:56:00 51.98 42.98 30.17 10 SFO 2014-01-01 02:56:00 51.08 44.06 30.17 10 SFO 2014-01-01 03:56:00 48.92 37.94 30.18 10 SFO 2014-01-01 04:56:00 50.00 39.92 30.18 10 SFO 2014-01-01 05:56:00 48.92 37.04 30.19 10 The outputted table shows weather data for a 24-hour period on January 1st, 2014 at the San Francisco International Airport. The valid column species when each weather report was generated, typically at 1-hour intervals. The tmpf and dwpf columns give the ambient air temperature and dew point in Fahrenheit (ºF). Other important variables in our project include air pressure (alti), measured in inches of mercury (in.Hg), and visibility (vsby) in miles. For more information on all available varibles, see Iowa State’s Documentation. Next, we will apply this function at a large scare across multiple sensors and timescales. 2.2.2 Finding ASOS Sensors The FAA collects weather data at hourly intervals for each meteorological station, with some stations providing half-hour intervals. Even querying for short periods of time can yield large amounts of data. To optimise performance, we want to only query data from stations in our study area. Finding Sensors by State In our project, we focus on certain counties in Illinois, Indiana, and Wisconsin, so we are interested in finding the sensors within that study area. The first step is to query the locations of all weather stations in the three states using the riem package. In the example below, we query for sensors in the Illinois ASOS sensor network. IL.stations = riem_stations(network = &#39;IL_ASOS&#39;) ## No encoding supplied: defaulting to UTF-8. id name lon lat ALN ALTON/ST LOUIS R -90.04599 38.88992 BMI BLOOMINGTON/NORM -88.91592 40.47711 CPS CAHOKIA/ST LOUIS -90.15622 38.57073 CIR Cairo -89.21960 37.06470 MDH CARBONDALE/MURPH -89.25000 37.78000 CUL Carmi -88.12306 38.08948 To query for data across multiple states, we are going the apply the riem_stations function to a list of weather station networks, as shown below. networks = list(&#39;IL_ASOS&#39;, &#39;IN_ASOS&#39;, &#39;WI_ASOS&#39;) library(dplyr, quietly = TRUE) station.locs = lapply(networks, riem::riem_stations) %&gt;% do.call(rbind, .) # Creates a single data table as output Note: You can find a list of state abbreviations by typing state.abb in your R console. Converting Latitude and Longitude Coordinates to Spatial Data The data tables returned by the riem package must be converted to spatial data to determine which sensors are located in the study area. Since the lon/lat coordinates are already provided, the data table is easily converted to a spatial sf object. station.locs.sf = sf::st_as_sf(station.locs, coords = c(&quot;lon&quot;, &quot;lat&quot;), crs = 4326) # Plot stations and study area boundaries to verify that the correct sensors were selected plot(station.locs.sf$geometry) plot(sf::st_read(&#39;https://uchicago.box.com/shared/static/uw0srt8nyyjfqo6l0dv07cyskwmv6r50.geojson&#39;, quiet = TRUE)$geometry, border = &#39;red&#39;, add = TRUE) We plot to results to verify that our query and data conversion process worked correctly. For reference, the boundaires of the study area is outlined in red. Selecting Sensors within our Study Area Next, we perform a spatial join to only keep the points located within the boundaries of our study area polygons. The spatial join is completed by the sf package, as shown below. For more information regarding spatial joins and spatial predicates, please see this helpful blog post by GISgeography.com. # Loading study area boundaries study.area = sf::st_read(&#39;https://uchicago.box.com/shared/static/uw0srt8nyyjfqo6l0dv07cyskwmv6r50.geojson&#39;, quiet = TRUE) study.sensors = sf::st_join(station.locs.sf, study.area, left = FALSE) # Verify Spatial Join by Plotting plot(study.area$geometry, border = &#39;red&#39;) plot(study.sensors$geometry, add = TRUE) title(&#39;Weather Stations Within the Study Area&#39;) Now that we have a dataset of which weather stations we are interested in, we can query for the weather data associated with each station. 2.2.3 Weather Data Query Again we use the lapply function in base R to execute the riem_measures function on a list of sensor IDs. This allows us to iteratively query for weather data from each individual sensor in a list. In the code snippet below, we take the study sensors obtained previously and query for a single day’s worth of weather data. library(dplyr, quietly = TRUE) weather.data = lapply(study.sensors$id, function(x){riem::riem_measures(x, date_start = &quot;2014-01-01&quot;, date_end = &quot;2014-01-02&quot;)}) %&gt;% do.call(rbind, .) # Creates a single data table as output station valid lon lat tmpf MDW 2014-01-01 00:49:00 -87.7524 41.786 12.20 MDW 2014-01-01 00:51:00 -87.7524 41.786 12.92 MDW 2014-01-01 01:51:00 -87.7524 41.786 12.92 MDW 2014-01-01 02:27:00 -87.7524 41.786 14.00 MDW 2014-01-01 02:51:00 -87.7524 41.786 14.00 Querying Full Weather Dataset Use caution when querying for a large amount of data. Data tables can easily become unwieldy after querying for a large number of weather stations across a wide time scale. The code snippet below downloads all ASOS weather data for sensors in our study area from January 1st 2014 to December 31st 2018, which is our study time period. It has approximately 4.8 Million records and takes 6-10 minutes to download. weather.data = lapply(study.sensors$id, function(x){riem::riem_measures(x, date_start = &quot;2014-01-01&quot;, date_end = &quot;2018-12-31&quot;)}) %&gt;% do.call(rbind, .) # Creates a single data table as output 2.3 Introduction This chapter will introduce how to convert point sensors to surfaces. In it, we will work with the CSV file for the 2017 National Emissions Inventory, downloadable from the EPA’s website here. If you wish to follow along with this chapter, please download the dataset now. If you have a specific interest area and would like to skip the data wrangling in R, you can download state-specific and pollutant-specific summaries from the NEI website. We will begin with a brief review of the basics of data wrangling and filter the relatively large CSV file to the considerably smaller subset of the data with which we are concerned. Then, we will reinforce the data visualization skills covered in a previous chapter by mapping out the point locations of emissions sources in our study area. Finally, we will transition into the process of creating a continuous surface in the form of a Kernal Density Estimation (KDE) of PM2.5 point emission source density in Cook County, Illinois. By the end of this tutorial you will be able to: Understand and wrangle National Emissions Inventory data Use the sp package in R Generate a Kernal Density Estimation using real data 2.4 Loading the Required Packages To process our data and create a Kernal Density Estimation, we will need the following packages: tidyverse (data wrangling) sp (Spatial data manipulation/analysis) rgdal (Spatial data) tmap (Spatial data visualization) spatialEco (Creating KDE) If you do not already have any of these packages installed, you will want to install them using install.package(\"*PackageName*\"). Once they are installed, we are going to load the required packages: library(tidyverse) library(sp) library(rgdal) library(spatialEco) library(tmap) library(readr) 2.5 Read and Examine the Data Now that we have loaded our required packages, we will now read in our National Emissions Inventory CSV. After unzipping the zipped folder downloaded from the EPA, you will have two files: “process_12345.csv” and “process678910.csv”. For the purposes of this chapter, we will only need “process_12345.csv”. This file is quite large, so beware that it may take 30 seconds to load. nei.data &lt;- readr::read_csv(&quot;../data/process_12345.csv.zip&quot;) Having successfully read our data into the R environment, let’s take a second to examine it. nrow(nei.data) ## [1] 3967781 names(nei.data) ## [1] &quot;epa region code&quot; &quot;state&quot; ## [3] &quot;fips state code&quot; &quot;tribal name&quot; ## [5] &quot;fips code&quot; &quot;county&quot; ## [7] &quot;eis facility id&quot; &quot;program system code&quot; ## [9] &quot;agency facility id&quot; &quot;tri facility id&quot; ## [11] &quot;company name&quot; &quot;site name&quot; ## [13] &quot;naics code&quot; &quot;naics description&quot; ## [15] &quot;facility source type&quot; &quot;site latitude&quot; ## [17] &quot;site longitude&quot; &quot;address&quot; ## [19] &quot;city&quot; &quot;zip code&quot; ## [21] &quot;postal abbreviation&quot; &quot;eis unit id&quot; ## [23] &quot;agency unit id&quot; &quot;unit type&quot; ## [25] &quot;unit description&quot; &quot;design capacity&quot; ## [27] &quot;design capacity uom&quot; &quot;eis process id&quot; ## [29] &quot;agency process id&quot; &quot;scc&quot; ## [31] &quot;reg codes&quot; &quot;reg code description&quot; ## [33] &quot;process description&quot; &quot;reporting period&quot; ## [35] &quot;emissions operating type&quot; &quot;calculation parameter value&quot; ## [37] &quot;calculation parameter uom&quot; &quot;calculation material&quot; ## [39] &quot;calculation parameter type&quot; &quot;calc data source&quot; ## [41] &quot;calc data year&quot; &quot;pollutant code&quot; ## [43] &quot;pollutant desc&quot; &quot;pollutant type(s)&quot; ## [45] &quot;total emissions&quot; &quot;emissions uom&quot; ## [47] &quot;emission factor&quot; &quot;ef numerator uom&quot; ## [49] &quot;ef denominator uom&quot; &quot;ef text&quot; ## [51] &quot;calc method code&quot; &quot;calculation method&quot; ## [53] &quot;emission comment&quot; &quot;source data set&quot; ## [55] &quot;data tagged&quot; &quot;data set&quot; As we can see, the dataset is huge, with over 3 million observations and 53 attributes. None of the existing spatial data packages in R are well equipped to handle such a dataset of such size. Luckily, we are only interested in a small subset of the data – PM2.5 emissions sources in Illinois, Michigan, Wisconsin, and Indiana. 2.6 Data Wrangling As a reminder, this dataset contains data for many pollutants across the entire United States. Looking at the code snippet above, we can see that the tibble contains columns for state abbreviations and pollutant descriptions, two factors which we are interested in filtering. First, let’s filter our tibble to only those observations within our state, Illinois. We are going to be using the filter() function from the dplyr package (included in the tidyverse). state.abbr &lt;- c(&quot;IL&quot;) state.nei &lt;- nei.data %&gt;% filter(state %in% state.abbr) nrow(state.nei) ## [1] 511731 With that, we’re already down to just 386,338 emissions sources. While we still have a ways to go with our filtering, this is certainly progress. Let’s take a second to look back over what we just did. The second line of this code is using the pipe (%&gt;%) operator to pipe in the complete nei dataset into the filter function covered in an earlier chapter. %in% is an infix operator that matches the items from the first vector (the complete list of state abbreviations for all point emissions sources) with those of the second (the state abbreviations for state of interest). This code is written this way to allow this code to be used for larger, multistate study areas. If you are interested in examining points from multiple states, simply add their abbreviations to the state.abbr vector. If you are only using one state, feel free to simplify the code to your liking. We are next going to filter our data down further to include only those points within Cook County, IL. county.names &lt;- c(&quot;Cook&quot;) county.nei &lt;- state.nei %&gt;% filter(county %in% county.names) nrow(county.nei) ## [1] 107692 Let’s finish filtering our data by restricting our results to only those emissions sources emitting PM2.5. We will first examine the different labels for pollution descriptions using the unique() function. We will then filter our dataset for only those labels that seem related to PM2.5 using the same process as above. unique(county.nei$`pollutant desc`) ## [1] &quot;Elemental Carbon portion of PM2.5-PRI&quot; ## [2] &quot;Sulfate Portion of PM2.5-PRI&quot; ## [3] &quot;Remaining PMFINE portion of PM2.5-PRI&quot; ## [4] &quot;Organic Carbon portion of PM2.5-PRI&quot; ## [5] &quot;Nitrate portion of PM2.5-PRI&quot; ## [6] &quot;Sulfur Dioxide&quot; ## [7] &quot;PM Condensible&quot; ## [8] &quot;PM10 Filterable&quot; ## [9] &quot;PM10 Primary (Filt + Cond)&quot; ## [10] &quot;Carbon Monoxide&quot; ## [11] &quot;Nitrogen Oxides&quot; ## [12] &quot;PM2.5 Primary (Filt + Cond)&quot; ## [13] &quot;Nitrous Oxide&quot; ## [14] &quot;Volatile Organic Compounds&quot; ## [15] &quot;PM2.5 Filterable&quot; ## [16] &quot;Naphthalene&quot; ## [17] &quot;Ammonia&quot; ## [18] &quot;Acrolein&quot; ## [19] &quot;Toluene&quot; ## [20] &quot;Hexane&quot; ## [21] &quot;Anthracene&quot; ## [22] &quot;Pyrene&quot; ## [23] &quot;Fluorene&quot; ## [24] &quot;Phenanthrene&quot; ## [25] &quot;Acenaphthene&quot; ## [26] &quot;Cobalt&quot; ## [27] &quot;Acetaldehyde&quot; ## [28] &quot;Selenium&quot; ## [29] &quot;Benzo[g,h,i,]Perylene&quot; ## [30] &quot;Indeno[1,2,3-c,d]Pyrene&quot; ## [31] &quot;Benzo[b]Fluoranthene&quot; ## [32] &quot;Fluoranthene&quot; ## [33] &quot;Benzo[k]Fluoranthene&quot; ## [34] &quot;Acenaphthylene&quot; ## [35] &quot;Chrysene&quot; ## [36] &quot;Formaldehyde&quot; ## [37] &quot;Benzo[a]Pyrene&quot; ## [38] &quot;Dibenzo[a,h]Anthracene&quot; ## [39] &quot;Benz[a]Anthracene&quot; ## [40] &quot;Benzene&quot; ## [41] &quot;Manganese&quot; ## [42] &quot;Mercury&quot; ## [43] &quot;Nickel&quot; ## [44] &quot;Arsenic&quot; ## [45] &quot;Beryllium&quot; ## [46] &quot;Cadmium&quot; ## [47] &quot;Chromium (VI)&quot; ## [48] &quot;Chromium III&quot; ## [49] &quot;Glycol Ethers&quot; ## [50] &quot;Xylenes (Mixed Isomers)&quot; ## [51] &quot;Methyl Isobutyl Ketone&quot; ## [52] &quot;Ethyl Benzene&quot; ## [53] &quot;Phenol&quot; ## [54] &quot;2,2,4-Trimethylpentane&quot; ## [55] &quot;Cumene&quot; ## [56] &quot;Hydrochloric Acid&quot; ## [57] &quot;Lead&quot; ## [58] &quot;2-Methylnaphthalene&quot; ## [59] &quot;Methanol&quot; ## [60] &quot;Vinyl Acetate&quot; ## [61] &quot;Tetrachloroethylene&quot; ## [62] &quot;1,2-Epoxybutane&quot; ## [63] &quot;Phosphorus&quot; ## [64] &quot;Ethylene Glycol&quot; ## [65] &quot;Biphenyl&quot; ## [66] &quot;Ethyl Acrylate&quot; ## [67] &quot;Diethanolamine&quot; ## [68] &quot;Styrene&quot; ## [69] &quot;Methyl Methacrylate&quot; ## [70] &quot;Methylene Chloride&quot; ## [71] &quot;Bis(2-Ethylhexyl)Phthalate&quot; ## [72] &quot;Ethylene Oxide&quot; ## [73] &quot;Trichloroethylene&quot; ## [74] &quot;Methyl Tert-Butyl Ether&quot; ## [75] &quot;2-Nitropropane&quot; ## [76] &quot;Propylene Oxide&quot; ## [77] &quot;Hydrogen Sulfide&quot; ## [78] &quot;Hydrogen Cyanide&quot; ## [79] &quot;Dibutyl Phthalate&quot; ## [80] &quot;1,3-Butadiene&quot; ## [81] &quot;m-Xylene&quot; ## [82] &quot;Propionaldehyde&quot; ## [83] &quot;o-Xylene&quot; ## [84] &quot;Phthalic Anhydride&quot; ## [85] &quot;Cyanide&quot; ## [86] &quot;Methyl Chloroform&quot; ## [87] &quot;Antimony&quot; ## [88] &quot;Aniline&quot; ## [89] &quot;Chlorine&quot; ## [90] &quot;Maleic Anhydride&quot; ## [91] &quot;Acrylic Acid&quot; ## [92] &quot;Hydrogen Fluoride&quot; ## [93] &quot;Phosphine&quot; ## [94] &quot;Triethylamine&quot; ## [95] &quot;Propylene Dichloride&quot; ## [96] &quot;Vinylidene Chloride&quot; ## [97] &quot;p-Xylene&quot; ## [98] &quot;Ethylene Dibromide&quot; ## [99] &quot;Ethylene Dichloride&quot; ## [100] &quot;Carbon Tetrachloride&quot; ## [101] &quot;Chloroform&quot; ## [102] &quot;1,1,2,2-Tetrachloroethane&quot; ## [103] &quot;Methyl Bromide&quot; ## [104] &quot;Methyl Chloride&quot; ## [105] &quot;Bromoform&quot; ## [106] &quot;Ethyl Chloride&quot; ## [107] &quot;Carbon Disulfide&quot; ## [108] &quot;Tert-butyl Acetate&quot; ## [109] &quot;PM25-Primary from certain diesel engines&quot; ## [110] &quot;PM10-Primary from certain diesel engines&quot; ## [111] &quot;Carbon Dioxide&quot; ## [112] &quot;Methane&quot; ## [113] &quot;Sulfur Hexafluoride&quot; ## [114] &quot;Vinyl Chloride&quot; ## [115] &quot;2,4-Toluene Diisocyanate&quot; ## [116] &quot;Chlorobenzene&quot; ## [117] &quot;1-Bromopropane&quot; ## [118] &quot;Polycyclic aromatic compounds (includes 25 specific compounds)&quot; ## [119] &quot;Benzyl Chloride&quot; ## [120] &quot;2,4-Dinitrotoluene&quot; ## [121] &quot;Dimethyl Sulfate&quot; ## [122] &quot;Isophorone&quot; ## [123] &quot;2-Chloroacetophenone&quot; ## [124] &quot;Methylhydrazine&quot; ## [125] &quot;Acetophenone&quot; ## [126] &quot;5-Methylchrysene&quot; ## [127] &quot;Epichlorohydrin&quot; ## [128] &quot;4,4-Methylenediphenyl Diisocyanate&quot; ## [129] &quot;o-Toluidine&quot; ## [130] &quot;Acetonitrile&quot; ## [131] &quot;Trifluralin&quot; ## [132] &quot;2,4-Dichlorophenoxy Acetic Acid&quot; ## [133] &quot;1,4-Dichlorobenzene&quot; ## [134] &quot;p-Dioxane&quot; ## [135] &quot;Ethylidene Dichloride&quot; ## [136] &quot;Acrylonitrile&quot; ## [137] &quot;Carbonyl Sulfide&quot; ## [138] &quot;Cresol/Cresylic Acid (Mixed Isomers)&quot; ## [139] &quot;Dibenzofuran&quot; ## [140] &quot;Acrylamide&quot; ## [141] &quot;Hydroquinone&quot; ## [142] &quot;Quinoline&quot; ## [143] &quot;Quinone&quot; pm25.names &lt;- c(&quot;PM2.5 Filterable&quot;, &quot;PM2.5 Primary (Filt + Cond)&quot;) county.pm25 &lt;- county.nei %&gt;% filter(`pollutant desc` %in% pm25.names) nrow(county.pm25) ## [1] 8326 Now, with a manageable number of observations in our area of interest, we are going to start looking at our data spatially. 2.7 Creating a Spatial Object using sp We first want to use our filtered tibble to create an sp Spatial Points object. #Assign the proper coordinates coordinates(county.pm25) &lt;- county.pm25[,c(&quot;site longitude&quot;,&quot;site latitude&quot;)] #Assign the proper projection for this data source (EPSG 4326) proj4string(county.pm25) &lt;- CRS(&quot;+init=epsg:4326&quot;) #Check data with basic plot plot(county.pm25) With everything looking as it should, let’s look back on what we just did. We initialized the Spatial Points object using the coordinates() function, assigning the proper longitude and latitude from the dataset. We then used the proj4string() function to assign the correct Coordinate Reference System (CRS) to our data. Be careful not to use the wrong projection (check your data source). If you need to transform the projection of your dataset, use the spTransform() function. Let’s now briefly review data visualization with the tmap package using this point data. 2.8 Data Visualization Review Here, we will use the spatial data visualization skills learned in an earlier chapter to visualize the point locations of PM2.5 sources in Cook County. #Read in Cook County Shapefile using sp&#39;s readOGR function cook.county &lt;- readOGR(&quot;../data/CookCounty.geojson&quot;) ## OGR data source with driver: GeoJSON ## Source: &quot;/Users/LorenzMenendez/Desktop/OpenAirQ-toolkit/data/CookCounty.geojson&quot;, layer: &quot;CookCounty&quot; ## with 1 features ## It has 2 fields #Check projection proj4string(cook.county) ## [1] &quot;+proj=tmerc +lat_0=36.66666666666666 +lon_0=-88.33333333333333 +k=0.9999749999999999 +x_0=300000.0000000001 +y_0=0 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=us-ft +no_defs &quot; #Create tmap plot tm_shape(cook.county) + tm_borders() + tm_shape(county.pm25) + tm_dots() This is clearly a very basic plot of the data. We can get a basic idea of where the point density may be highest, but we cannot tell much else about the data. Let’s now create an interactive map with the dots colored and sized based off of the volume of emissions (self-reported) given off at each point location. #Set tmap mode to view tmap_mode(&quot;view&quot;) tm_shape(cook.county) + tm_borders() + tm_shape(county.pm25) + tm_bubbles(col = &quot;total emissions&quot;, alpha = 0.3, size = &quot;total emissions&quot;, style = &quot;fisher&quot;) Here, we used the tmap_mode() function to change the map style to interactive viewing and changed the arguments of the tm_bubbles() function to change the appearance of the point locations. Let’s now construct a continuous surface Kernal Density Estimation from our point data. 2.9 Constructing a Kernal Density Estimation A Kernal Density Estimation (KDE) map at its most basic level is, as the name suggests, a means of representing the density of features over a given area. The term heatmap is often used interchangeably with KDE. Constructing a KDE gives us a continuous surface from discrete point data. This is quite useful as both an end product or an input to a model that requires continuous surface inputs. Each cell of the constructed raster is assigned a value based on the estimated density of ponits in that part of the map. This value can either be entirely unweighted (based solely on the number of points in an area) or weighted on a given variable (points with higher values for that variable will make an area appear denser). There are countless online resources available for learning more about the mathematics/history of KDE. Let’s now create our first KDE from the point data we’ve been using. We are going to be using the sp.kde() function from the spatialEco package, however there are several other R packages that achieve a more or less identical outcome. #Construct KDE county.kde &lt;- sp.kde(county.pm25, nr=500, nc=500) plot(county.kde) We’ve now produced a continuous surface representing the density of PM2.5 emissions sources across Cook County. Let’s look over the sp.kde() function in a little more detail. In addition to inputting our sp object, we also input values of 500 for the nr and nc arguments. These abbreviations are short for “number of rows” and “number of columns” respectively. The sp.kde function creates a grid on which to map the results of the KDE, and these arguments tell the function what the dimensions of this grid should be. Let’s look at how changing these two arguments changes the appearance of our KDE map: #10x10 grid county.kde.10 &lt;- sp.kde(county.pm25, nr=10, nc=10) plot(county.kde.10) #100x100 grid county.kde.100 &lt;- sp.kde(county.pm25, nr=100, nc=100) plot(county.kde.100) #500x500 grid county.kde.500 &lt;- sp.kde(county.pm25, nr=500, nc=500) plot(county.kde.500) Note the changes in the plots as the resolution of the grid is increased. Let’s now look at the y argument used to add a weight to the KDE. Let’s say we wanted to weigh our KDE based on the amount of total emissions from individual sites. Here’s how you would do that: #Construct weighted KDE county.kde.weighted &lt;- sp.kde(county.pm25, y=county.pm25$`total emissions`, nr=500, nc=500) plot(county.kde.weighted) As you can see, weighing the KDE on the total emissions amount dramatically changes the map. These changes can be deemphasized/accentuated if you transform the variable weighing the data. If you are interested in reading more about the arguments of this function check out its R Documentation page. This tutorial demonstrates how to compare common interpolation models empirically to select the one that seems most appropriate for a given spatial extent. It follows the steps provided in an R-Spatial tutorial on interpolating pollution vairables. Possible interpolation models are voronoi polygons, nearest neighbor interpolation, inverse distance weights (IDW), and finally kriging. The oprimal model is one with the lowest RMSE compared to all other models. The models are also evaluated against a “NULL Model”, where the mean value is assigned in all grid cells. "],
["example-interpolating-average-temperature-across-the-21-county-study-area-.html", "3 Example: Interpolating Average Temperature across the 21-county study area. 3.1 Wrangling Data 3.2 Exploring the Null Model 3.3 Model 1: Voronoi Model 3.4 Model 2: Nearest Neighbor Interpolation 3.5 Model 3: IDW Interpolation using baseline parameters 3.6 Model 4: Optimized IDW Interpolation 3.7 Model 5: Thin Plate Spline Model 3.8 Model 6: Ordinary Kriging 3.9 Model 7: Blending all models 3.10 Conclusion", " 3 Example: Interpolating Average Temperature across the 21-county study area. This section describes how an interpolation model was selected to interpolate average temperature from airport weather stations in the 21 county area. 3.1 Wrangling Data Reading in monthly temperature averages tmpf = readr::read_csv(&#39;../data/ASOS_tmpf_2014.2018.csv&#39;) head(tmpf) ## # A tibble: 6 x 5 ## moyr site_num latitude longitude tmpf ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2014-01 ARR 41.8 -88.5 14.4 ## 2 2014-01 BUU 42.7 -88.3 10.8 ## 3 2014-01 C09 41.4 -88.4 16.3 ## 4 2014-01 DPA 41.9 -88.2 16.6 ## 5 2014-01 ENW 42.6 -87.9 16.6 ## 6 2014-01 GYY 41.6 -87.4 20.6 Let’s filter for August 2018 Data tmpf = dplyr::filter(tmpf, moyr == &#39;2018-08&#39;) Mapping the station values library(tmap) tmap_mode(&quot;view&quot;) ## tmap mode set to interactive viewing counties = sf::st_read(&#39;../data/LargeAreaCounties/LargeAreaCounties.shp&#39;) ## Reading layer `LargeAreaCounties&#39; from data source `/Users/LorenzMenendez/Desktop/OpenAirQ-toolkit/data/LargeAreaCounties/LargeAreaCounties.shp&#39; using driver `ESRI Shapefile&#39; ## Simple feature collection with 21 features and 8 fields ## geometry type: MULTIPOLYGON ## dimension: XY ## bbox: xmin: -88.77879 ymin: 40.73641 xmax: -86.46629 ymax: 43.21831 ## CRS: 4326 sensors = sf::st_as_sf(tmpf, coords = c(&quot;longitude&quot;, &quot;latitude&quot;), crs = 4326) tm_shape(counties) + tm_borders() + tm_shape(sensors) + tm_dots(col = &quot;tmpf&quot;, palette = &quot;-RdBu&quot;, title = &quot;Average August 2018 Temperature (ºF)&quot;, popup.vars = c(&quot;Temp&quot; = &quot;tmpf&quot;, &quot;Airport Code&quot; = &quot;site_num&quot;)) 3.2 Exploring the Null Model This model follows the null hypothesis, that there’s no variation in precipitation across space, by taking the mean precipitation and assigning that value across the entire area. RMSE &lt;- function(observed, predicted) { sqrt(mean((predicted - observed)^2, na.rm=TRUE)) } null &lt;- RMSE(mean(tmpf$tmpf), tmpf$tmpf) null ## [1] 2.098343 The Root Mean Swuare Error (RMSE) for the null model is 2.09. 3.3 Model 1: Voronoi Model This model takes sensor locations and generates a prediction area for that sensor. The 21 county area is divided into polygons representing areas closest to each sensor. Creating Voronoi Polygons library(dismo) v &lt;- voronoi(dsp) ## Loading required namespace: deldir tm_shape(v) + tm_borders() + tm_shape(sensors) + tm_dots(popup.vars = c(&quot;Temp&quot; = &quot;tmpf&quot;, &quot;Airport Code&quot; = &quot;site_num&quot;)) Tenperature values can then be assigned to each voronoi polygon based on temperature readings from the sensor located in the given polygon. # Assigning values to polygons il &lt;- aggregate(IL) vil &lt;- raster::intersect(v, il) tm_shape(vil) + tm_fill(&#39;tmpf&#39;, alpha = 0.5, palette = &quot;-RdBu&quot;, title = &quot;Average August 2018 Temperature (ºF)&quot;, popup.vars = c(&quot;Temp&quot; = &quot;tmpf&quot;)) + tm_borders() Rasterizing voronoi polygons r &lt;- blank.raster vr &lt;- rasterize(vil, r, &#39;tmpf&#39;) 3.3.1 Validating the Voronoi Model We will use 5-fold cross-validation to determine the improvement compared to our baseline, the NULL model. set.seed(5132015) # Randomly partition the Dataset into 5 groups (1 through 5) kf &lt;- kfold(nrow(dsp)) # Initialize a vector of length 5 vorrmse &lt;- rep(NA, 5) # Validate for (k in 1:5) { test &lt;- dsp[kf == k, ] # Learn on group k train &lt;- dsp[kf != k, ] # Train on groups != k v &lt;- voronoi(train) p1 &lt;- raster::extract(v, test)$tmpf vorrmse[k] &lt;- RMSE(test$tmpf, p1) # Save the RMSE } print(&quot;RMSE for each of the five folds&quot;) ## [1] &quot;RMSE for each of the five folds&quot; vorrmse ## [1] 1.7271479 1.5581327 1.3602474 1.3123567 0.7395483 # Take the mean RMSE and get percentage improvement print(&quot;Mean RMSE&quot;) ## [1] &quot;Mean RMSE&quot; mean(vorrmse) ## [1] 1.339487 print(&quot;Improvement over NULL model&quot;) ## [1] &quot;Improvement over NULL model&quot; 1 - (mean(vorrmse) / null) ## [1] 0.3616454 The RMSE for the Voronoi model is 1.33, which represents a 36% increase in accuracy from the null model. The percentage improvement metric will determine the most useful model to choose. 3.4 Model 2: Nearest Neighbor Interpolation The next model to test is a nearest neighbor interpolation. The interpolation takes into account the nearest 5 sensors when determining the temperature value at a given grid cell. The decay parameters is seto to zero. set.seed(5132015) library(gstat) gs &lt;- gstat(formula=tmpf~1, locations=dsp, nmax=5, set=list(idp = 0)) nn &lt;- interpolate(r, gs) ## [inverse distance weighted interpolation] nnmsk &lt;- mask(nn, vr) tm_shape(nnmsk) + tm_raster(n = 5, alpha = 0.5, palette = &quot;-RdBu&quot;, title = &quot;Average August 2018 Temperature (ºF)&quot;) Overall the interpolation yield provdes a similar estimate of temperature as the voronoi polygons. This is expected as voronoi polygons are a form of nearest neighbor interpolation. The differenc here is that this mode takes into account the temperature values at the five nearest sensors, not just one nearest sensor. Cross validating the result using the gstat predict() function. nnrmse &lt;- rep(NA, 5) for (k in 1:5) { test &lt;- dsp[kf == k, ] train &lt;- dsp[kf != k, ] gscv &lt;- gstat(formula=tmpf~1, locations=train, nmax=5, set=list(idp = 0)) p2 &lt;- predict(gscv, test)$var1.pred nnrmse[k] &lt;- RMSE(test$tmpf, p2) } ## [inverse distance weighted interpolation] ## [inverse distance weighted interpolation] ## [inverse distance weighted interpolation] ## [inverse distance weighted interpolation] ## [inverse distance weighted interpolation] print(&quot;RMSE for each of the five folds&quot;) ## [1] &quot;RMSE for each of the five folds&quot; nnrmse ## [1] 2.163080 1.204564 1.779832 2.062489 1.802264 print(&quot;Mean RMSE&quot;) ## [1] &quot;Mean RMSE&quot; mean(nnrmse) ## [1] 1.802446 print(&quot;Improvement over NULL model&quot;) ## [1] &quot;Improvement over NULL model&quot; 1 - (mean(nnrmse) / null) ## [1] 0.1410146 The average RMSE after 5-fold cross validation is 1.80. This model is 14% more accurate than the null model. This suggests that it might be a less effective estimate of temperature in our case. 3.5 Model 3: IDW Interpolation using baseline parameters IDW stands for Inverse Distance Weighted interpolation. This model estimates temperature at a given cell by taking into account temperature values located at various nearby sensors and each sensor’s straight line distance to the grid cell. Data from sensors located closer to the target grid cell are given more weight in the final estimate of temperature in that cell. This model is a logical outcropping of Tobler’s first law of geography, “everything is related to everything else, but near things are more related than distant things.” set.seed(5132015) library(gstat) gs &lt;- gstat(formula=tmpf~1, locations=dsp) idw &lt;- interpolate(r, gs) ## [inverse distance weighted interpolation] idwr &lt;- mask(idw, vr) plot(idwr) tm_shape(idwr) + tm_raster(n = 10, alpha = 0.5, palette = &quot;-RdBu&quot;, title = &quot;Average August 2018 Temperature (ºF)&quot;) The IDW model creates a smoother temperature surface compared to voronoi polygons and nearest neighbor interpolations. Hard breaks between individual sensor regions is reduced to a minimum. However, IDW also introduced it’s own distortion. The ‘bullseye’ effect occurs when a sensor value is significantly different than the rest, an artefact that is clearly visible around almost all snesor locations in our map. rmse &lt;- rep(NA, 5) for (k in 1:5) { test &lt;- dsp[kf == k, ] train &lt;- dsp[kf != k, ] gs &lt;- gstat(formula=tmpf~1, locations=train) p &lt;- predict(gs, test) rmse[k] &lt;- RMSE(test$tmpf, p$var1.pred) } ## [inverse distance weighted interpolation] ## [inverse distance weighted interpolation] ## [inverse distance weighted interpolation] ## [inverse distance weighted interpolation] ## [inverse distance weighted interpolation] print(&quot;RMSE for each of the five folds&quot;) ## [1] &quot;RMSE for each of the five folds&quot; rmse ## [1] 1.681240 1.269816 1.615228 1.836818 1.209793 print(&quot;Mean RMSE&quot;) ## [1] &quot;Mean RMSE&quot; mean(rmse) ## [1] 1.522579 print(&quot;Improvement over NULL model&quot;) ## [1] &quot;Improvement over NULL model&quot; 1 - (mean(rmse) / null) ## [1] 0.2743897 The IDW model has an RMSE of 1.52, a 27% improvement over the null model. 3.6 Model 4: Optimized IDW Interpolation IDW models are highly sensitive to two user defined parameters. 1) The maximum number of sensors to take into account and 2) a decay or friction of distance parameter. Since models are evaluated using RMSE, an optimization algorithm can be used to find an optimal number of sensors and decay parameter that minimizes RMSE. This optimization is performed below. f1 &lt;- function(x, test, train) { nmx &lt;- x[1] idp &lt;- x[2] if (nmx &lt; 1) return(Inf) if (idp &lt; .001) return(Inf) m &lt;- gstat(formula=tmpf~1, locations=train, nmax=nmx, set=list(idp=idp)) p &lt;- predict(m, newdata=test, debug.level=0)$var1.pred RMSE(test$tmpf, p) } set.seed(20150518) i &lt;- sample(nrow(dsp), 0.2 * nrow(dsp)) tst &lt;- dsp[i,] trn &lt;- dsp[-i,] opt &lt;- optim(c(8, .5), f1, test=tst, train=trn) opt ## $par ## [1] 4.902864 8.446538 ## ## $value ## [1] 1.506649 ## ## $counts ## function gradient ## 57 NA ## ## $convergence ## [1] 0 ## ## $message ## NULL The optimal IDW interpolation can be gleaned from the opt$par variable. The number of sensors to consider should be ~4.90 while the decay parameter should be 8.44. Performing the IDW interpolation with these parameters yields the following results. m &lt;- gstat::gstat(formula=tmpf~1, locations=dsp, nmax=opt$par[1], set=list(idp=opt$par[2])) idw &lt;- interpolate(r, m) ## [inverse distance weighted interpolation] idw &lt;- mask(idw, il) tm_shape(idw) + tm_raster(n = 10, alpha = 0.5, palette = &quot;-RdBu&quot;, title = &quot;Average August 2018 Temperature (ºF)&quot;) The output from this model is super interesting. It looks similar to the voronoi polygons, but as if someone took a paintbrush to the edges of each polygon and mixed the colors together. In scientific terms, it’s as if someone took the voronoi polygons and added a thin gradient between each polygon. Let’s cross validate and get the RMSE idwrmse &lt;- rep(NA, 5) for (k in 1:5) { test &lt;- dsp[kf == k, ] train &lt;- dsp[kf != k, ] m &lt;- gstat(formula=tmpf~1, locations=train, nmax=opt$par[1], set=list(idp=opt$par[2])) p4 &lt;- predict(m, test)$var1.pred idwrmse[k] &lt;- RMSE(test$tmpf, p4) } ## [inverse distance weighted interpolation] ## [inverse distance weighted interpolation] ## [inverse distance weighted interpolation] ## [inverse distance weighted interpolation] ## [inverse distance weighted interpolation] print(&quot;RMSE for each of the five folds&quot;) ## [1] &quot;RMSE for each of the five folds&quot; idwrmse ## [1] 1.780982 1.453620 1.407686 1.287353 1.212273 print(&quot;Mean RMSE&quot;) ## [1] &quot;Mean RMSE&quot; mean(idwrmse) ## [1] 1.428383 print(&quot;Improvement over NULL model&quot;) ## [1] &quot;Improvement over NULL model&quot; 1 - (mean(idwrmse) / null) ## [1] 0.3192805 The RMSE is 1.42, which represents an improvement of 32% over the null model. 3.7 Model 5: Thin Plate Spline Model Originally, a non-spatial interpolation method, this model seeks to “smooth” the temperature from each sensor across grid cells. It’s name comes from this models ability to penalize non-smooth data, similar to how a thin but rigid sheets resists bending. library(fields) ## Loading required package: spam ## Loading required package: dotCall64 ## Loading required package: grid ## ## Attaching package: &#39;grid&#39; ## The following object is masked from &#39;package:spatialEco&#39;: ## ## explode ## Spam version 2.5-1 (2019-12-12) is loaded. ## Type &#39;help( Spam)&#39; or &#39;demo( spam)&#39; for a short introduction ## and overview of this package. ## Help for individual functions is also obtained by adding the ## suffix &#39;.spam&#39; to the function name, e.g. &#39;help( chol.spam)&#39;. ## ## Attaching package: &#39;spam&#39; ## The following objects are masked from &#39;package:base&#39;: ## ## backsolve, forwardsolve ## Loading required package: maps ## ## Attaching package: &#39;maps&#39; ## The following object is masked from &#39;package:purrr&#39;: ## ## map ## See https://github.com/NCAR/Fields for ## an extensive vignette, other supplements and source code m &lt;- Tps(coordinates(dsp), tmpf$tmpf) tps &lt;- interpolate(r, m) tps &lt;- mask(tps, idw) tm_shape(tps) + tm_raster(n = 5, alpha = 0.5, palette = &quot;-RdBu&quot;, title = &quot;Average August 2018 Temperature (ºF)&quot;) This model produces evently spaced temperature bands, as expected considering the rigidity of the model. Compared to other models, it might seem less correct or represnetative of the real-world. However, only RMSE will tell. tpsrmse &lt;- rep(NA, 5) for (k in 1:5) { test &lt;- dsp[kf == k, ] train &lt;- dsp[kf != k, ] m &lt;- Tps(coordinates(train), train$tmpf) p5 &lt;- predict(m, coordinates(test)) tpsrmse[k] &lt;- RMSE(test$tmpf, p5) } print(&quot;RMSE for each of the five folds&quot;) ## [1] &quot;RMSE for each of the five folds&quot; tpsrmse ## [1] 1.815358 1.517021 1.567168 1.619682 1.477376 print(&quot;Mean RMSE&quot;) ## [1] &quot;Mean RMSE&quot; mean(tpsrmse) ## [1] 1.599321 print(&quot;Improvement over NULL model&quot;) ## [1] &quot;Improvement over NULL model&quot; 1 - (mean(tpsrmse) / null) ## [1] 0.237817 The RMSE for this model is 1.60, a 24% improvement over the null model. 3.8 Model 6: Ordinary Kriging Kriging is a complex interpolation method that seeks to find the best linear predictor of intermediate values. In our spatial context, this means that it seeks The first step is to fit a variogram over the temperature data library(gstat) gs &lt;- gstat(formula=tmpf~1, locations=dsp) v &lt;- variogram(gs, width=20) head(v) ## np dist gamma dir.hor dir.ver id ## 1 7 14.82348 1.349970 0 0 var1 ## 2 31 31.27996 3.546520 0 0 var1 ## 3 35 49.86338 2.841576 0 0 var1 ## 4 31 70.06044 3.377242 0 0 var1 ## 5 18 88.05822 3.862724 0 0 var1 plot(v) We notice that there are only five points below the mean, which is very few points to properly fit a model to. But, let’s continue to see what happens. Next we fit the variogram. This time, we use the autofitVarogram function from the automap package. fve = automap:::autofitVariogram(formula = tmpf~1, input_data = dsp) fve ## $exp_var ## np dist gamma dir.hor dir.ver id ## 1 14 18.36815 2.110076 0 0 var1 ## 2 12 30.29441 4.746482 0 0 var1 ## 3 30 41.75700 3.110922 0 0 var1 ## 4 24 57.31836 2.323814 0 0 var1 ## 5 22 71.91136 3.795503 0 0 var1 ## 6 25 89.30900 3.669922 0 0 var1 ## ## $var_model ## model psill range ## 1 Nug 0.000000 0.00000 ## 2 Gau 3.531433 17.77443 ## ## $sserr ## [1] 0.04157277 ## ## attr(,&quot;class&quot;) ## [1] &quot;autofitVariogram&quot; &quot;list&quot; plot(fve) The autofitVariogram function fitted a Gaussian variogram to our small sample of datapoints. Executing an ordiary kriging model kp = krige(tmpf~1, dsp, as(blank.raster, &#39;SpatialGrid&#39;), model=fve$var_model) ## [using ordinary kriging] spplot(kp) Plotting this on the 21 counties ok &lt;- brick(kp) ok &lt;- mask(ok, il) names(ok) &lt;- c(&#39;prediction&#39;, &#39;variance&#39;) plot(ok) tm_shape(ok[[1]]) + tm_raster(n = 5, alpha = 0.5, palette = &quot;-RdBu&quot;, title = &quot;Average August 2018 Temperature (ºF)&quot;) Cross-Validating the Kriging Model set.seed(20150518) krigrmse = rep(NA, 5) for (i in 1:5) { test &lt;- dsp[kf == i,] train &lt;- dsp[kf != i, ] fve = automap:::autofitVariogram(formula = tmpf~1, input_data = train) kp = krige(tmpf~1, train, as(blank.raster, &#39;SpatialGrid&#39;), model=fve$var_model) p6 = raster::extract(as(kp, &#39;RasterLayer&#39;), test) krigrmse[i] &lt;- RMSE(test$tmpf, p6) } ## [using ordinary kriging] ## [using ordinary kriging] ## [using ordinary kriging] ## [using ordinary kriging] ## [using ordinary kriging] print(&quot;RMSE for each of the five folds&quot;) ## [1] &quot;RMSE for each of the five folds&quot; krigrmse ## [1] 1.650569 1.443141 2.077150 2.060613 1.774075 print(&quot;Mean RMSE&quot;) ## [1] &quot;Mean RMSE&quot; mean(krigrmse) ## [1] 1.80111 print(&quot;Improvement over NULL model&quot;) ## [1] &quot;Improvement over NULL model&quot; 1 - (mean(krigrmse) / null) ## [1] 0.1416513 After 5 fold cross-validation, the RMSE is 1.80. This is a 14% improvement over the null model. 3.9 Model 7: Blending all models Next, we will attempt to created a blended model that takes a weighted average of the predicted values for each model, weighted by their RMSE. This ensures that the more accurate models have more influence on the blended model than the models with poor predictions. This code chunk re-runs each model, creating an ensemble model, while cross-validating the results. set.seed(20150518) # Initialize rmse vectors vorrmse &lt;- nnrmse &lt;- idwrmse &lt;- krigrmse &lt;- tpsrmse &lt;- ensrmse &lt;- rep(NA, 5) for (i in 1:5) { # Creating Test &amp; Training Data test &lt;- dsp[kf == i, ] # Learn on group k train &lt;- dsp[kf != i, ] # Train on groups != k # Voronoi v &lt;- voronoi(train) p1 &lt;- raster::extract(v, test)$tmpf vorrmse[i] &lt;- RMSE(test$tmpf, p1) # Save the RMSE # Nearest Neighbbor gscv &lt;- gstat(formula=tmpf~1, locations=train, nmax=5, set=list(idp = 0)) p2 &lt;- predict(gscv, test)$var1.pred nnrmse[i] &lt;- RMSE(test$tmpf, p2) # Optimized IDW m &lt;- gstat(formula=tmpf~1, locations=train, nmax=opt$par[1], set=list(idp=opt$par[2])) p3 &lt;- predict(m, test)$var1.pred idwrmse[i] &lt;- RMSE(test$tmpf, p3) # Thin Plate Spline tpsm &lt;- Tps(coordinates(train), train$tmpf) p4 &lt;- predict(tpsm, coordinates(test))[,1] tpsrmse[i] &lt;- RMSE(test$tmpf, p4) # Kriging fve = automap:::autofitVariogram(formula = tmpf~1, input_data = train) kp = krige(tmpf~1, test, as(blank.raster, &#39;SpatialGrid&#39;), model=fve$var_model) p5 = raster::extract(as(kp, &#39;RasterLayer&#39;), test) krigrmse[i] &lt;- RMSE(test$tmpf, p5) # Weighting w &lt;- c(vorrmse[i], nnrmse[i], idwrmse[i], tpsrmse[i], krigrmse[i]) weights &lt;- w / sum(w) ensemble &lt;- p1 * weights[1] + p2 * weights[2] + p3 * weights[3] + p4 * weights[4] + p5 * weights[5] ensrmse[i] &lt;- RMSE(test$tmpf, ensemble) } ## [inverse distance weighted interpolation] ## [inverse distance weighted interpolation] ## [using ordinary kriging] ## [inverse distance weighted interpolation] ## [inverse distance weighted interpolation] ## [using ordinary kriging] ## [inverse distance weighted interpolation] ## [inverse distance weighted interpolation] ## [using ordinary kriging] ## [inverse distance weighted interpolation] ## [inverse distance weighted interpolation] ## [using ordinary kriging] ## [inverse distance weighted interpolation] ## [inverse distance weighted interpolation] ## [using ordinary kriging] print(&quot;RMSE for each of the five folds&quot;) ## [1] &quot;RMSE for each of the five folds&quot; ensrmse ## [1] 1.644697 1.336210 1.387885 1.608958 1.296885 print(&quot;Mean RMSE&quot;) ## [1] &quot;Mean RMSE&quot; mean(ensrmse) ## [1] 1.454927 print(&quot;Improvement over NULL model&quot;) ## [1] &quot;Improvement over NULL model&quot; 1 - (mean(ensrmse) / null) ## [1] 0.3066305 The RMSE is 1.45, representing a 31% improvement over the null model We can quickly see how this compared the RMSEs of the component models. # Voronoi 1 - (mean(vorrmse) / null) ## [1] 0.3616454 # Nearest Neighbbor 1 - (mean(nnrmse) / null) ## [1] 0.1410146 # Optimized IDW 1 - (mean(idwrmse) / null) ## [1] 0.3192805 # Thin Plate Spline 1 - (mean(tpsrmse) / null) ## [1] 0.237817 # Kriging 1 - (mean(krigrmse) / null) ## [1] 0.6565196 We notice that the improvements of each model is highly variable depending on the model chosen. Kriging has the highest performance, with a 66% improvement over the null model. 3.10 Conclusion The highest performing model is the ordinary kriging interpolation, with a 65% improvement over the null model. The worst performance came from the nearest neighbor interpolation, with only 14% improvement. The Voronoi, Optimized IDW, and Thin Plate Spline were solidly in the middle of the pack with improvement between 23% and 36%. These values give us a great point estimate of each model’s performance, however we made no effort to show that the differences in improvement (i.e. RMSE) are statistically significant. Selecting which model to use requires more sensor locations and some basic hypothesis testing. "],
["merging-satellite-and-point-sensor-data.html", "4 Merging Satellite and Point Sensor Data 4.1 Generate Rasters 4.2 Export to CSV", " 4 Merging Satellite and Point Sensor Data There’s a fundamental challenge in our project to merge AOD data and predictor variables because the data capture techniques are very different. The satellite-based AOD data is measured continuously across the surface of the earth on a 1km by 1km grid system. Meanwhile, sensor data is only captured locally at the sensor location. Therefore, a method to interpolate local sensor data to generate a continuous surface of data is required. An ‘Optimized’ IDW interpolation was used to estimate sensor values across a 1km by 1km grid system. This method takes into account the sensor locations and value for each variable to estimate values in grid cells without a sensor based on a linear interpolation of nearby sensor values. The specific number of sensors to take into account and the distance decay power function were optimized by medinimizing the RMSE (Error). This method was adapted from an RSpatial Tutorial on IDW interpolation with pollution and weather data. To simplify implementation and replication, the entire workflow was coded in R and bundled into a packaged named sensor2raster. The next sections demonstrate how to apply this package to sensor data. # devtools::install_local(&#39;../data/sensor2raster_0.04.tar.gz&#39;) library(sensor2raster, quietly = TRUE) 4.1 Generate Rasters Creating raster surfaces is easy using the sensor2raster function. This function takes the raw output from the riem or aqsr packages and identifies the sensor locations and data values to interpolate. The underlying IDW interpolation is performed by the gstat package. The code chunk below demonstrates how to take ASOS weather data and convert it to a Raster format. The weather data variable is a data frame containing temperature data measured at airports in the Greater Chicago and Milwaukee Areas in 2018. We also pass the AOD.grid, a RasterLayer object representing the grid cells where we want to predict temperature. These grid cells correspond exactly to the pixels of satellite AOD data. temp.rasters = sensor2raster(sensor.data = weather.data, # Input the raw data.frame from riem data.type = &#39;ASOS&#39;, # Specify data type (&#39;ASOS&#39; or &#39;EPA&#39;) reference.grid = AOD.grid, # Grid to interpolate over subvariable = &#39;tmpf&#39;) # Column name of variable to interpolate 4.2 Export to CSV While Raster data is helpful for spatial data analysis and geovisualizations, it is sometimes helpful to store the interpolation in a non-spatial format. The grid2csv function allows you to convert Raster data to CSV either cell-by-cell, or by aggregating to a vector geometry. The exported data.frame is halfway between Long and Wide format due to the 3-dimensional nature of our data. The table below describes how these CSVs are structured in the cell-by-cell case. Var_Name Raster.Cell M1.2018 M2.2018 … M12.2018 Monthly_Temperature 1 23 25 … 20 Monthly_Temperature 2 23 25 … 20 … … … … … … Monthly_Temperature 100 10 15 … 11 The length of the table equals the number of cells in the RasterStack. Each cell is given a unique identifier stored in the Raster.Cell column. The Var_Name colums represent the variable of interest. When there are multiple variables, they are row binded together, giving a long table format. The rest of the columns represent the names given to the layers within each RasterStack. Im this case, each column represents a month and year combination. Additional time periods are appended column-wise to the table. The following table describes the outputted data frame from the monthly temperature Rasters generated earlier. temp.export = grid2csv(rasters = list(temp.rasters[[1]]), var.names = &#39;Monthly_Temperature&#39;) Var_Name Raster.Cell M1.2018 M2.2018 M3.2018 M4.2018 M5.2018 M6.2018 M7.2018 M8.2018 M9.2018 M10.2018 M11.2018 M12.2018 Monthly_Temperature 1 22.75343 24.89301 32.54753 36.13429 59.56493 64.5328 70.55568 70.15553 62.38351 48.01338 30.81887 29.27243 Monthly_Temperature 2 22.75343 24.89301 32.54753 36.13429 59.56493 64.5328 70.55568 70.15553 62.38351 48.01338 30.81887 29.27243 Monthly_Temperature 3 22.75343 24.89301 32.54753 36.13429 59.56493 64.5328 70.55568 70.15553 62.38351 48.01338 30.81887 29.27243 Monthly_Temperature 4 22.75343 24.89301 32.54753 36.13429 59.56493 64.5328 70.55568 70.15553 62.38351 48.01338 30.81887 29.27243 Monthly_Temperature 5 22.75343 24.89301 32.54753 36.13429 59.56493 64.5328 70.55568 70.15553 62.38351 48.01338 30.81887 29.27243 Monthly_Temperature 6 22.75343 24.89301 32.54753 36.13429 59.56493 64.5328 70.55568 70.15553 62.38351 48.01338 30.81887 29.27243 The format of the table changes slightly when subsetting using a vecor object. A new column sj_join appears in the table, representing a unique allowing the table to be joined back to the origin sf object if needed. When subsetting by point features, the Raster_Cell column describes the cell that overlapped with each point feature. When subsettting by line or polygon features, the Raster_Cell column describes the cell that overlapped with the centroid of each geometry. temp.export.sf = grid2csv(rasters = list(temp.rasters[[1]]), var.names = &#39;Monthly_Temperature&#39;, sf.obj = counties) Var_Name Raster_Cell sf_join M1.2018 M2.2018 21 Monthly_Temperature 4745 21 22.74704 24.89580 3 Monthly_Temperature 4970 3 23.00074 25.13390 8 Monthly_Temperature 11038 8 23.88734 26.08940 20 Monthly_Temperature 12957 20 23.22283 25.46473 19 Monthly_Temperature 15155 19 24.72884 26.94046 4 Monthly_Temperature 21000 4 24.81107 27.53006 The code chunk below demonstrates how to exploit the sf_join field to join the table data back to the spatial sf object. # counties.join = counties %&gt;% tibble::rowid_to_column() %&gt;% dplyr::rename(sf_join = rowid) # # counties.join = grid2csv(rasters = list(temp.rasters[[1]]), var.names = &#39;Monthly_Temperature&#39;, sf.obj = counties.join) %&gt;% # left_join(counties.join) %&gt;% # st_as_sf() "],
["appendix-a-ndvi.html", "Appendix A: NDVI 4.3 Loading Packages and Data", " Appendix A: NDVI In this tutorial, we will learn to deal with raster data. For illustrative purposes, we will take a look at the NDVI data (Normalized Difference Vegetation Index) of 21 large counties in the Midwest. 4.3 Loading Packages and Data First, let’s load the packages needed for this exercise. library(raster) library(sf) library(tmap) Both the “sf” package and the “tmap” package are standard tools to handle spatial data. Since we are mainly analyzing raster data in this tutorial, we also load the “raster” package, which “implements basic and high-level functions for raster data and for vector data operations such as intersections.” The detailed documentation can be found here. Next, we load the data. The data we are analyzing is the quarterly average of the Normalized Difference Vegetation Index for 21 large counties in the Midwest, from 2014 to 2018. It is stacked chronologically. Note that the original dataset is quite large (over 50 gigabytes), so we processed the data in advance. We can load the pre-processed data simply by running the command below. ndvi.quarterly &lt;- stack(&quot;../data/NDVILargeAreaQuarterlyStack.tif&quot;) (Remark: You may encounter trouble loading the data if you do not have the “rgdal” package installed!) It is a good idea to take a glance at our dataset before we proceed any further. ndvi.quarterly ## class : RasterStack ## dimensions : 1194, 784, 936096, 20 (nrow, ncol, ncell, nlayers) ## resolution : 0.00295, 0.00208 (x, y) ## extent : -88.77872, -86.46592, 40.73568, 43.2192 (xmin, xmax, ymin, ymax) ## crs : +proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0 ## names : NDVILarge//rlyStack.1, NDVILarge//rlyStack.2, NDVILarge//rlyStack.3, NDVILarge//rlyStack.4, NDVILarge//rlyStack.5, NDVILarge//rlyStack.6, NDVILarge//rlyStack.7, NDVILarge//rlyStack.8, NDVILarge//rlyStack.9, NDVILarge//lyStack.10, NDVILarge//lyStack.11, NDVILarge//lyStack.12, NDVILarge//lyStack.13, NDVILarge//lyStack.14, NDVILarge//lyStack.15, ... ## min values : -0.1979000, -0.1400464, -0.1463546, -0.1714621, -0.1730933, -0.1068576, -0.1513947, -0.1989500, -0.1977000, -0.1508639, -0.1616515, -0.1834559, -0.1792000, -0.1500082, -0.1286048, ... ## max values : 0.4057376, 0.8217609, 0.9162192, 0.7483148, 0.5150092, 0.8381958, 0.9147448, 0.8577355, 0.7820903, 0.8141744, 0.9077643, 0.8238385, 0.8000358, 0.8359118, 0.8964590, ... Also remember to load the shapfile of the 21 large counties. We will use it shortly when we start plotting. counties &lt;- st_read(&quot;../data/LargeAreaCounties&quot;) ## Reading layer `LargeAreaCounties&#39; from data source `/Users/LorenzMenendez/Desktop/OpenAirQ-toolkit/data/LargeAreaCounties&#39; using driver `ESRI Shapefile&#39; ## Simple feature collection with 21 features and 8 fields ## geometry type: MULTIPOLYGON ## dimension: XY ## bbox: xmin: -88.77879 ymin: 40.73641 xmax: -86.46629 ymax: 43.21831 ## CRS: 4326 "],
["data-manipulation-and-plotting.html", "5 Data manipulation and Plotting", " 5 Data manipulation and Plotting We start our analysis by looking at the data for one specific quarter - the 3rd quarter of 2018. Since this is the 19th layer of our dataset (recall that our dataset is stacked chronologically), we can easily extract the data using the line of code shown below. #this.qtr &lt;- ndvi.quarterly[[19]] this.qtr &lt;- raster::subset(ndvi.quarterly, 19, drop = FALSE) Thanks to the data processing done beforehand, we don’t have much data manipulation to do. It is time for us to start making plots! We begin with the most basic raster map. tm_shape(this.qtr) + tm_raster() + tm_layout(legend.outside = TRUE) This plot gives an overview of the NDVI of the 21 large counties. Without the county borders explicitly drawn, we are unable to compare the NDVI across counties. Moreover, the plot is far from aesthetically pleasing. Hence we redraw the graph, this time with counties borders as well as an informative title for the plot. In addition, we modify the title of the legend so that it is more comprehensible to readers. tm_shape(this.qtr) + tm_raster(title = &quot;NDVI&quot;) + tm_shape(counties) + tm_borders() + tm_layout(legend.outside = TRUE, main.title = &quot;NDVI - The 3rd Quarter of 2018&quot;) With this nicer plot, it is possible to compare the NDVI of different counties. Those who are familiar with the geography of the Midwest would immediately recognize that Cook county seems to have a lower NDVI than other counties. It is not at all surprising since the City of Chicago is located in Cook County, and metropolitan areas tend to have less vegetation than rural areas. We can also make the plot interactive so that the readers can explore the plot more thoroughly. tmap_mode(&quot;view&quot;) tm_shape(this.qtr) + tm_raster(title = &quot;NDVI&quot;) + tm_shape(counties) + tm_borders() + tm_layout(legend.outside = TRUE, main.title = &quot;NDVI - the 3rd Quarter of 2018&quot;) We can turn off the interactive mode by running the following code. tmap_mode(&quot;plot&quot;) Before we jump to the next section, it may be helpful to quickly examine the summary statistics for the sub-dataset this.qtr. Notice that we use raster::summary here to specifiy that we want R to use the summary function from the “raster” package. raster::summary(this.qtr) ## NDVILargeAreaQuarterlyStack.19 ## Min. -9.315484e-02 ## 1st Qu. 6.263003e-01 ## Median 6.918114e-01 ## 3rd Qu. 7.413675e-01 ## Max. 9.006166e-01 ## NA&#39;s 4.571800e+05 "],
["more-plotting.html", "6 More Plotting!", " 6 More Plotting! Now that we have learned how to make a raster map, let’s make more. In this section, we will place our focus on the NDVI of Cook County, IL - the home county of Chicago. First, we will extract the shape of Cook County. cook.county &lt;- counties[counties$COUNTYNAME == &quot;Cook&quot;, ] Before we do anything else, we can plot the shape of Cook County that we just extracted to make sure we subsetted the dataset correctly. plot(cook.county$geometry) This is a very crude plot since it only shows the border of Cook County. Rest assured that our final product is much more visually pleasing than this! The following two lines of code crop and mask raster data to Cook County. Cropping and then masking accelerates the actions for large raster data. It probably does not matter in our case, since we are only looking at Cook County, and the volume of our data is rather small. cook.ndvi &lt;- raster::crop(this.qtr, cook.county) cook.ndvi &lt;- raster::mask(cook.ndvi, cook.county) Now we can plot a raster map for Cook County. The commands we use here are very similar to the ones we used before. tm_shape(cook.ndvi) + tm_raster(title = &quot;NDVI&quot;, palette = &quot;Greens&quot;) + tm_shape(cook.county) + tm_borders() + tm_layout(legend.outside = TRUE, main.title = &quot;NDVI for Cook County:\\nThe 3rd Quarter of 2018&quot;) With this raster map, we are able to discern patterns of NDVI with Cook County. The middle-east part of Cook County, where the City of Chicago is located, is shown in a lighter green than other parts of the county, signaling a lower NDVI. The northern and southern parts of the county, which are mostly rural areas, are shown to have a higher NDVI, much as we would expect. Before we conclude this tutorial, let’s again take a look at the summary statistics. raster::summary(cook.ndvi) ## NDVILargeAreaQuarterlyStack.19 ## Min. 2.923896e-02 ## 1st Qu. 3.955676e-01 ## Median 5.302855e-01 ## 3rd Qu. 6.440714e-01 ## Max. 8.844491e-01 ## NA&#39;s 3.973500e+04 base::mean(na.omit(getValues(cook.ndvi))) ## [1] 0.5213509 base::mean(na.omit(getValues(this.qtr))) ## [1] 0.6662209 The mean NDVI for Cook County is 0.52, while the mean for all 21 counties is 0.67. This is reasonable because NDVI tends to be lower in large cities, and Cook County happens to be at the center of the Chicago metropolitan area. Moreover, this is consistent with what we observe from the plots - Cook County is shown in a lighter green than other counties, which we have pointed out earlier in the tutorial. This marks the end of our tutorial. Hopefully, now you feel comfortable dealing with raster data. Of course, there are many more techniques to be learned in order to conduct more sophisticated spatial analysis with raster data. There are abundant online resources that introduce more complicated tools to handle raster data, which you may want to explore on your own. "],
["appendix-b-point-emission-data.html", "Appendix B: Point Emission Data 6.1 Loading Packages and Data 6.2 Data Manipulation 6.3 Making Point Data Maps", " Appendix B: Point Emission Data In this tutorial, we will demonstrate how to represent PM2.5 point emission data on maps. As an example, we will look at the PM2.5 data for four states - Illinois, Indiana, Wisconsin, and Michigan. The goal of this exercise is to create a point data map that provides a clear visualization of the point source of PM2.5 emission. 6.1 Loading Packages and Data We start by loading the necessary packages - “tidyverse”, “sf”, and “tmap”. “Tidyverse” is a system of packages for data manipulation and is frequently utilized in statistical analyses. “Sf” is a package that offers a standardized way to encode spatial vector data in R. Lastly, “tmap” allows us to generate and customize a variety of maps, although we will only use it to create a simple point data map in the tutorial. library(tidyverse) library(sf) library(tmap) Besides the packages, we also need to load our data, which can be done by running the commened-out code below. We call the data frame pe1. # pe1 &lt;- read_csv(&quot;process_12345.csv&quot;) 6.2 Data Manipulation Once we have our data and the packages ready, we will start the data manipulation process. Since we will only look at the data from Illinois, Indiana, Wisconsin, and Michigan, we can use the filter function to pick out only the four states that we are interested in, and we name the new data frame fourstate.pe1. This is accomplished by the commented-out code below. This dataset comes from 2014 National Emissions Inventory, which is “a comprehensive and detailed estimate of air emissions of criteria pollutants, criteria precursors, and hazardous air pollutants from air emissions sources.” You can read more about this dataset on this website. # states.abbr &lt;- c(&quot;IL&quot;, &quot;IN&quot;, &quot;WI&quot;, &quot;MI&quot;) # fourstate.pe &lt;- pe1 %&gt;% # filter(state %in% states.abbr) Next, we turn our focus to the pollutant data. To familiarize ourselves with the variable pollutant desc, we use the unique function to examine all the unique values of this variable. #Find pollutant names for pm2.5 unique(fourstate.pe$`pollutant desc`) ## [1] &quot;Hexane&quot; ## [2] &quot;Toluene&quot; ## [3] &quot;Propionaldehyde&quot; ## [4] &quot;Xylenes (Mixed Isomers)&quot; ## [5] &quot;Benzo[g,h,i,]Perylene&quot; ## [6] &quot;Indeno[1,2,3-c,d]Pyrene&quot; ## [7] &quot;Benzo[b]Fluoranthene&quot; ## [8] &quot;Fluoranthene&quot; ## [9] &quot;Benzo[k]Fluoranthene&quot; ## [10] &quot;Acenaphthylene&quot; ## [11] &quot;Chrysene&quot; ## [12] &quot;Formaldehyde&quot; ## [13] &quot;Benzo[a]Pyrene&quot; ## [14] &quot;2,2,4-Trimethylpentane&quot; ## [15] &quot;Benz[a]Anthracene&quot; ## [16] &quot;Benzene&quot; ## [17] &quot;Lead&quot; ## [18] &quot;Acetaldehyde&quot; ## [19] &quot;Acenaphthene&quot; ## [20] &quot;Phenanthrene&quot; ## [21] &quot;Fluorene&quot; ## [22] &quot;Naphthalene&quot; ## [23] &quot;Carbon Monoxide&quot; ## [24] &quot;Nitrogen Oxides&quot; ## [25] &quot;PM10 Primary (Filt + Cond)&quot; ## [26] &quot;PM2.5 Primary (Filt + Cond)&quot; ## [27] &quot;Sulfur Dioxide&quot; ## [28] &quot;Volatile Organic Compounds&quot; ## [29] &quot;Ethyl Benzene&quot; ## [30] &quot;Styrene&quot; ## [31] &quot;1,3-Butadiene&quot; ## [32] &quot;Acrolein&quot; ## [33] &quot;Anthracene&quot; ## [34] &quot;m-Xylene&quot; ## [35] &quot;Phenol&quot; ## [36] &quot;Methanol&quot; ## [37] &quot;2-Methylnaphthalene&quot; ## [38] &quot;o-Xylene&quot; ## [39] &quot;Cumene&quot; ## [40] &quot;Dibenzo[a,h]Anthracene&quot; ## [41] &quot;PM2.5 Filterable&quot; ## [42] &quot;PM10 Filterable&quot; ## [43] &quot;PM Condensible&quot; ## [44] &quot;Carbon Dioxide&quot; ## [45] &quot;Methane&quot; ## [46] &quot;Nitrous Oxide&quot; ## [47] &quot;Cadmium&quot; ## [48] &quot;Hexamethylene Diisocyanate&quot; ## [49] &quot;Ammonia&quot; ## [50] &quot;Glycol Ethers&quot; ## [51] &quot;Manganese&quot; ## [52] &quot;Methylene Chloride&quot; ## [53] &quot;Chromium III&quot; ## [54] &quot;Chromium (VI)&quot; ## [55] &quot;Nickel&quot; ## [56] &quot;Arsenic&quot; ## [57] &quot;Cobalt&quot; ## [58] &quot;Mercury&quot; ## [59] &quot;Antimony&quot; ## [60] &quot;Selenium&quot; ## [61] &quot;Beryllium&quot; ## [62] &quot;Hydrochloric Acid&quot; ## [63] &quot;Hydrogen Fluoride&quot; ## [64] &quot;Pyrene&quot; ## [65] &quot;Isophorone&quot; ## [66] &quot;Vinyl Chloride&quot; ## [67] &quot;Biphenyl&quot; ## [68] &quot;Tetrachloroethylene&quot; ## [69] &quot;Methyl Chloroform&quot; ## [70] &quot;PAH/POM - Unspecified&quot; ## [71] &quot;2,4-Dinitrophenol&quot; ## [72] &quot;Chlorine&quot; ## [73] &quot;Pentachlorophenol&quot; ## [74] &quot;4-Nitrophenol&quot; ## [75] &quot;Ethyl Chloride&quot; ## [76] &quot;Carbon Disulfide&quot; ## [77] &quot;Ethylidene Dichloride&quot; ## [78] &quot;Propylene Dichloride&quot; ## [79] &quot;Trichloroethylene&quot; ## [80] &quot;1,1,2,2-Tetrachloroethane&quot; ## [81] &quot;Ethylene Dichloride&quot; ## [82] &quot;Acrylonitrile&quot; ## [83] &quot;Methyl Isobutyl Ketone&quot; ## [84] &quot;Chlorobenzene&quot; ## [85] &quot;Carbonyl Sulfide&quot; ## [86] &quot;Carbon Tetrachloride&quot; ## [87] &quot;Chloroform&quot; ## [88] &quot;Dimethyl Phthalate&quot; ## [89] &quot;PAH, total&quot; ## [90] &quot;Acrylic Acid&quot; ## [91] &quot;Polychlorinated Biphenyls&quot; ## [92] &quot;Ethylene Dibromide&quot; ## [93] &quot;1,3-Dichloropropene&quot; ## [94] &quot;Methyl Chloride&quot; ## [95] &quot;Phosphorus&quot; ## [96] &quot;Propylene Oxide&quot; ## [97] &quot;Vinyl Acetate&quot; ## [98] &quot;Methyl Tert-Butyl Ether&quot; ## [99] &quot;Acetophenone&quot; ## [100] &quot;Benzo[e]Pyrene&quot; ## [101] &quot;Perylene&quot; ## [102] &quot;3-Methylcholanthrene&quot; ## [103] &quot;Benzofluoranthenes&quot; ## [104] &quot;p-Xylene&quot; ## [105] &quot;Methyl Bromide&quot; ## [106] &quot;Quinoline&quot; ## [107] &quot;2,4-Dichlorophenoxy Acetic Acid&quot; ## [108] &quot;1-Bromopropane&quot; ## [109] &quot;Allyl Chloride&quot; ## [110] &quot;Ethylene Glycol&quot; ## [111] &quot;Chloromethyl Methyl Ether&quot; ## [112] &quot;Hexachlorobenzene&quot; ## [113] &quot;Triethylamine&quot; ## [114] &quot;2,4,6-Trichlorophenol&quot; ## [115] &quot;Hydrazine&quot; ## [116] &quot;N,N-Dimethylformamide&quot; ## [117] &quot;Acetonitrile&quot; ## [118] &quot;Vinylidene Chloride&quot; ## [119] &quot;Phosgene&quot; ## [120] &quot;Chloroacetic Acid&quot; ## [121] &quot;2,4-Toluene Diisocyanate&quot; ## [122] &quot;Diethanolamine&quot; ## [123] &quot;Methyl Methacrylate&quot; ## [124] &quot;Maleic Anhydride&quot; ## [125] &quot;Hexachloroethane&quot; ## [126] &quot;Cyanide&quot; ## [127] &quot;Phthalic Anhydride&quot; ## [128] &quot;Polycyclic aromatic compounds (includes 25 specific compounds)&quot; ## [129] &quot;Epichlorohydrin&quot; ## [130] &quot;Dimethyl Sulfate&quot; ## [131] &quot;Hydroquinone&quot; ## [132] &quot;Chloroprene&quot; ## [133] &quot;Hydrogen Sulfide&quot; ## [134] &quot;Acrylamide&quot; ## [135] &quot;Cresol/Cresylic Acid (Mixed Isomers)&quot; ## [136] &quot;4,4 -Methylenediphenyl Diisocyanate&quot; ## [137] &quot;Methyl Isocyanate&quot; ## [138] &quot;Tert-butyl Acetate&quot; ## [139] &quot;Bis(2-Ethylhexyl)Phthalate&quot; ## [140] &quot;Ethylene Oxide&quot; ## [141] &quot;1,1,2-Trichloroethane&quot; ## [142] &quot;o-Cresol&quot; ## [143] &quot;Catechol&quot; ## [144] &quot;Sulfur Hexafluoride&quot; ## [145] &quot;2,3,3 ,4,4 ,5/2,3,3 ,4,4 ,5-Hexachlorobiphenyl (PCBs156/157)&quot; ## [146] &quot;2,3,3 ,4,4 -Pentachlorobiphenyl (PCB-105)&quot; ## [147] &quot;2,3 ,4,4 ,5-Pentachlorobiphenyl (PCB118)&quot; ## [148] &quot;3,3 ,4,4 -Tetrachlorobiphenyl (PCB-77)&quot; ## [149] &quot;2,3 ,4,4 ,5,5 -Hexachlorobiphenyl (PCB-167)&quot; ## [150] &quot;2,3,4,4 ,5-Pentachlorobiphenyl (PCB-114)&quot; ## [151] &quot;Methylhydrazine&quot; ## [152] &quot;Benzyl Chloride&quot; ## [153] &quot;1,4-Dichlorobenzene&quot; ## [154] &quot;Bromoform&quot; ## [155] &quot;2-Nitropropane&quot; ## [156] &quot;2-Chloronaphthalene&quot; ## [157] &quot;Quinone&quot; ## [158] &quot;2-Chloroacetophenone&quot; ## [159] &quot;2,4-Dinitrotoluene&quot; ## [160] &quot;Carbazole&quot; ## [161] &quot;Aniline&quot; ## [162] &quot;p-Dioxane&quot; ## [163] &quot;4,6-Dinitro-o-Cresol&quot; ## [164] &quot;Dibutyl Phthalate&quot; ## [165] &quot;1,3-Propanesultone&quot; ## [166] &quot;Benzidine&quot; ## [167] &quot;Dibenzofuran&quot; ## [168] &quot;Ethyl Carbamate&quot; ## [169] &quot;1,2-Epoxybutane&quot; ## [170] &quot;Hydrogen Cyanide&quot; ## [171] &quot;Cellosolve Acetate&quot; ## [172] &quot;Ethyl Acrylate&quot; ## [173] &quot;o-Toluidine&quot; ## [174] &quot;7,12-Dimethylbenz[a]Anthracene&quot; ## [175] &quot;Captan&quot; ## [176] &quot;5-Methylchrysene&quot; ## [177] &quot;Methyl Iodide&quot; ## [178] &quot;Nitrobenzene&quot; ## [179] &quot;Hexachlorobutadiene&quot; ## [180] &quot;Hexachlorocyclopentadiene&quot; ## [181] &quot;1,2,4-Trichlorobenzene&quot; ## [182] &quot;Asbestos&quot; ## [183] &quot;Dichloroethyl Ether&quot; ## [184] &quot;Coke Oven Emissions&quot; ## [185] &quot;Trifluralin&quot; ## [186] &quot;Toxaphene&quot; ## [187] &quot;Carbaryl&quot; ## [188] &quot;Heptachlor&quot; ## [189] &quot;4,4 -Methylenebis(2-Chloraniline)&quot; ## [190] &quot;m-Cresol&quot; ## [191] &quot;Methoxychlor&quot; ## [192] &quot;Phosphine&quot; ## [193] &quot;Calcium Cyanamide&quot; ## [194] &quot;1,2-Propylenimine&quot; ## [195] &quot;Diethyl Sulfate&quot; ## [196] &quot;p-Cresol&quot; ## [197] &quot;Ethylene Thiourea&quot; ## [198] &quot;Chlordane&quot; ## [199] &quot;N,N-Dimethylaniline&quot; ## [200] &quot;p-Phenylenediamine&quot; ## [201] &quot;o-Anisidine&quot; ## [202] &quot;1,2-Diphenylhydrazine&quot; We see that there are quite a number of different pollutants, but we are primarily interested in the PM2.5 data. Therefore, we use the filter function again to subset the data, retaining only those observations with the pollutant being “PM2.5 Filterable” or “PM2.5 Primary (Filt + Cond)”. pm25 &lt;- c(&quot;PM2.5 Filterable&quot;, &quot;PM2.5 Primary (Filt + Cond)&quot;) #Filter for pm2.5 fourstate.pm &lt;- fourstate.pe %&gt;% filter(`pollutant desc` %in% pm25) Now we will take care of the duplicates and missing values, both of which should be removed from our data. The following line of code eliminates any duplicated values in eis facility id. #remove duplicates fourstate.pm.final &lt;- fourstate.pm[!duplicated(fourstate.pm$`eis facility id`),] The following line of code eliminates any missing values in site latitude. We call this cleaned data frame fourstate.pm.final. #remove na coords fourstate.pm.final &lt;- fourstate.pm.final[!is.na(fourstate.pm.final$`site latitude`),] The last step in the data manipulation process is to turn our data points into a spatial object, which we can accomplish by using the st_as_sf function. Notice that we use the coordinate reference system 4326, which is the geodetic coordinate system for world. More information about coordinate reference systems can be found on this website. #Turn into sf object fourstate.pm.spatial &lt;- st_as_sf(fourstate.pm.final, coords = c(&quot;site longitude&quot;, &quot;site latitude&quot;), crs = 4326) 6.3 Making Point Data Maps At this point, we only have the point data for the source of PM2.5 emission. To make a map, however, we also need to load the shapefile for the four states that we are analyzing. fourstates &lt;- st_read(&quot;../data/FourStates&quot;) ## Reading layer `FourStates&#39; from data source `/Users/LorenzMenendez/Desktop/OpenAirQ-toolkit/data/FourStates&#39; using driver `ESRI Shapefile&#39; ## Simple feature collection with 4 features and 14 fields ## geometry type: POLYGON ## dimension: XY ## bbox: xmin: -92.88943 ymin: 36.9703 xmax: -82.12297 ymax: 48.30606 ## CRS: 4326 Finally, we are ready to make some maps! The command for generating a point data map is actually quite easy. We just need to specify the shapefile that stores the shape of the states and the point data which we have turned into a spatial object earlier. We adjust the size of the dots to 0.01 so that the pattern of those points is discernible. tm_shape(fourstates) + tm_borders() + tm_shape(fourstate.pm.spatial) + tm_dots(size = 0.01) The map above is neat, but it only conveys limited information. It is impossible, for example, to tell from the map how much PM2.5 emission each of these dots produces. To improve the simple map above, we use the tem_bubbles function instead of tm_dots. Within tm_bubbles, we can choose how to classify the PM2.5 data by specifying the style. Some common choices of style include “fisher”, “jenks”, “quantile”, etc. Customization of the color palette is also possible, and there are plenty of online tutorials on this topic. tm_shape(fourstates) + tm_borders() + tm_shape(fourstate.pm.spatial) + tm_bubbles(col = &quot;total emissions&quot;, size = 0.01, style = &quot;fisher&quot;, palette = &quot;Reds&quot;) As we can see, the map above is not only more aesthetically pleasing, but it also communicates more information regarding the quantity of PM2.5 that is produced by each site. Sites that generate more PM2.5 are shown in a darker red. With this map, we can identify those sites of heavy PM2.5 emission with great ease by zooming in. This concludes our tutorial. Following the simple steps above would allow you to create some simple point data maps, which are often a neat and easily interpretable visualization of the spatial data that we seek to analyze. "],
["appendix-c-elevation-data.html", "Appendix C: Elevation Data 6.4 Loading Packages and Data 6.5 Inspecting Data 6.6 Data Manipulation", " Appendix C: Elevation Data The goal of this tutorial is to introduce the packages and techniques that are commonly utilized to manipulate raster data and create grid cells. We will use the elevation data of several larges counties in the Midwest as an example, and special emphasis will be placed on introducing the functionalities of the “velox” package. 6.4 Loading Packages and Data We start the tutorial by loading the required packages. The packages “sf,” “raster,” “gstat,” and “tmap” are all standard packages widely used in spatial analysis. The package “tidyverse” is great for all kinds of data manipulation, and its application is not limited to spatial data. The only package that is worth special mention the “velox,” which has excellent performance in fast raster data manipulation. Unfortunately, this package is no longer available. For the lack of substitutes, however, we will still utilize the “velox” package in this tutorial. To install “velox” from archive, we can run the following code. library(devtools) devtools::install_github(&quot;https://github.com/hunzikp/velox&quot;) (Note, you may have to install an installer, such as Xcode, in order to compile the package locally. Some helpful information can be found here.) library(sf) library(tidyverse) library(raster) library(gstat) library(tmap) library(velox) library(tmap) Loading data is the next step. We load the “grid” data (in kilometers), which is named km.grid, the shapefile of large counties, which is named lac, and also the elevation data, which is named lac.elevation. km.grid &lt;- st_read(&quot;../data/Km_Grid&quot;) ## Reading layer `Km_Grid&#39; from data source `/Users/LorenzMenendez/Desktop/OpenAirQ-toolkit/data/Km_Grid&#39; using driver `ESRI Shapefile&#39; ## Simple feature collection with 29903 features and 1 field ## geometry type: POLYGON ## dimension: XY ## bbox: xmin: -88.78167 ymin: 40.73704 xmax: -86.46887 ymax: 43.19439 ## CRS: 4326 lac &lt;- st_read(&quot;../data/LargeAreaCounties&quot;) ## Reading layer `LargeAreaCounties&#39; from data source `/Users/LorenzMenendez/Desktop/OpenAirQ-toolkit/data/LargeAreaCounties&#39; using driver `ESRI Shapefile&#39; ## Simple feature collection with 21 features and 8 fields ## geometry type: MULTIPOLYGON ## dimension: XY ## bbox: xmin: -88.77879 ymin: 40.73641 xmax: -86.46629 ymax: 43.21831 ## CRS: 4326 lac.elevation &lt;- readRDS(&quot;../data/LAC_Elevation.rds&quot;) The elevation data comes from the 3D Elevation Program (3DEP) of United States Geological Survey. One nice thing about this program is that all of its data are available “free of charge and without use restrictions”. If you are interested in learning more about what data are available, feel free to explore this website. 6.5 Inspecting Data Before we proceed any further, let’s check the resolution of our elevation data first. The resolution is shown in degrees because that is the unit of CRS. raster::res(lac.elevation) ## [1] 0.0002777778 0.0002777778 We can now plot a raster map, as shown below. tm_shape(lac.elevation) + tm_raster(alpha = .5) + tm_shape(lac) + tm_borders() + tm_layout(legend.outside = TRUE, main.title = &quot;Elevation of Large Counties in Midwest&quot;, main.title.size = 1.2) 6.6 Data Manipulation Raster data manipulation is where the “velox” packages really shines as it contains a large number of functions that you will find useful in handling raster data. For example, aggregate allows users to aggregate a VeloxRaster object to a lower resolution. Another function, crop, as its name suggests, lets you crop a VeloxRaster function. If you want to rasterize a set of polygons, then you can use rasterize. There are many more functions that are of interest in raster data manipulation, but it is impossible to list all of them here. Detailed documentation about this package can be found here, and you should feel free to explore it at your own pace. In our tutorial, we will mainly be using velox and extract. First, we call the velox function to convert lac.elevation into a VeloxRaster object. Then we extract the mean value of each grid (km \\(\\times\\) km), and we name it km.elecation.vx. Note that “velox” is indeed good at fast raster data manipulation - running the two lines of code takes very little time. elevation.vx &lt;- velox(lac.elevation) km.elevation.vx &lt;- elevation.vx$extract(km.grid, fun = mean) head(km.elevation.vx) Let’s pause for a second and examine the object km.elevation.vx. As we would expect, it is just a list of the averages of each grid cell. Next, we join this elevation data with km.grid, which we loaded earlier but haven’t touched at all. km.grid$Elevation &lt;- as.numeric(km.elevation.vx) ##S Plotting Grid Map With the data ready, we can now plot a graph. tm_shape(km.grid) + tm_fill(&quot;Elevation&quot;) + tm_layout(legend.outside = TRUE, main.title = &quot;Elevation of Large Counties in Midwest&quot;, main.title.size = 1.2) We now have a nice grid map - we have successfully accomplished the goal of this tutorial. To save the results of all this work, we can use st_write to output a new shapefile. st_write(km.grid, &quot;Elevation_Grid.shp&quot;) This is the end of the tutorial. The example of the elevation data is not extraordinarily exciting on its own. It is only used to demonstrate the techniques to create grid cells from raster data. The “velox” introduced in this tutorial is sadly no longer available. However, we still recommend trying out this package as its ability to perform fast raster processing is unmatched. "]
]
