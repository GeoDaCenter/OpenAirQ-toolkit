[
["index.html", "Welcome to the Documentation! Preface Hello World!", " Welcome to the Documentation! Authors Names Here 12/19/2019 Preface Hello World! This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com. When you click the Knit button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this: summary(cars) ## speed dist ## Min. : 4.0 Min. : 2.00 ## 1st Qu.:12.0 1st Qu.: 26.00 ## Median :15.0 Median : 36.00 ## Mean :15.4 Mean : 42.98 ## 3rd Qu.:19.0 3rd Qu.: 56.00 ## Max. :25.0 Max. :120.00 "],
["vector-data-mapping.html", "1 Vector Data Mapping 1.1 Required Packages 1.2 Political Boundaries 1.3 Ground-based Data Sensor Locations 1.4 Point Sources of Pollution", " 1 Vector Data Mapping 1.1 Required Packages tmap: Flexible thematic mapping sf: Spatial vector data manipulation dplyr: data.frame manipulation tmap will be set to interactive mode for this tutorial. tmap_mode(&#39;view&#39;) ## tmap mode set to interactive viewing 1.2 Political Boundaries Air quality modeling occured over the Chicago metropolitan area. This geography consist of 21 individual counties within the states of Illinois, Indiana, and Wisconsin. Read in County Boundaries counties = sf::st_read(&#39;../data/LargeAreaCounties/LargeAreaCounties.shp&#39;) ## Reading layer `LargeAreaCounties&#39; from data source `/Users/LorenzMenendez/Desktop/OpenAirQ-toolkit/data/LargeAreaCounties/LargeAreaCounties.shp&#39; using driver `ESRI Shapefile&#39; ## Simple feature collection with 21 features and 8 fields ## geometry type: MULTIPOLYGON ## dimension: XY ## bbox: xmin: -88.77879 ymin: 40.73641 xmax: -86.46629 ymax: 43.21831 ## CRS: 4326 Plot with tmap tm_shape(counties) + tm_borders() + tm_text(&quot;COUNTYNAME&quot;, size = 0.7, along.lines = TRUE) + tm_fill(col = &quot;STATE&quot;, alpha = 0.5) Air modeling data, however, is collected at a larger spatial scale to account for region-wide effects that could affect the 21 country study area. Four midwestern states (Illinois, Indiana, Michigan, and Wisconsin) were chosen as a data collection area. states = sf::st_read(&#39;../data/FourStates/FourStates.shp&#39;) ## Reading layer `FourStates&#39; from data source `/Users/LorenzMenendez/Desktop/OpenAirQ-toolkit/data/FourStates/FourStates.shp&#39; using driver `ESRI Shapefile&#39; ## Simple feature collection with 4 features and 14 fields ## geometry type: POLYGON ## dimension: XY ## bbox: xmin: -92.88943 ymin: 36.9703 xmax: -82.12297 ymax: 48.30606 ## CRS: 4326 tm_shape(states) + tm_borders() + tm_text(&quot;NAME&quot;, size = 0.8, auto.placement = TRUE) + tm_fill(col = &quot;NAME&quot;, alpha = 0.5) + tm_shape(counties) + tm_fill(col = &quot;black&quot;, alpha = 0.25) + tm_borders(col = &quot;black&quot;, alpha = 0.25) 1.3 Ground-based Data Sensor Locations 1.3.1 EPA Particulate Matter Sensors Over 127 PM2.5 pollution monitoring stations are located across the four state data collection area. The map below describes the distribution of these point locations. More information on the data collection methods and output data from each sensor will be discussed later on. sensors = sf::st_read(&#39;../data/PM25_4States_2014.2018_POINTLOCS.geojson&#39;) %&gt;% dplyr::mutate(rec_duration = as.numeric(lastRec - firstRec)) ## Reading layer `PM25_4States_2014.2018_POINTLOCS&#39; from data source `/Users/LorenzMenendez/Desktop/OpenAirQ-toolkit/data/PM25_4States_2014.2018_POINTLOCS.geojson&#39; using driver `GeoJSON&#39; ## Simple feature collection with 127 features and 3 fields ## geometry type: POINT ## dimension: XY ## bbox: xmin: -91.2269 ymin: 37.97444 xmax: -82.45623 ymax: 46.60225 ## CRS: 4326 basemap + tm_shape(sensors) + tm_markers(shape = tmap_icons(&#39;../data/assets/EPA_logo.png&#39;)) 1.3.2 Weather Stations High temporal resolution weather data was sourced from a large network of ground-based weather stations co-located at many airports. They provide data at regular one hour intervals on variables such as temperature, pressure, wind velocity, and wind direction. The map below describes the distribution of sensors in the four-state data collection area. asos = sf::st_read(&#39;../data/4States_ASOS_2018_Locations.geojson&#39;) ## Reading layer `4States_ASOS_2018_Locations&#39; from data source `/Users/LorenzMenendez/Desktop/OpenAirQ-toolkit/data/4States_ASOS_2018_Locations.geojson&#39; using driver `GeoJSON&#39; ## Simple feature collection with 250 features and 2 fields ## geometry type: POINT ## dimension: XY ## bbox: xmin: -92.69 ymin: 37.0647 xmax: -82.52886 ymax: 47.46694 ## CRS: 4326 basemap + tm_shape(asos) + tm_markers(shape = tmap_icons(&#39;../data/assets/airport_icon.png&#39;)) 1.4 Point Sources of Pollution The EPA National Emissions Inventory (NEI) dataset (2014) describes the location of large sources of pollution, such as powerplants, factories, and other industrial buildings. points.pollution = sf::st_read(&#39;../data/Point_Source_Emissions_4States_2014.geojson&#39;) ## Reading layer `Point_Source_Emissions_4States_2014&#39; from data source `/Users/LorenzMenendez/Desktop/OpenAirQ-toolkit/data/Point_Source_Emissions_4States_2014.geojson&#39; using driver `GeoJSON&#39; ## Simple feature collection with 33783 features and 10 fields ## geometry type: POINT ## dimension: XY ## bbox: xmin: -92.7558 ymin: 37.01605 xmax: -82.4233 ymax: 47.1859 ## CRS: 4326 basemap + tm_shape(points.pollution) + tm_markers() "],
["data-prep-management.html", "2 Data Prep &amp; Management 2.1 EPA Pollution Data 2.2 FAA Weather Data", " 2 Data Prep &amp; Management This project uses weather and pollution data from remotely sensed satellite imagery, but also ground based sensors maintained by the EPA and FAA to model air quality in the Midwest. Using the ground sensors, the team can attempt to predict pollutions levels based on satellite data. This chapter focuses on how weather and pollution data from ground sensors was downloaded and prepared for use in refining the prediction. 2.1 EPA Pollution Data Figure 2.1: EPA Pollution Monitoring Site (EPA.gov) EPA data was seamlessly imported into R using the aqsr package by Joshua P. Keller at Colorado State University. The package takes advanatge of the EPA AQS DataMart API to load data in R as data.frame objects with only a couple lines of code. It allows users to query for sensor data across multiple air quality variables, geographies, and timeframes. Let’s get started by downloading the package. # devtools::install_github(&quot;jpkeller/aqsr&quot;) library(aqsr) 2.1.1 Getting Started This section describes the process for querying EPA sensor data using the aqsr package. For more information on how each function works, please reference the package documentation. Obtaining an API Key For first time users of the AQS DataMart API, you must first register your email to recieve an API key. (Users who already have a DataMart API key, please skep to the next step). The API key is a required input for all querying functions in the aqsr package. Obtaining a key is made simple by calling the aqs_signup() function and inputting your own email address. aqs_signup(&#39;YourEmailHere@uchicago.edu&#39;) Save your API key from the email confirmation for future reference. In case you don’t recieve an email, verify that your email address was typed correctly, and check your spam folder. 2.1.1.1 Using your API Key in aqsr Setup your AQI key with the aqr package by using the create_user() function. This way, you won’t have to keep typing your email and API key each time you query for data. myuser = create_user(email = &#39;YourEmailHere@uchicago.edu&#39;, key = &#39;apikey123&#39;) 2.1.2 PM2.5 Data Query This section describes how to query for PM2.5 concetration data from EPA pollution sensors. We are looking for at PM2.5 data Wisconsin, Illinois, and Indiana between 2014 and 2018 for our project. First, let’s start small and query only for Illinois data for the first week of 2018. IL.data = aqs_dailyData_byState(aqs_user = myuser, # Previously defined user emailand API key param = 88101, # EPA AQS Parameter Code for PM2.5 bdate = &quot;20180101&quot;, # Starting Date (Jan 1st ,2018) edate = &quot;20180107&quot;, # Ending Date (Jan 7th, 2018) state = &quot;17&quot;) # State FIPS Code for Illinois state_code county_code site_number parameter_code poc latitude longitude datum parameter sample_duration 2 17 111 0001 88101 1 42.22144 -88.24221 WGS84 PM2.5 - Local Conditions 24 HOUR 2100 17 111 0001 88101 1 42.22144 -88.24221 WGS84 PM2.5 - Local Conditions 24 HOUR 3 17 111 0001 88101 1 42.22144 -88.24221 WGS84 PM2.5 - Local Conditions 24 HOUR 4 17 111 0001 88101 1 42.22144 -88.24221 WGS84 PM2.5 - Local Conditions 24 HOUR 5 17 163 0010 88101 1 38.61203 -90.16048 WGS84 PM2.5 - Local Conditions 24 HOUR The outputted data frame includes many fields regarding the PM2.5 observation, including spatial data for the sensor’s location. We will focus on these details later on in our data wrangling process. The next code chunk describes how to query for PM2.5 data across our three states and four years. library(dplyr) # List of States to Iterate Through states = c(&quot;17&quot;, &quot;18&quot;, &quot;55&quot;) # Matrix of Start Dates and End Dates to Iterate Through dates = matrix(c(&quot;20140101&quot;, &quot;20141231&quot;, &quot;20150101&quot;, &quot;20151231&quot;, &quot;20160101&quot;, &quot;20161231&quot;, &quot;20170101&quot;, &quot;20171231&quot;, &quot;20180101&quot;, &quot;20181231&quot;), ncol = 2, byrow = TRUE) # Leveraging apply functions to iterate through both states and dates full.data = lapply(states, function(x){ mapply(aqs_dailyData_byState, bdate = dates[,1], edate = dates[,2], MoreArgs = list(aqs_user = myuser, param = 88101, state = x), SIMPLIFY = FALSE ) %&gt;% do.call(&quot;rbind&quot;, .) }) %&gt;% do.call(&quot;rbind&quot;, .) 2.2 FAA Weather Data Figure 2.2: An ASOS Observation Station in Elko, NV. (Wikimedia Commons) FAA weather data gathered from the Automated Surface Observing System (ASOS) can be imported using the riem package. This package, created by ROpenSci, queries weather data from the Iowa Environmental Mesonet, an online portal for international ASOS data maintained by Iowa State University. First, let’s load the package. # devtools::install_github(&#39;ropensci/riem&#39;) library(riem, quietly = TRUE) 2.2.1 Sample Query Below is an R code snippet that performs the simplest weather data query possible in the riem package. It specifies a particular weather station using an airport code and a date range to query for. The output is a tibble table of raw ASOS weather data. The code snippet below extracts sensor data at the San Francisco International Airport. SFO.weather = riem_measures(station = &#39;KSFO&#39;, date_start = &quot;2014-01-01&quot;, date_end = &#39;2014-01-02&#39;) station valid tmpf dwpf relh drct SFO 2014-01-01 00:56:00 53.96 42.98 66.28 290 SFO 2014-01-01 01:56:00 51.98 42.98 71.28 280 SFO 2014-01-01 02:56:00 51.08 44.06 76.80 290 SFO 2014-01-01 03:56:00 48.92 37.94 65.67 0 SFO 2014-01-01 04:56:00 50.00 39.92 68.15 0 station valid tmpf dwpf alti vsby SFO 2014-01-01 00:56:00 53.96 42.98 30.16 10 SFO 2014-01-01 01:56:00 51.98 42.98 30.17 10 SFO 2014-01-01 02:56:00 51.08 44.06 30.17 10 SFO 2014-01-01 03:56:00 48.92 37.94 30.18 10 SFO 2014-01-01 04:56:00 50.00 39.92 30.18 10 SFO 2014-01-01 05:56:00 48.92 37.04 30.19 10 The outputted table shows weather data for a 24-hour period on January 1st, 2014 at the San Francisco International Airport. The valid column species when each weather report was generated, typically at 1-hour intervals. The tmpf and dwpf columns give the ambient air temperature and dew point in Fahrenheit (ºF). Other important variables in our project include air pressure (alti), measured in inches of mercury (in.Hg), and visibility (vsby) in miles. For more information on all available varibles, see Iowa State’s Documentation. Next, we will apply this function at a large scare across multiple sensors and timescales. 2.2.2 Finding ASOS Sensors The FAA collects weather data at hourly intervals for each meteorological station, with some stations providing half-hour intervals. Even querying for short periods of time can yield large amounts of data. To optimise performance, we want to only query data from stations in our study area. Finding Sensors by State In our project, we focus on certain counties in Illinois, Indiana, and Wisconsin, so we are interested in finding the sensors within that study area. The first step is to query the locations of all weather stations in the three states using the riem package. In the example below, we query for sensors in the Illinois ASOS sensor network. IL.stations = riem_stations(network = &#39;IL_ASOS&#39;) ## No encoding supplied: defaulting to UTF-8. id name lon lat ALN ALTON/ST LOUIS R -90.04599 38.88992 BMI BLOOMINGTON/NORM -88.91592 40.47711 CPS CAHOKIA/ST LOUIS -90.15622 38.57073 CIR Cairo -89.21960 37.06470 MDH CARBONDALE/MURPH -89.25000 37.78000 CUL Carmi -88.12306 38.08948 To query for data across multiple states, we are going the apply the riem_stations function to a list of weather station networks, as shown below. networks = list(&#39;IL_ASOS&#39;, &#39;IN_ASOS&#39;, &#39;WI_ASOS&#39;) library(dplyr, quietly = TRUE) station.locs = lapply(networks, riem::riem_stations) %&gt;% do.call(rbind, .) # Creates a single data table as output Note: You can find a list of state abbreviations by typing state.abb in your R console. Converting Latitude and Longitude Coordinates to Spatial Data The data tables returned by the riem package must be converted to spatial data to determine which sensors are located in the study area. Since the lon/lat coordinates are already provided, the data table is easily converted to a spatial sf object. station.locs.sf = sf::st_as_sf(station.locs, coords = c(&quot;lon&quot;, &quot;lat&quot;), crs = 4326) # Plot stations and study area boundaries to verify that the correct sensors were selected plot(station.locs.sf$geometry) plot(sf::st_read(&#39;https://uchicago.box.com/shared/static/uw0srt8nyyjfqo6l0dv07cyskwmv6r50.geojson&#39;, quiet = TRUE)$geometry, border = &#39;red&#39;, add = TRUE) We plot to results to verify that our query and data conversion process worked correctly. For reference, the boundaires of the study area is outlined in red. Selecting Sensors within our Study Area Next, we perform a spatial join to only keep the points located within the boundaries of our study area polygons. The spatial join is completed by the sf package, as shown below. For more information regarding spatial joins and spatial predicates, please see this helpful blog post by GISgeography.com. # Loading study area boundaries study.area = sf::st_read(&#39;https://uchicago.box.com/shared/static/uw0srt8nyyjfqo6l0dv07cyskwmv6r50.geojson&#39;, quiet = TRUE) study.sensors = sf::st_join(station.locs.sf, study.area, left = FALSE) # Verify Spatial Join by Plotting plot(study.area$geometry, border = &#39;red&#39;) plot(study.sensors$geometry, add = TRUE) title(&#39;Weather Stations Within the Study Area&#39;) Now that we have a dataset of which weather stations we are interested in, we can query for the weather data associated with each station. 2.2.3 Weather Data Query Again we use the lapply function in base R to execute the riem_measures function on a list of sensor IDs. This allows us to iteratively query for weather data from each individual sensor in a list. In the code snippet below, we take the study sensors obtained previously and query for a single day’s worth of weather data. library(dplyr, quietly = TRUE) weather.data = lapply(study.sensors$id, function(x){riem::riem_measures(x, date_start = &quot;2014-01-01&quot;, date_end = &quot;2014-01-02&quot;)}) %&gt;% do.call(rbind, .) # Creates a single data table as output station valid lon lat tmpf MDW 2014-01-01 00:49:00 -87.7524 41.786 12.20 MDW 2014-01-01 00:51:00 -87.7524 41.786 12.92 MDW 2014-01-01 01:51:00 -87.7524 41.786 12.92 MDW 2014-01-01 02:27:00 -87.7524 41.786 14.00 MDW 2014-01-01 02:51:00 -87.7524 41.786 14.00 Querying Full Weather Dataset Use caution when querying for a large amount of data. Data tables can easily become unwieldy after querying for a large number of weather stations across a wide time scale. The code snippet below downloads all ASOS weather data for sensors in our study area from January 1st 2014 to December 31st 2018, which is our study time period. It has approximately 4.8 Million records and takes 6-10 minutes to download. weather.data = lapply(study.sensors$id, function(x){riem::riem_measures(x, date_start = &quot;2014-01-01&quot;, date_end = &quot;2018-12-31&quot;)}) %&gt;% do.call(rbind, .) # Creates a single data table as output "],
["merging-satellite-and-point-sensor-data.html", "3 Merging Satellite and Point Sensor Data 3.1 Generate Rasters 3.2 Export to CSV", " 3 Merging Satellite and Point Sensor Data There’s a fundamental challenge in our project to merge AOD data and predictor variables because the data capture techniques are very different. The satellite-based AOD data is measured continuously across the surface of the earth on a 1km by 1km grid system. Meanwhile, sensor data is only captured locally at the sensor location. Therefore, a method to interpolate local sensor data to generate a continuous surface of data is required. An ‘Optimized’ IDW interpolation was used to estimate sensor values across a 1km by 1km grid system. This method takes into account the sensor locations and value for each variable to estimate values in grid cells without a sensor based on a linear interpolation of nearby sensor values. The specific number of sensors to take into account and the distance decay power function were optimized by medinimizing the RMSE (Error). This method was adapted from an RSpatial Tutorial on IDW interpolation with pollution and weather data. To simplify implementation and replication, the entire workflow was coded in R and bundled into a packaged named sensor2raster. The next sections demonstrate how to apply this package to sensor data. # devtools::install_local(&#39;../data/sensor2raster_0.04.tar.gz&#39;) library(sensor2raster, quietly = TRUE) 3.1 Generate Rasters Creating raster surfaces is easy using the sensor2raster function. This function takes the raw output from the riem or aqsr packages and identifies the sensor locations and data values to interpolate. The underlying IDW interpolation is performed by the gstat package. The code chunk below demonstrates how to take ASOS weather data and convert it to a Raster format. The weather data variable is a data frame containing temperature data measured at airports in the Greater Chicago and Milwaukee Areas in 2018. We also pass the AOD.grid, a RasterLayer object representing the grid cells where we want to predict temperature. These grid cells correspond exactly to the pixels of satellite AOD data. temp.rasters = sensor2raster(sensor.data = weather.data, # Input the raw data.frame from riem data.type = &#39;ASOS&#39;, # Specify data type (&#39;ASOS&#39; or &#39;EPA&#39;) reference.grid = AOD.grid, # Grid to interpolate over subvariable = &#39;tmpf&#39;) # Column name of variable to interpolate 3.2 Export to CSV While Raster data is helpful for spatial data analysis and geovisualizations, it is sometimes helpful to store the interpolation in a non-spatial format. The grid2csv function allows you to convert Raster data to CSV either cell-by-cell, or by aggregating to a vector geometry. The exported data.frame is halfway between Long and Wide format due to the 3-dimensional nature of our data. The table below describes how these CSVs are structured in the cell-by-cell case. Var_Name Raster.Cell M1.2018 M2.2018 … M12.2018 Monthly_Temperature 1 23 25 … 20 Monthly_Temperature 2 23 25 … 20 … … … … … … Monthly_Temperature 100 10 15 … 11 The length of the table equals the number of cells in the RasterStack. Each cell is given a unique identifier stored in the Raster.Cell column. The Var_Name colums represent the variable of interest. When there are multiple variables, they are row binded together, giving a long table format. The rest of the columns represent the names given to the layers within each RasterStack. Im this case, each column represents a month and year combination. Additional time periods are appended column-wise to the table. The following table describes the outputted data frame from the monthly temperature Rasters generated earlier. temp.export = grid2csv(rasters = list(temp.rasters[[1]]), var.names = &#39;Monthly_Temperature&#39;) Var_Name Raster.Cell M1.2018 M2.2018 M3.2018 M4.2018 M5.2018 M6.2018 M7.2018 M8.2018 M9.2018 M10.2018 M11.2018 M12.2018 Monthly_Temperature 1 22.75343 24.89301 32.54753 36.13429 59.56493 64.5328 70.55568 70.15553 62.38351 48.01338 30.81887 29.27243 Monthly_Temperature 2 22.75343 24.89301 32.54753 36.13429 59.56493 64.5328 70.55568 70.15553 62.38351 48.01338 30.81887 29.27243 Monthly_Temperature 3 22.75343 24.89301 32.54753 36.13429 59.56493 64.5328 70.55568 70.15553 62.38351 48.01338 30.81887 29.27243 Monthly_Temperature 4 22.75343 24.89301 32.54753 36.13429 59.56493 64.5328 70.55568 70.15553 62.38351 48.01338 30.81887 29.27243 Monthly_Temperature 5 22.75343 24.89301 32.54753 36.13429 59.56493 64.5328 70.55568 70.15553 62.38351 48.01338 30.81887 29.27243 Monthly_Temperature 6 22.75343 24.89301 32.54753 36.13429 59.56493 64.5328 70.55568 70.15553 62.38351 48.01338 30.81887 29.27243 The format of the table changes slightly when subsetting using a vecor object. A new column sj_join appears in the table, representing a unique allowing the table to be joined back to the origin sf object if needed. When subsetting by point features, the Raster_Cell column describes the cell that overlapped with each point feature. When subsettting by line or polygon features, the Raster_Cell column describes the cell that overlapped with the centroid of each geometry. temp.export.sf = grid2csv(rasters = list(temp.rasters[[1]]), var.names = &#39;Monthly_Temperature&#39;, sf.obj = counties) Var_Name Raster_Cell sf_join M1.2018 M2.2018 21 Monthly_Temperature 4745 21 22.74704 24.89580 3 Monthly_Temperature 4970 3 23.00074 25.13390 8 Monthly_Temperature 11038 8 23.88734 26.08940 20 Monthly_Temperature 12957 20 23.22283 25.46473 19 Monthly_Temperature 15155 19 24.72884 26.94046 4 Monthly_Temperature 21000 4 24.81107 27.53006 The code chunk below demonstrates how to exploit the sf_join field to join the table data back to the spatial sf object. # counties.join = counties %&gt;% tibble::rowid_to_column() %&gt;% dplyr::rename(sf_join = rowid) # # counties.join = grid2csv(rasters = list(temp.rasters[[1]]), var.names = &#39;Monthly_Temperature&#39;, sf.obj = counties.join) %&gt;% # left_join(counties.join) %&gt;% # st_as_sf() "]
]
